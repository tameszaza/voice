{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements\n",
    "\n",
    "```markdown\n",
    "# Import necessary libraries and modules for the project\n",
    "#\n",
    "# This section imports various Python libraries and modules required for:\n",
    "# - File and directory operations (os)\n",
    "# - Random number generation (random, numpy)\n",
    "# - Mathematical operations (math, numpy)\n",
    "# - PyTorch for deep learning (torch and its submodules)\n",
    "# - Audio processing (soundfile, librosa)\n",
    "# - File globbing (glob)\n",
    "# - Text-to-phoneme conversion (phonemizer)\n",
    "# - Data loading and processing (torch.utils.data)\n",
    "# - Neural network normalization (torch.nn.utils)\n",
    "# - Gradient checkpointing (torch.utils.checkpoint)\n",
    "# - Plotting (matplotlib)\n",
    "# - Garbage collection (gc)\n",
    "# - Logging (logging)\n",
    "# - Automatic mixed precision training (torch.amp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from glob import glob\n",
    "from phonemizer import phonemize\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils import spectral_norm\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import logging\n",
    "from torch.amp import GradScaler, autocast\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################\n",
    "# Utility and Setup Functions\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def load_flac(file_path, target_sr=16000):\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
    "    max_val = np.max(np.abs(audio))\n",
    "    if max_val > 0:\n",
    "        audio = audio / max_val  # Normalize to [-1, 1]\n",
    "    return audio\n",
    "\n",
    "def pad_or_trim(audio, target_length=64000):\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:target_length]\n",
    "    return audio\n",
    "\n",
    "def get_transcription(file_path):\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "    base_name = os.path.basename(file_path)\n",
    "    file_id = os.path.splitext(base_name)[0]\n",
    "    transcription_file = None\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith(\".trans.txt\"):\n",
    "            transcription_file = os.path.join(dir_path, file)\n",
    "            break\n",
    "    if not transcription_file:\n",
    "        raise FileNotFoundError(f\"No transcription file found in {dir_path}\")\n",
    "\n",
    "    with open(transcription_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\" \", 1)\n",
    "            if parts[0] == file_id:\n",
    "                transcription = parts[1]\n",
    "                return transcription\n",
    "    raise ValueError(f\"No transcription found for file {file_id}\")\n",
    "\n",
    "def save_waveform_to_audio(waveform, sample_rate, filename):\n",
    "    if isinstance(waveform, torch.Tensor):\n",
    "        waveform = waveform.detach().cpu().numpy()\n",
    "    waveform = np.squeeze(waveform)\n",
    "    max_val = np.max(np.abs(waveform))\n",
    "    if max_val > 0:\n",
    "        waveform = waveform / max_val\n",
    "    sf.write(filename, waveform, sample_rate)\n",
    "\n",
    "def generate_noise(batch_size, z_dim, device):\n",
    "    return torch.randn(batch_size, z_dim, device=device)\n",
    "\n",
    "def orthogonal_loss(feature1, feature2):\n",
    "    inner_product = torch.sum(feature1 * feature2, dim=1)\n",
    "    norm1 = torch.norm(feature1, dim=1)\n",
    "    norm2 = torch.norm(feature2, dim=1)\n",
    "    cosine_similarity = inner_product / (norm1 * norm2 + 1e-8)\n",
    "    return torch.mean(cosine_similarity**2)  # Minimize similarity to enforce orthogonality\n",
    "\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, conditions, device):\n",
    "    batch_size = real_samples.size(0)\n",
    "    epsilon = torch.rand(batch_size, 1, 1, device=device)\n",
    "    epsilon = epsilon.expand_as(real_samples)\n",
    "    interpolates = (epsilon * real_samples + (1 - epsilon) * fake_samples).requires_grad_(True)\n",
    "\n",
    "    real_outputs, fake_outputs = discriminator(interpolates, fake_samples, conditions)\n",
    "    real_output = real_outputs[0]\n",
    "\n",
    "    grad_outputs = torch.ones_like(real_output, device=device)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=real_output,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=grad_outputs,\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "def visualize_and_save_generated_waveforms(generators, z_dim, features_emb, num_waveforms, device, epoch, sample_rate=16000, output_dir='generated_audio'):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for idx, gen in enumerate(generators):\n",
    "        gen.eval()\n",
    "        with torch.no_grad():\n",
    "            noise = generate_noise(num_waveforms, z_dim, device)\n",
    "            fake_waveforms = gen(features_emb[:num_waveforms], noise).cpu()\n",
    "            num_available_waveforms = fake_waveforms.size(0)\n",
    "            if num_available_waveforms < num_waveforms:\n",
    "                print(f\"Warning: Requested {num_waveforms} waveforms, got {num_available_waveforms}\")\n",
    "            for i in range(num_available_waveforms):\n",
    "                waveform = fake_waveforms[i]\n",
    "                filename = f'epoch{epoch+1}_gen{idx+1}_sample{i+1}.wav'\n",
    "                filepath = os.path.join(output_dir, filename)\n",
    "                save_waveform_to_audio(waveform, sample_rate, filepath)\n",
    "                print(f\"Saved {filepath}\")\n",
    "\n",
    "def pad_tensors_to_match(tensor_list):\n",
    "    # Pad a list of tensors along the last dimension to match their sizes\n",
    "    max_len = max(t.size(-1) for t in tensor_list)\n",
    "    max_channels = max(t.size(1) for t in tensor_list)\n",
    "    padded_tensors = []\n",
    "    for t in tensor_list:\n",
    "        diff_len = max_len - t.size(-1)\n",
    "        diff_channels = max_channels - t.size(1)\n",
    "        if diff_len > 0 or diff_channels > 0:\n",
    "            t = F.pad(t, (0, diff_len, 0, diff_channels))\n",
    "        padded_tensors.append(t)\n",
    "    return torch.stack(padded_tensors, dim=0)\n",
    "\n",
    "def clear_all_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"Cleared GPU memory cache.\")\n",
    "    gc.collect()\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
    "                del obj\n",
    "        except:\n",
    "            pass\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Final GPU cleanup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###################################\n",
    "# Custom Dataset\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MyAudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, target_sr=16000, target_length=64000, feature_length=100):\n",
    "        self.root_dir = root_dir\n",
    "        self.target_sr = target_sr\n",
    "        self.target_length = target_length\n",
    "        self.feature_length = feature_length\n",
    "        self.flac_files = glob(os.path.join(root_dir, '**', '*.flac'), recursive=True)\n",
    "        logger.info(f\"Found {len(self.flac_files)} .flac files in {root_dir}.\")\n",
    "        if len(self.flac_files) == 0:\n",
    "            raise ValueError(\"No .flac files found.\")\n",
    "        \n",
    "        # Build a global phoneme-to-id dictionary by scanning all files\n",
    "        all_phonemes = set()\n",
    "        for f in self.flac_files:\n",
    "            try:\n",
    "                transcription = get_transcription(f)\n",
    "                phonemes = phonemize(transcription, backend=\"espeak\", language=\"en-us\")\n",
    "                all_phonemes.update(list(phonemes))\n",
    "            except Exception as e:\n",
    "                logger.info(f\"Error processing file {f}: {e}\")\n",
    "        \n",
    "        self.phoneme_to_id = {p: i for i, p in enumerate(sorted(all_phonemes))}\n",
    "        logger.info(f\"Phoneme vocabulary size: {len(self.phoneme_to_id)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.flac_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.flac_files[idx]\n",
    "        try:\n",
    "            audio = load_flac(file, self.target_sr)\n",
    "            audio = pad_or_trim(audio, self.target_length)\n",
    "            transcription = get_transcription(file)\n",
    "            if not transcription or not transcription.strip():\n",
    "                raise ValueError(f\"Empty or invalid transcription for file: {file}\")\n",
    "\n",
    "            phonemes = phonemize(transcription, backend=\"espeak\", language=\"en-us\")\n",
    "            phonetic_features = [self.phoneme_to_id[p] for p in phonemes]\n",
    "\n",
    "            # Convert to tensor and pad/truncate\n",
    "            phonetic_features = torch.tensor(phonetic_features, dtype=torch.long)\n",
    "            if len(phonetic_features) < self.feature_length:\n",
    "                phonetic_features = F.pad(phonetic_features, (0, self.feature_length - len(phonetic_features)))\n",
    "            else:\n",
    "                phonetic_features = phonetic_features[:self.feature_length]\n",
    "\n",
    "            audio = torch.tensor(audio, dtype=torch.float32).unsqueeze(0)  # [B,1,T]\n",
    "            # features is [feature_length], long tensor of phoneme IDs\n",
    "            return audio, phonetic_features\n",
    "        except Exception as e:\n",
    "            # If any error occurs, return a dummy sample or raise\n",
    "            logger.info(f\"Error reading {file}: {e}\")\n",
    "            # Return dummy data (should rarely happen)\n",
    "            dummy_audio = torch.zeros((1, self.target_length), dtype=torch.float32)\n",
    "            dummy_features = torch.zeros(self.feature_length, dtype=torch.long)\n",
    "            return dummy_audio, dummy_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################\n",
    "# Models\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, upsample_factor):\n",
    "        super(UpsampleNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.upsample_factor = upsample_factor  # Add this line\n",
    "\n",
    "        layer = nn.ConvTranspose1d(input_size, output_size, upsample_factor * 2,\n",
    "                                   upsample_factor, padding=upsample_factor // 2)\n",
    "        nn.init.orthogonal_(layer.weight)\n",
    "        self.layer = spectral_norm(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.layer(inputs)\n",
    "        outputs = outputs[:, :, : inputs.size(-1) * self.upsample_factor]\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class GBlock(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, z_channels, upsample_factor):\n",
    "        super(GBlock, self).__init__()\n",
    "        self.condition_norm1 = nn.GroupNorm(32, in_channels)\n",
    "        self.first_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            UpsampleNet(in_channels, in_channels, upsample_factor),\n",
    "            nn.Conv1d(in_channels, hidden_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.condition_norm2 = nn.GroupNorm(32, hidden_channels)\n",
    "        self.second_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, dilation=1, padding=1)\n",
    "        )\n",
    "        self.residual1 = nn.Sequential(\n",
    "            UpsampleNet(in_channels, in_channels, upsample_factor),\n",
    "            nn.Conv1d(in_channels, hidden_channels, kernel_size=1)\n",
    "        )\n",
    "        self.condition_norm3 = nn.GroupNorm(32, hidden_channels)\n",
    "        self.third_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        )\n",
    "        self.condition_norm4 = nn.GroupNorm(32, hidden_channels)\n",
    "        self.fourth_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, condition, z):\n",
    "        def run_forward(inputs):\n",
    "            outputs = self.condition_norm1(inputs)\n",
    "            outputs = self.first_stack(outputs)\n",
    "            outputs = self.condition_norm2(outputs)\n",
    "            outputs = self.second_stack(outputs)\n",
    "            residual_outputs = self.residual1(inputs) + outputs\n",
    "            outputs = self.condition_norm3(residual_outputs)\n",
    "            outputs = self.third_stack(outputs)\n",
    "            outputs = self.condition_norm4(outputs)\n",
    "            outputs = self.fourth_stack(outputs)\n",
    "            outputs = outputs + residual_outputs\n",
    "            return outputs\n",
    "        outputs = checkpoint.checkpoint(run_forward, condition)\n",
    "        return outputs\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=64, z_channels=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        nn.init.normal_(self.embedding.weight, 0.0, 0.1)\n",
    "\n",
    "        self.preprocess = nn.Conv1d(embedding_dim, 768, kernel_size=3, padding=1)\n",
    "        self.gblocks = nn.ModuleList([\n",
    "            GBlock(768, 768, z_channels, 5),\n",
    "            GBlock(768, 768, z_channels, 4),\n",
    "            GBlock(768, 384, z_channels, 4),\n",
    "            GBlock(384, 384, z_channels, 4),\n",
    "            GBlock(384, 192, z_channels, 2),\n",
    "        ])\n",
    "        self.postprocess = nn.Sequential(\n",
    "            nn.Conv1d(192, 1, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, conditions_emb, z):\n",
    "        # conditions_emb: [B, embedding_dim, T]\n",
    "        outputs = self.preprocess(conditions_emb)\n",
    "        for layer in self.gblocks:\n",
    "            outputs = layer(outputs, z)\n",
    "        outputs = self.postprocess(outputs)\n",
    "        return outputs\n",
    "\n",
    "class DBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample_factor):\n",
    "        super(DBlock, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.AvgPool1d(downsample_factor, stride=downsample_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        )\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1),\n",
    "            nn.AvgPool1d(downsample_factor, stride=downsample_factor)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.layers(inputs) + self.residual(inputs)\n",
    "        return outputs\n",
    "\n",
    "class CondDBlock(nn.Module):\n",
    "    def __init__(self, in_channels, lc_channels, upsample_factor):\n",
    "        super(CondDBlock, self).__init__()\n",
    "        self.lc_conv1d = nn.Conv1d(lc_channels, in_channels, kernel_size=1)\n",
    "        self.start = nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.end = nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.residual = nn.Conv1d(in_channels, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, inputs, conditions):\n",
    "        conditions = self.lc_conv1d(conditions)\n",
    "        outputs = self.start(inputs) + conditions\n",
    "        outputs = self.end(outputs)\n",
    "        residual_outputs = self.residual(inputs)\n",
    "        return outputs + residual_outputs\n",
    "\n",
    "class ConditionalDBlocks(nn.Module):\n",
    "    def __init__(self, in_channels, lc_channels, factors=(2,2,2), out_channels=(128,256)):\n",
    "        super(ConditionalDBlocks, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.lc_channels = lc_channels\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(DBlock(in_channels, 64, 1))\n",
    "        in_channels = 64\n",
    "        for i, channel in enumerate(out_channels):\n",
    "            self.layers.append(DBlock(in_channels, channel, factors[i]))\n",
    "            in_channels = channel\n",
    "        self.cond_layer = CondDBlock(in_channels, lc_channels, factors[-1])\n",
    "        self.adjust_channels = nn.Conv1d(in_channels, 512, kernel_size=1)\n",
    "        self.post_process = nn.ModuleList([DBlock(512, 512, 1), DBlock(512, 512, 1)])\n",
    "\n",
    "    def forward(self, inputs, conditions):\n",
    "        batch_size = inputs.size(0)\n",
    "        outputs = inputs.view(batch_size, self.in_channels, -1)\n",
    "        for layer in self.layers:\n",
    "            outputs = layer(outputs)\n",
    "        # conditions: [B, lc_channels, T'] -> pool to 1 then expand\n",
    "        conditions_pooled = F.adaptive_avg_pool1d(conditions, 1)\n",
    "        conditions_pooled = conditions_pooled.expand(-1, self.lc_channels, outputs.size(-1))\n",
    "        outputs = self.cond_layer(outputs, conditions_pooled)\n",
    "        outputs = self.adjust_channels(outputs)\n",
    "        for layer in self.post_process:\n",
    "            outputs = layer(outputs)\n",
    "        return outputs\n",
    "\n",
    "class UnConditionalDBlocks(nn.Module):\n",
    "    def __init__(self, in_channels, factors=(5, 3), out_channels=(128, 256)):\n",
    "        super(UnConditionalDBlocks, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(DBlock(in_channels, 64, 1))\n",
    "        in_channels = 64\n",
    "        for (i, factor) in enumerate(factors):\n",
    "            self.layers.append(DBlock(in_channels, out_channels[i], factor))\n",
    "            in_channels = out_channels[i]\n",
    "        self.layers.append(DBlock(in_channels, in_channels, 1))\n",
    "        self.layers.append(DBlock(in_channels, in_channels, 1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        outputs = inputs.view(batch_size, self.in_channels, -1)\n",
    "        for layer in self.layers:\n",
    "            outputs = layer(outputs)\n",
    "        return outputs\n",
    "\n",
    "class Multiple_Random_Window_Discriminators(nn.Module):\n",
    "    def __init__(self, lc_channels, window_size=(2,4,8,16,30), upsample_factor=120):\n",
    "        super(Multiple_Random_Window_Discriminators, self).__init__()\n",
    "        self.lc_channels = lc_channels\n",
    "        self.window_size = window_size\n",
    "        self.upsample_factor = upsample_factor\n",
    "\n",
    "        self.udiscriminators = nn.ModuleList([\n",
    "            UnConditionalDBlocks(in_channels=1, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=2, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=4, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=8, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=15, factors=(2, 2), out_channels=(128, 256)),\n",
    "        ])\n",
    "\n",
    "        self.discriminators = nn.ModuleList([\n",
    "            ConditionalDBlocks(in_channels=1, lc_channels=lc_channels,\n",
    "                               factors=(5, 3, 2, 2, 2), out_channels=(128, 128, 256, 256)),\n",
    "            ConditionalDBlocks(in_channels=2, lc_channels=lc_channels,\n",
    "                               factors=(5, 3, 2, 2), out_channels=(128, 256, 256)),\n",
    "            ConditionalDBlocks(in_channels=4, lc_channels=lc_channels,\n",
    "                               factors=(5, 3, 2), out_channels=(128, 256)),\n",
    "            ConditionalDBlocks(in_channels=8, lc_channels=lc_channels,\n",
    "                               factors=(5, 3), out_channels=(256,)),\n",
    "            ConditionalDBlocks(in_channels=15, lc_channels=lc_channels,\n",
    "                               factors=(2, 2, 2), out_channels=(128, 256)),\n",
    "        ])\n",
    "\n",
    "    def forward(self, real_samples, fake_samples, conditions):\n",
    "        real_outputs, fake_outputs = [], []\n",
    "        # Unconditional\n",
    "        for (size, layer) in zip(self.window_size, self.udiscriminators):\n",
    "            size = size * self.upsample_factor\n",
    "            index = np.random.randint(0, real_samples.size(-1) - size + 1)\n",
    "            real_slice = real_samples[:, :, index: index + size]\n",
    "            fake_slice = fake_samples[:, :, index: index + size]\n",
    "            real_output = layer(real_slice)\n",
    "            fake_output = layer(fake_slice)\n",
    "            real_outputs.append(real_output)\n",
    "            fake_outputs.append(fake_output)\n",
    "\n",
    "        # Conditional\n",
    "        for (size, layer) in zip(self.window_size, self.discriminators):\n",
    "            lc_index = np.random.randint(0, conditions.size(-1) - size + 1)\n",
    "            sample_index = lc_index * self.upsample_factor\n",
    "            real_x = real_samples[:, :, sample_index: (lc_index + size)*self.upsample_factor]\n",
    "            fake_x = fake_samples[:, :, sample_index: (lc_index + size)*self.upsample_factor]\n",
    "            lc = conditions[:, :, lc_index: lc_index + size]\n",
    "            real_output = layer(real_x, lc)\n",
    "            fake_output = layer(fake_x, lc)\n",
    "            real_outputs.append(real_output)\n",
    "            fake_outputs.append(fake_output)\n",
    "\n",
    "        return real_outputs, fake_outputs\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, audio_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.audio_length = audio_length\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# -------------------------------------------\n",
    "# Preprocessing code\n",
    "# -------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(root_dir, preprocessed_dir, target_sr=16000, target_length=64000, feature_length=100):\n",
    "    \"\"\"\n",
    "    Preprocess all .flac files found in root_dir. Saves the results in preprocessed_dir as .pt files.\n",
    "    Each file will contain a dictionary with:\n",
    "    {\n",
    "        'audio': torch.Tensor [1, T],\n",
    "        'phonemes': torch.Tensor [feature_length]\n",
    "    }\n",
    "\n",
    "    Also builds phoneme_to_id by scanning all files first.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(preprocessed_dir):\n",
    "        os.makedirs(preprocessed_dir)\n",
    "\n",
    "    flac_files = glob(os.path.join(root_dir, '**', '*.flac'), recursive=True)\n",
    "    if len(flac_files) == 0:\n",
    "        raise ValueError(\"No .flac files found.\")\n",
    "\n",
    "    # Collect all phonemes globally\n",
    "    all_phonemes = set()\n",
    "    for f in flac_files:\n",
    "        try:\n",
    "            transcription = get_transcription(f)\n",
    "            phonemes = phonemize(transcription, backend=\"espeak\", language=\"en-us\")\n",
    "            all_phonemes.update(list(phonemes))\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error processing {f}: {e}\")\n",
    "\n",
    "    phoneme_to_id = {p: i for i, p in enumerate(sorted(all_phonemes))}\n",
    "    logger.info(f\"Phoneme vocabulary size: {len(phoneme_to_id)}\")\n",
    "\n",
    "    # Save phoneme_to_id as a dictionary\n",
    "    torch.save(phoneme_to_id, os.path.join(preprocessed_dir, 'phoneme_to_id.pt'))\n",
    "\n",
    "    # Preprocess each file, free memory aggressively after each iteration\n",
    "    for idx, f in enumerate(flac_files):\n",
    "        try:\n",
    "            audio = load_flac(f, target_sr)\n",
    "            audio = pad_or_trim(audio, target_length)\n",
    "            transcription = get_transcription(f)\n",
    "            if not transcription or not transcription.strip():\n",
    "                raise ValueError(f\"Empty transcription for file {f}\")\n",
    "\n",
    "            phonemes = phonemize(transcription, backend=\"espeak\", language=\"en-us\")\n",
    "            phonetic_features = [phoneme_to_id[p] for p in phonemes]\n",
    "            phonetic_features = torch.tensor(phonetic_features, dtype=torch.long)\n",
    "            if len(phonetic_features) < feature_length:\n",
    "                phonetic_features = F.pad(phonetic_features, (0, feature_length - len(phonetic_features)))\n",
    "            else:\n",
    "                phonetic_features = phonetic_features[:feature_length]\n",
    "\n",
    "            audio_tensor = torch.tensor(audio, dtype=torch.float32).unsqueeze(0)\n",
    "            sample_data = {\n",
    "                'audio': audio_tensor,\n",
    "                'phonemes': phonetic_features\n",
    "            }\n",
    "\n",
    "            base_name = os.path.splitext(os.path.basename(f))[0]\n",
    "            save_path = os.path.join(preprocessed_dir, f\"{base_name}.pt\")\n",
    "            torch.save(sample_data, save_path)\n",
    "\n",
    "            # Free memory associated with this iteration\n",
    "            del sample_data, audio_tensor, phonetic_features, audio, phonemes\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error pre-processing {f}: {e}\")\n",
    "            # Optionally, you can also try to free memory here if something went wrong\n",
    "            gc.collect()\n",
    "\n",
    "    # After all preprocessing is done, a final GC call\n",
    "    gc.collect()\n",
    "\n",
    "class MyAudioDataset(Dataset):\n",
    "    def __init__(self, preprocessed_dir):\n",
    "        \"\"\"\n",
    "        preprocessed_dir should contain:\n",
    "          - phoneme_to_id.pt (phoneme dictionary)\n",
    "          - multiple .pt files each with {'audio', 'phonemes'}\n",
    "        \"\"\"\n",
    "        self.preprocessed_dir = preprocessed_dir\n",
    "        self.files = glob(os.path.join(preprocessed_dir, '*.pt'))\n",
    "        # Remove phoneme_to_id.pt from the list\n",
    "        self.files = [f for f in self.files if not f.endswith('phoneme_to_id.pt')]\n",
    "        if len(self.files) == 0:\n",
    "            raise ValueError(\"No preprocessed .pt files found in directory.\")\n",
    "\n",
    "        self.phoneme_to_id = torch.load(os.path.join(preprocessed_dir, 'phoneme_to_id.pt'))\n",
    "        self.vocab_size = len(self.phoneme_to_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.files[idx]\n",
    "        # Load data and immediately discard what's not needed\n",
    "        data = torch.load(path, map_location='cpu')  # Ensure CPU loading, no GPU memory\n",
    "        audio = data['audio']       # [1, T]\n",
    "        phonemes = data['phonemes'] # [feature_length]\n",
    "\n",
    "        # Remove reference to data dictionary to free memory\n",
    "        del data\n",
    "        gc.collect()\n",
    "\n",
    "        # If you REALLY need to free disk space and don't plan to reuse the dataset:\n",
    "        # WARNING: This will remove the file and you can't rerun the dataset again.\n",
    "        # os.remove(path)\n",
    "\n",
    "        return audio, phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###################################\n",
    "# Training Functions\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pretrain_single_generator(\n",
    "    num_epochs, z_dim, lr_gen, lr_disc, batch_size, seed,\n",
    "    audio_length, output_dir, train_dataset, checkpoint_path=\"checkpoint.pth\",\n",
    "    resume=False, vocab_size=100, embedding_dim=64\n",
    "):\n",
    "    set_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    phoneme_embedding = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "    nn.init.normal_(phoneme_embedding.weight, 0.0, 0.1)\n",
    "\n",
    "    generator = Generator(vocab_size=vocab_size, embedding_dim=embedding_dim, z_channels=z_dim).to(device)\n",
    "    discriminator = Multiple_Random_Window_Discriminators(lc_channels=embedding_dim).to(device)\n",
    "\n",
    "    optimizer_gen = optim.Adam(generator.parameters(), lr=lr_gen, betas=(0.5, 0.9))\n",
    "    optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "    scaler_gen = GradScaler()\n",
    "    scaler_disc = GradScaler()\n",
    "\n",
    "    start_epoch = 0\n",
    "    if resume and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        optimizer_gen.load_state_dict(checkpoint['optimizer_gen_state_dict'])\n",
    "        optimizer_disc.load_state_dict(checkpoint['optimizer_disc_state_dict'])\n",
    "        scaler_gen.load_state_dict(checkpoint['scaler_gen'])\n",
    "        scaler_disc.load_state_dict(checkpoint['scaler_disc'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        logger.info(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        for batch_idx, (real_audio, features_ids) in enumerate(train_loader):\n",
    "            real_audio = real_audio.to(device)\n",
    "            features_ids = features_ids.to(device) \n",
    "            # Embed features\n",
    "            features_emb = phoneme_embedding(features_ids).transpose(1, 2)\n",
    "\n",
    "            # Train Discriminator\n",
    "            for _ in range(5):\n",
    "                optimizer_disc.zero_grad()\n",
    "                noise = generate_noise(real_audio.size(0), z_dim, device)\n",
    "                with autocast(device_type='cuda'):\n",
    "                    fake_audio = generator(features_emb, noise).detach()\n",
    "                    real_outputs, fake_outputs = discriminator(real_audio, fake_audio, features_emb)\n",
    "                    \n",
    "                    loss_disc = sum(torch.mean(f) - torch.mean(r) for r, f in zip(real_outputs, fake_outputs))\n",
    "                    gradient_penalty = compute_gradient_penalty(discriminator, real_audio, fake_audio, features_emb, device)\n",
    "                    loss_disc += 10 * gradient_penalty\n",
    "\n",
    "                scaler_disc.scale(loss_disc).backward(retain_graph=True)\n",
    "                scaler_disc.step(optimizer_disc)\n",
    "                scaler_disc.update()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_gen.zero_grad()\n",
    "            noise = generate_noise(real_audio.size(0), z_dim, device)\n",
    "            with autocast(device_type='cuda'):\n",
    "                fake_audio = generator(features_emb, noise)\n",
    "                _, fake_outputs = discriminator(fake_audio, fake_audio, features_emb)\n",
    "                fake_outputs = pad_tensors_to_match(fake_outputs)\n",
    "                loss_gen = -torch.mean(fake_outputs)\n",
    "\n",
    "            scaler_gen.scale(loss_gen).backward(retain_graph=True)\n",
    "            scaler_gen.step(optimizer_gen)\n",
    "            scaler_gen.update()\n",
    "\n",
    "        logger.info(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {loss_disc.item():.4f}, Loss G: {loss_gen.item():.4f}\")\n",
    "\n",
    "        visualize_and_save_generated_waveforms([generator], z_dim, features_emb, num_waveforms=5, device=device, epoch=epoch, sample_rate=16000, output_dir=output_dir)\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch+1,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_gen_state_dict': optimizer_gen.state_dict(),\n",
    "            'optimizer_disc_state_dict': optimizer_disc.state_dict(),\n",
    "            'scaler_gen': scaler_gen.state_dict(),\n",
    "            'scaler_disc': scaler_disc.state_dict(),\n",
    "            'z_dim': z_dim,\n",
    "            'lr_gen': lr_gen,\n",
    "            'lr_disc': lr_disc,\n",
    "            'batch_size': batch_size,\n",
    "            'seed': seed,\n",
    "            'audio_length': audio_length,\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        logger.info(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "        torch.save(generator.state_dict(), os.path.join(output_dir, f\"pretrained_generator_epoch{epoch+1}.pth\"))\n",
    "\n",
    "    logger.info(\"Pretraining complete.\")\n",
    "    return generator, phoneme_embedding\n",
    "\n",
    "def train_gan_with_pretrained_generators(\n",
    "    pretrained_generator, phoneme_embedding,\n",
    "    num_epochs, z_dim, lr_gen, lr_disc, batch_size, train_dataset,\n",
    "    num_generators, seed, audio_length, output_dir, checkpoint_path=\"multi_gan_checkpoint.pth\",\n",
    "    resume=False\n",
    "):\n",
    "    set_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    generators = []\n",
    "    for _ in range(num_generators):\n",
    "        gen = Generator(vocab_size=phoneme_embedding.num_embeddings, embedding_dim=phoneme_embedding.embedding_dim, z_channels=z_dim).to(device)\n",
    "        gen.load_state_dict(pretrained_generator.state_dict())\n",
    "        generators.append(gen)\n",
    "\n",
    "    discriminator = Multiple_Random_Window_Discriminators(lc_channels=phoneme_embedding.embedding_dim).to(device)\n",
    "    encoder = Encoder(audio_length).to(device)\n",
    "\n",
    "    optimizer_gens = [optim.Adam(gen.parameters(), lr=lr_gen, betas=(0.5, 0.9)) for gen in generators]\n",
    "    optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "    optimizer_encoder = optim.Adam(encoder.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "    scaler_gens = [GradScaler() for _ in range(num_generators)]\n",
    "    scaler_disc = GradScaler()\n",
    "    scaler_encoder = GradScaler()\n",
    "\n",
    "    start_epoch = 0\n",
    "    if resume and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        for idx, gen in enumerate(generators):\n",
    "            gen.load_state_dict(checkpoint['generator_state_dicts'][idx])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "        for idx, opt_gen in enumerate(optimizer_gens):\n",
    "            opt_gen.load_state_dict(checkpoint['optimizer_gen_state_dicts'][idx])\n",
    "        optimizer_disc.load_state_dict(checkpoint['optimizer_disc_state_dict'])\n",
    "        optimizer_encoder.load_state_dict(checkpoint['optimizer_encoder_state_dict'])\n",
    "        logger.info(f\"Resumed training from epoch {start_epoch}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=False)\n",
    "\n",
    "    lambda_gp = 10\n",
    "    lambda_ortho = 0.1\n",
    "    num_critic = 5\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        for batch_idx, (real_audio, features_ids) in enumerate(train_loader):\n",
    "            real_audio = real_audio.to(device)\n",
    "            features_ids = features_ids.to(device)\n",
    "            features_emb = phoneme_embedding(features_ids).transpose(1,2)\n",
    "\n",
    "            # Train Discriminator\n",
    "            for _ in range(num_critic):\n",
    "                optimizer_disc.zero_grad()\n",
    "                noises = [generate_noise(real_audio.size(0), z_dim, device) for _ in range(num_generators)]\n",
    "                with autocast(device_type='cuda'):\n",
    "                    fakes = [gen(features_emb, noises[i]).detach() for i, gen in enumerate(generators)]\n",
    "                    real_outputs, fake_outputs = discriminator(real_audio, fakes[0], features_emb)\n",
    "                    loss_disc = sum(torch.mean(fake) - torch.mean(real) for real, fake in zip(real_outputs, fake_outputs))\n",
    "                    gradient_penalty = compute_gradient_penalty(discriminator, real_audio, fakes[0], features_emb, device)\n",
    "                    loss_disc += lambda_gp * gradient_penalty\n",
    "\n",
    "                scaler_disc.scale(loss_disc).backward()\n",
    "                scaler_disc.step(optimizer_disc)\n",
    "                scaler_disc.update()\n",
    "\n",
    "            # Train Generators + Encoder\n",
    "            for idx, gen in enumerate(generators):\n",
    "                optimizer_gens[idx].zero_grad()\n",
    "                optimizer_encoder.zero_grad()\n",
    "\n",
    "                noise = generate_noise(real_audio.size(0), z_dim, device)\n",
    "                with autocast(device_type='cuda'):\n",
    "                    fake = gen(features_emb, noise)\n",
    "                    fake_outputs = discriminator(fake, fake, features_emb)[1]\n",
    "                    fake_outputs = pad_tensors_to_match(fake_outputs)\n",
    "                    loss_gen = -torch.mean(fake_outputs)\n",
    "\n",
    "                    # Orthogonal Loss\n",
    "                    gen_feature = encoder(fake)\n",
    "                    ortho_loss_val = 0\n",
    "                    for other_idx, other_gen in enumerate(generators):\n",
    "                        if idx != other_idx:\n",
    "                            other_noise = generate_noise(real_audio.size(0), z_dim, device)\n",
    "                            other_fake = other_gen(features_emb, other_noise)\n",
    "                            other_feature = encoder(other_fake)\n",
    "                            ortho_loss_val += orthogonal_loss(gen_feature, other_feature)\n",
    "                    ortho_loss_val /= (num_generators - 1)\n",
    "                    total_loss_gen = loss_gen + lambda_ortho * ortho_loss_val\n",
    "\n",
    "                scaler_gens[idx].scale(total_loss_gen).backward()\n",
    "                scaler_gens[idx].step(optimizer_gens[idx])\n",
    "                scaler_gens[idx].update()\n",
    "                scaler_encoder.step(optimizer_encoder)\n",
    "                scaler_encoder.update()\n",
    "\n",
    "        logger.info(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {loss_disc.item():.4f}, Loss G: {loss_gen.item():.4f}\")\n",
    "\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'generator_state_dicts': [gen.state_dict() for gen in generators],\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'encoder_state_dict': encoder.state_dict(),\n",
    "            'optimizer_gen_state_dicts': [opt.state_dict() for opt in optimizer_gens],\n",
    "            'optimizer_disc_state_dict': optimizer_disc.state_dict(),\n",
    "            'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        logger.info(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    logger.info(\"Training complete.\")\n",
    "    return generators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################\n",
    "# Main Execution\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 14:17:32,692 - INFO - Logging system initialized\n",
      "2024-12-13 14:17:32,692 - INFO - Logging system initialized\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# Import necessary modules and classes\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming the previously defined functions and classes are in a module named 'gan_module'\n",
    "# from gan_module import (\n",
    "#     set_seed, preprocess_data, MyAudioDataset,\n",
    "#     pretrain_single_generator, train_gan_with_pretrained_generators,\n",
    "#     clear_all_memory\n",
    "# )\n",
    "# For this example, we'll assume they are defined in the same script.\n",
    "\n",
    "# -------------------------------------------\n",
    "# Logging Configuration\n",
    "# -------------------------------------------\n",
    "\n",
    "# Configure logging to file\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    filename=\"training_500.log\",  # Specify a file to write logs to\n",
    "    filemode=\"a\"  # Append mode\n",
    ")\n",
    "\n",
    "# Create a console handler to also log INFO logs to the console\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the console handler to the root logger\n",
    "logging.getLogger('').addHandler(console_handler)\n",
    "\n",
    "# Now you can use logging throughout your code\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Example usage:\n",
    "logger.info(\"Logging system initialized\")\n",
    "\n",
    "# -------------------------------------------\n",
    "# Hyperparameters and Configuration\n",
    "# -------------------------------------------\n",
    "\n",
    "set_seed(42)\n",
    "audio_length = 64000\n",
    "z_dim = 128\n",
    "lr_gen = 0.0002\n",
    "lr_disc = 0.0002\n",
    "batch_size = 10\n",
    "num_epochs = 50\n",
    "root_dir = \"./data_small\"\n",
    "preprocessed_dir = \"./output/preprocessed_data_small\"  # Directory to store preprocessed data\n",
    "sample_rate = 16000\n",
    "num_generators = 5\n",
    "output_dir = 'output/generated_audio_small'\n",
    "feature_length = 100  # Ensure consistency with preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 14:13:09,593 - INFO - Starting preprocessing of data...\n",
      "2024-12-13 14:13:09,628 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:09,741 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:09,766 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:09,789 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:09,812 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:09,835 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:09,971 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:10,114 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:10,138 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:10,180 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:10,205 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:10,253 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:10,276 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:10,364 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:10,412 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:10,436 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:10,460 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:10,484 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:10,616 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:10,642 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:10,666 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:10,692 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:10,717 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:10,740 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:10,921 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:10,967 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:11,013 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:11,056 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:11,144 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:11,210 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:11,380 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:11,422 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:11,445 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:11,468 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:11,558 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:11,604 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:11,627 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:11,741 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:11,969 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:12,039 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:12,060 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:12,177 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:12,221 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:12,366 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:12,390 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:12,546 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:12,610 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:12,634 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:12,656 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:12,678 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:12,744 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:12,767 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:12,789 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:12,812 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:12,835 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:12,859 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:12,883 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:12,905 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:13,021 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:13,041 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:13,068 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:13,094 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:13,143 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:13,166 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:13,257 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:13,327 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:13,489 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:13,557 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:13,581 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:13,604 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:13,652 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:13,722 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:13,769 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:14,053 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:14,080 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:14,126 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:14,150 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:14,248 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:14,393 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:14,417 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:14,441 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:14,491 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:14,517 - INFO - Error processing ./data_small/LibriSpeech/LibriSpeech/dev-clean/2412/153947/2412-153947-0001.flac: No transcription file found in ./data_small/LibriSpeech/LibriSpeech/dev-clean/2412/153947\n",
      "2024-12-13 14:13:14,517 - INFO - Error processing ./data_small/LibriSpeech/LibriSpeech/dev-clean/2412/153947/2412-153947-0000.flac: No transcription file found in ./data_small/LibriSpeech/LibriSpeech/dev-clean/2412/153947\n",
      "2024-12-13 14:13:14,710 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:14,734 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:14,825 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:14,918 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:15,096 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:15,142 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:15,213 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:15,305 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:15,352 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:15,403 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:15,506 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:15,530 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:15,578 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:15,652 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:15,704 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:15,729 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:15,756 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:15,778 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:15,801 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:15,872 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:15,898 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:15,966 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:15,992 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:16,085 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:16,112 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:16,136 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:16,161 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:16,184 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:16,207 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:16,414 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:16,670 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:16,737 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:16,880 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:16,992 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:17,276 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:17,345 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:17,436 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:17,459 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:17,482 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:17,551 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:17,577 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:17,723 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:17,818 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:17,841 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:17,863 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:17,887 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:18,024 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:18,091 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:18,201 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:18,225 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:18,250 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:18,321 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:18,369 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:18,395 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:18,605 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:18,632 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:18,707 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:18,780 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:18,804 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:18,852 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:18,879 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:18,954 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:18,980 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,031 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,057 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,108 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,160 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,184 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,209 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,233 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,258 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,282 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,333 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,358 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,384 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,408 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,433 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,459 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,506 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,557 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,607 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,632 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,834 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:19,928 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:20,051 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:20,168 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:20,193 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:20,214 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:20,238 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:20,358 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:20,382 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:20,410 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:20,511 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:20,689 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:20,713 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:20,762 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:20,786 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:20,839 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:20,916 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:20,986 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:21,033 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:21,128 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:21,176 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:21,200 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:21,252 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:21,280 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:21,378 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:21,452 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:21,525 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:21,794 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:21,891 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:21,915 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:21,985 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:22,028 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:22,149 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:22,290 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:22,392 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:22,445 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:22,490 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:22,533 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:22,561 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:22,585 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:22,636 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:22,733 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:22,782 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:22,835 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:22,888 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:22,960 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:22,984 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:23,059 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:23,106 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:23,179 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:23,204 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:23,228 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:23,322 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:23,542 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:23,571 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:23,670 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:23,693 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:23,718 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:23,766 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:23,819 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:23,846 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:23,872 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:23,984 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,012 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,039 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,068 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,094 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,119 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,142 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,169 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,192 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,218 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,243 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,267 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,344 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,370 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,395 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,450 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,477 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,505 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,558 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,582 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,607 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,633 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,660 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,713 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,738 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,761 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,788 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:24,984 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:25,089 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:25,142 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:25,192 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:25,219 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:25,243 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:25,268 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:25,315 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:25,342 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:25,367 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:25,392 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:25,419 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:25,636 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:25,663 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:25,761 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:25,811 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:25,837 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:25,885 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:26,061 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:26,087 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:26,138 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:26,165 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:26,224 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:26,250 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:26,301 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:26,324 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:26,375 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:26,400 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:26,525 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:26,548 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:26,597 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:26,623 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:26,649 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:26,676 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:26,726 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:26,851 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:26,973 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:26,999 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:27,025 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:27,077 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:27,153 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:27,178 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:27,204 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:27,282 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:27,355 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:27,433 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:27,459 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:27,538 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:27,592 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:27,747 - INFO - Phoneme vocabulary size: 44\n",
      "2024-12-13 14:13:28,432 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:29,179 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:29,313 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:29,448 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:29,587 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:29,755 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:30,602 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:31,506 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:31,648 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:31,931 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:32,079 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:32,378 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:32,530 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:33,117 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:33,426 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:33,581 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:33,733 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:33,876 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:34,279 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:34,432 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:34,589 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:34,741 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:34,887 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:35,031 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:36,134 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:36,439 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:36,768 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:37,066 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:37,694 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:38,135 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:39,285 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:39,584 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:39,725 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:39,869 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:40,449 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:40,732 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:40,873 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:41,609 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:43,079 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:43,542 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:43,694 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:44,412 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:44,726 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:45,593 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:45,731 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:46,834 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:47,264 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:47,415 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:47,562 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:47,709 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:48,128 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:48,280 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:48,451 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:48,608 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:48,762 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:48,927 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:49,087 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:49,228 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:49,980 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:50,120 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:50,265 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:50,422 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:50,755 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:50,914 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:51,504 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:51,925 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:52,947 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:53,402 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:53,549 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:53,706 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:53,998 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:54,427 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:54,750 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:56,621 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:56,784 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:57,100 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:57,270 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:57,841 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:58,782 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:58,945 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:59,096 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:13:59,418 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:01,166 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:01,310 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:01,908 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:02,547 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:03,791 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:04,101 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:04,565 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:05,160 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:05,475 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:05,764 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:06,398 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:06,564 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:06,874 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:07,318 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:07,649 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:07,817 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:07,972 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:08,122 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:08,286 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:08,776 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:08,933 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:09,411 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:09,592 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:10,233 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:10,393 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:10,561 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:10,704 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:10,873 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:11,029 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:11,973 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:13,626 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:14,114 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:15,017 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:15,805 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:17,657 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:18,141 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:18,749 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:18,908 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:19,072 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:19,564 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:19,712 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:20,645 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:21,242 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:21,388 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:21,555 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:21,703 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:22,637 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:23,110 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:23,866 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:24,017 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:24,177 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:24,646 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:24,968 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:25,113 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:26,575 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:26,727 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:27,177 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:27,644 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:27,798 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:28,101 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:28,263 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:28,717 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:28,884 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:29,208 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:29,377 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:29,679 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:29,989 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:30,154 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:30,326 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:30,492 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:30,670 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:30,849 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:31,161 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:31,327 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:31,478 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:31,650 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:31,821 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:31,970 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:32,287 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:32,599 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:32,942 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:33,114 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:34,474 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:35,154 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:35,977 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:36,799 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:36,979 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:37,145 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:37,297 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:38,125 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:38,278 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:38,430 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:39,042 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:40,243 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:40,397 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:40,729 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:40,910 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:41,217 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:41,687 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:42,204 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:42,562 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:43,286 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:43,621 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:43,799 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:44,112 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:44,280 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:44,904 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:45,370 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:45,856 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:47,615 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:48,290 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:48,478 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:48,994 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:49,302 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:50,100 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:51,045 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:51,667 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:51,977 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:52,295 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:52,620 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:52,786 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:52,942 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:53,270 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:53,936 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:54,251 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:54,579 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:54,916 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:55,387 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:55,541 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:56,021 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:56,337 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:56,808 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:56,988 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:57,142 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:57,770 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:59,250 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:14:59,415 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:00,062 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:00,226 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:00,393 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:00,722 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:01,051 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:01,215 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:01,377 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:01,545 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:01,712 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:01,875 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:02,038 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:02,201 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:02,374 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:02,545 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:02,711 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:02,874 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:03,039 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:03,199 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:03,361 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:03,844 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:04,006 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:04,170 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:04,523 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:04,683 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:04,840 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:05,162 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:05,335 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:05,499 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:05,687 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:05,859 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:06,170 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:06,336 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:06,497 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:06,678 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:08,043 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:08,691 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:09,020 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:09,360 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:09,518 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:09,676 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:09,834 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:10,192 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:10,368 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:10,540 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:10,704 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:10,879 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:12,406 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:12,584 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:13,263 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:13,590 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:13,787 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:14,134 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:15,278 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:15,450 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:15,800 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:15,958 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:16,316 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:16,500 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:16,826 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:16,984 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:17,331 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:17,496 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:18,353 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:18,522 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:18,842 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:19,021 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:19,187 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:19,353 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:19,690 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:20,518 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:21,372 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:21,562 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:21,758 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:22,125 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:22,641 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:22,810 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:22,999 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:23,518 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:24,047 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:24,602 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:24,780 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:25,294 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:25,647 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-13 14:15:26,948 - INFO - Data preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Starting preprocessing of data...\")\n",
    "\n",
    "# Step 1: Preprocess the data\n",
    "preprocess_data(\n",
    "    root_dir=root_dir,\n",
    "    preprocessed_dir=preprocessed_dir,\n",
    "    target_sr=sample_rate,\n",
    "    target_length=audio_length,\n",
    "    feature_length=feature_length\n",
    ")\n",
    "logger.info(\"Data preprocessing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 14:16:42,735 - INFO - Loading preprocessed dataset...\n",
      "/tmp/ipykernel_610154/1803159625.py:88: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.phoneme_to_id = torch.load(os.path.join(preprocessed_dir, 'phoneme_to_id.pt'))\n",
      "2024-12-13 14:16:42,737 - INFO - Dataset loaded with 746 samples and vocabulary size 44.\n",
      "2024-12-13 14:16:42,737 - INFO - Clearing all memory caches...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU memory cache.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml/Documents/myenv/lib/python3.12/site-packages/torch/__init__.py:1021: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(obj, torch.Tensor)\n",
      "/tmp/ipykernel_610154/1384976981.py:125: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
      "/tmp/ipykernel_610154/1384976981.py:125: UserWarning: Code: data is not defined in namespace BRICK\n",
      "  if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
      "/tmp/ipykernel_610154/1384976981.py:125: UserWarning: Code: data is not defined in namespace DCAT\n",
      "  if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
      "/tmp/ipykernel_610154/1384976981.py:125: UserWarning: Code: data is not defined in namespace GEO\n",
      "  if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
      "/tmp/ipykernel_610154/1384976981.py:125: UserWarning: Code: data is not defined in namespace PROF\n",
      "  if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
      "/tmp/ipykernel_610154/1384976981.py:125: UserWarning: Code: data is not defined in namespace SDO\n",
      "  if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
      "/tmp/ipykernel_610154/1384976981.py:125: UserWarning: Code: data is not defined in namespace SOSA\n",
      "  if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
      "/tmp/ipykernel_610154/1384976981.py:125: UserWarning: Code: data is not defined in namespace SSN\n",
      "  if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
      "/tmp/ipykernel_610154/1384976981.py:125: UserWarning: Code: data is not defined in namespace TIME\n",
      "  if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
      "/tmp/ipykernel_610154/1384976981.py:125: UserWarning: Code: data is not defined in namespace WGS\n",
      "  if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
      "/tmp/ipykernel_610154/1384976981.py:125: UserWarning: Code: data is not defined in namespace XSD\n",
      "  if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
      "2024-12-13 14:16:43,270 - INFO - Memory caches cleared.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final GPU cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Create dataset from preprocessed data\n",
    "logger.info(\"Loading preprocessed dataset...\")\n",
    "train_dataset = MyAudioDataset(preprocessed_dir=preprocessed_dir)\n",
    "vocab_size = len(train_dataset.phoneme_to_id)\n",
    "logger.info(f\"Dataset loaded with {len(train_dataset)} samples and vocabulary size {vocab_size}.\")\n",
    "\n",
    "# Step 3: Clear any existing memory caches\n",
    "logger.info(\"Clearing all memory caches...\")\n",
    "clear_all_memory()\n",
    "logger.info(\"Memory caches cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 14:27:07,375 - INFO - Clearing all memory caches...\n",
      "2024-12-13 14:27:07,375 - INFO - Clearing all memory caches...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU memory cache.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_610154/1384976981.py:125: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
      "2024-12-13 14:27:07,908 - INFO - Memory caches cleared.\n",
      "2024-12-13 14:27:07,908 - INFO - Memory caches cleared.\n",
      "2024-12-13 14:27:07,909 - INFO - Starting pretraining of a single generator...\n",
      "2024-12-13 14:27:07,909 - INFO - Starting pretraining of a single generator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final GPU cleanup complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_610154/1803159625.py:97: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(path, map_location='cpu')  # Ensure CPU loading, no GPU memory\n",
      "/tmp/ipykernel_610154/1803159625.py:97: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(path, map_location='cpu')  # Ensure CPU loading, no GPU memory\n",
      "/home/ml/Documents/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Clearing all memory caches...\")\n",
    "clear_all_memory()\n",
    "logger.info(\"Memory caches cleared.\")\n",
    "# Step 4: Pretrain a single generator\n",
    "logger.info(\"Starting pretraining of a single generator...\")\n",
    "pretrained_generator, phoneme_embedding = pretrain_single_generator(\n",
    "    num_epochs=20,\n",
    "    z_dim=z_dim,\n",
    "    lr_gen=lr_gen,\n",
    "    lr_disc=lr_disc,\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    audio_length=audio_length,\n",
    "    output_dir='output/waveform_pre_linguistics_o1',\n",
    "    train_dataset=train_dataset,\n",
    "    checkpoint_path=\"output/gan_single_check.pth\",\n",
    "    embedding_dim=64,\n",
    "    vocab_size=vocab_size\n",
    ")\n",
    "logger.info(\"Pretraining of single generator completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train GAN with multiple pretrained generators\n",
    "logger.info(\"Starting GAN training with multiple pretrained generators...\")\n",
    "train_gan_with_pretrained_generators(\n",
    "    pretrained_generator=pretrained_generator,\n",
    "    phoneme_embedding=phoneme_embedding,\n",
    "    num_epochs=num_epochs,\n",
    "    z_dim=z_dim,\n",
    "    lr_gen=lr_gen,\n",
    "    lr_disc=lr_disc,\n",
    "    batch_size=batch_size,\n",
    "    train_dataset=train_dataset,\n",
    "    num_generators=num_generators,\n",
    "    seed=42,\n",
    "    audio_length=audio_length,\n",
    "    output_dir=output_dir,\n",
    "    checkpoint_path='output/my_checkpoints/multi_gan_checkpoint.pth',\n",
    "    resume=True\n",
    ")\n",
    "logger.info(\"GAN training with multiple generators completed.\")\n",
    "\n",
    "logger.info(\"All training processes have finished successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
