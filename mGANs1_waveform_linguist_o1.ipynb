{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from glob import glob\n",
    "from phonemizer import phonemize\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils import spectral_norm\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import logging\n",
    "from torch.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################\n",
    "# Utility and Setup Functions\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def load_flac(file_path, target_sr=16000):\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
    "    max_val = np.max(np.abs(audio))\n",
    "    if max_val > 0:\n",
    "        audio = audio / max_val  # Normalize to [-1, 1]\n",
    "    return audio\n",
    "\n",
    "def pad_or_trim(audio, target_length=64000):\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:target_length]\n",
    "    return audio\n",
    "\n",
    "def get_transcription(file_path):\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "    base_name = os.path.basename(file_path)\n",
    "    file_id = os.path.splitext(base_name)[0]\n",
    "    transcription_file = None\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith(\".trans.txt\"):\n",
    "            transcription_file = os.path.join(dir_path, file)\n",
    "            break\n",
    "    if not transcription_file:\n",
    "        raise FileNotFoundError(f\"No transcription file found in {dir_path}\")\n",
    "\n",
    "    with open(transcription_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\" \", 1)\n",
    "            if parts[0] == file_id:\n",
    "                transcription = parts[1]\n",
    "                return transcription\n",
    "    raise ValueError(f\"No transcription found for file {file_id}\")\n",
    "\n",
    "def save_waveform_to_audio(waveform, sample_rate, filename):\n",
    "    if isinstance(waveform, torch.Tensor):\n",
    "        waveform = waveform.detach().cpu().numpy()\n",
    "    waveform = np.squeeze(waveform)\n",
    "    max_val = np.max(np.abs(waveform))\n",
    "    if max_val > 0:\n",
    "        waveform = waveform / max_val\n",
    "    sf.write(filename, waveform, sample_rate)\n",
    "\n",
    "def generate_noise(batch_size, z_dim, device):\n",
    "    return torch.randn(batch_size, z_dim, device=device)\n",
    "\n",
    "def orthogonal_loss(feature1, feature2):\n",
    "    inner_product = torch.sum(feature1 * feature2, dim=1)\n",
    "    norm1 = torch.norm(feature1, dim=1)\n",
    "    norm2 = torch.norm(feature2, dim=1)\n",
    "    cosine_similarity = inner_product / (norm1 * norm2 + 1e-8)\n",
    "    return torch.mean(cosine_similarity**2)  # Minimize similarity to enforce orthogonality\n",
    "\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, conditions, device):\n",
    "    batch_size = real_samples.size(0)\n",
    "    epsilon = torch.rand(batch_size, 1, 1, device=device)\n",
    "    epsilon = epsilon.expand_as(real_samples)\n",
    "    interpolates = (epsilon * real_samples + (1 - epsilon) * fake_samples).requires_grad_(True)\n",
    "\n",
    "    real_outputs, fake_outputs = discriminator(interpolates, fake_samples, conditions)\n",
    "    # Assuming just one set of outputs for simplicity:\n",
    "    real_output = real_outputs[0]\n",
    "\n",
    "    grad_outputs = torch.ones_like(real_output, device=device)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=real_output,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=grad_outputs,\n",
    "        create_graph=True,    # Make sure this is True\n",
    "        retain_graph=True      # Consider adding this if needed\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "def visualize_and_save_generated_waveforms(generators, z_dim, features_emb, num_waveforms, device, epoch, sample_rate=16000, output_dir='generated_audio'):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for idx, gen in enumerate(generators):\n",
    "        gen.eval()\n",
    "        with torch.no_grad():\n",
    "            noise = generate_noise(num_waveforms, z_dim, device)\n",
    "            fake_waveforms = gen(features_emb[:num_waveforms], noise).cpu()\n",
    "            num_available_waveforms = fake_waveforms.size(0)\n",
    "            if num_available_waveforms < num_waveforms:\n",
    "                print(f\"Warning: Requested {num_waveforms} waveforms, got {num_available_waveforms}\")\n",
    "\n",
    "            for i in range(num_available_waveforms):\n",
    "                waveform = fake_waveforms[i]\n",
    "                filename = f'epoch{epoch+1}_gen{idx+1}_sample{i+1}.wav'\n",
    "                filepath = os.path.join(output_dir, filename)\n",
    "                save_waveform_to_audio(waveform, sample_rate, filepath)\n",
    "                print(f\"Saved {filepath}\")\n",
    "\n",
    "def pad_tensors_to_match(tensor_list):\n",
    "    # Pad a list of tensors along the last dimension to match their sizes\n",
    "    max_len = max(t.size(-1) for t in tensor_list)\n",
    "    max_channels = max(t.size(1) for t in tensor_list)\n",
    "    padded_tensors = []\n",
    "    for t in tensor_list:\n",
    "        diff_len = max_len - t.size(-1)\n",
    "        diff_channels = max_channels - t.size(1)\n",
    "        if diff_len > 0 or diff_channels > 0:\n",
    "            t = F.pad(t, (0, diff_len, 0, diff_channels))\n",
    "        padded_tensors.append(t)\n",
    "    return torch.stack(padded_tensors, dim=0)\n",
    "\n",
    "def clear_all_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"Cleared GPU memory cache.\")\n",
    "    gc.collect()\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
    "                del obj\n",
    "        except:\n",
    "            pass\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Final GPU cleanup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###################################\n",
    "# Custom Dataset\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, target_sr=16000, target_length=64000, feature_length=100):\n",
    "        self.root_dir = root_dir\n",
    "        self.target_sr = target_sr\n",
    "        self.target_length = target_length\n",
    "        self.feature_length = feature_length\n",
    "        self.flac_files = glob(os.path.join(root_dir, '**', '*.flac'), recursive=True)\n",
    "        print(f\"Found {len(self.flac_files)} .flac files in {root_dir}.\")\n",
    "        if len(self.flac_files) == 0:\n",
    "            raise ValueError(\"No .flac files found.\")\n",
    "        \n",
    "        # Build a global phoneme-to-id dictionary by scanning all files\n",
    "        all_phonemes = set()\n",
    "        for f in self.flac_files:\n",
    "            try:\n",
    "                transcription = get_transcription(f)\n",
    "                phonemes = phonemize(transcription, backend=\"espeak\", language=\"en-us\")\n",
    "                all_phonemes.update(list(phonemes))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {f}: {e}\")\n",
    "        \n",
    "        self.phoneme_to_id = {p: i for i, p in enumerate(sorted(all_phonemes))}\n",
    "        print(f\"Phoneme vocabulary size: {len(self.phoneme_to_id)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.flac_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.flac_files[idx]\n",
    "        try:\n",
    "            audio = load_flac(file, self.target_sr)\n",
    "            audio = pad_or_trim(audio, self.target_length)\n",
    "            transcription = get_transcription(file)\n",
    "            phonemes = phonemize(transcription, backend=\"espeak\", language=\"en-us\")\n",
    "            phonetic_features = [self.phoneme_to_id[p] for p in phonemes]\n",
    "\n",
    "            # Convert to tensor and pad/truncate\n",
    "            phonetic_features = torch.tensor(phonetic_features, dtype=torch.long)\n",
    "            if len(phonetic_features) < self.feature_length:\n",
    "                phonetic_features = F.pad(phonetic_features, (0, self.feature_length - len(phonetic_features)))\n",
    "            else:\n",
    "                phonetic_features = phonetic_features[:self.feature_length]\n",
    "\n",
    "            audio = torch.tensor(audio, dtype=torch.float32).unsqueeze(0)  # [B,1,T]\n",
    "            # features is [feature_length], long tensor of phoneme IDs\n",
    "            return audio, phonetic_features\n",
    "        except Exception as e:\n",
    "            # If any error occurs, return a dummy sample or raise\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "            # Return dummy data (should rarely happen)\n",
    "            dummy_audio = torch.zeros((1, self.target_length), dtype=torch.float32)\n",
    "            dummy_features = torch.zeros(self.feature_length, dtype=torch.long)\n",
    "            return dummy_audio, dummy_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################\n",
    "# Models\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, upsample_factor):\n",
    "        super(UpsampleNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.upsample_factor = upsample_factor  # Add this line\n",
    "\n",
    "        layer = nn.ConvTranspose1d(input_size, output_size, upsample_factor * 2,\n",
    "                                   upsample_factor, padding=upsample_factor // 2)\n",
    "        nn.init.orthogonal_(layer.weight)\n",
    "        self.layer = spectral_norm(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.layer(inputs)\n",
    "        outputs = outputs[:, :, : inputs.size(-1) * self.upsample_factor]\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class GBlock(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, z_channels, upsample_factor):\n",
    "        super(GBlock, self).__init__()\n",
    "        self.condition_norm1 = nn.GroupNorm(32, in_channels)\n",
    "        self.first_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            UpsampleNet(in_channels, in_channels, upsample_factor),\n",
    "            nn.Conv1d(in_channels, hidden_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.condition_norm2 = nn.GroupNorm(32, hidden_channels)\n",
    "        self.second_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, dilation=1, padding=1)\n",
    "        )\n",
    "        self.residual1 = nn.Sequential(\n",
    "            UpsampleNet(in_channels, in_channels, upsample_factor),\n",
    "            nn.Conv1d(in_channels, hidden_channels, kernel_size=1)\n",
    "        )\n",
    "        self.condition_norm3 = nn.GroupNorm(32, hidden_channels)\n",
    "        self.third_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        )\n",
    "        self.condition_norm4 = nn.GroupNorm(32, hidden_channels)\n",
    "        self.fourth_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, condition, z):\n",
    "        def run_forward(inputs):\n",
    "            outputs = self.condition_norm1(inputs)\n",
    "            outputs = self.first_stack(outputs)\n",
    "            outputs = self.condition_norm2(outputs)\n",
    "            outputs = self.second_stack(outputs)\n",
    "            residual_outputs = self.residual1(inputs) + outputs\n",
    "            outputs = self.condition_norm3(residual_outputs)\n",
    "            outputs = self.third_stack(outputs)\n",
    "            outputs = self.condition_norm4(outputs)\n",
    "            outputs = self.fourth_stack(outputs)\n",
    "            outputs = outputs + residual_outputs\n",
    "            return outputs\n",
    "        outputs = checkpoint.checkpoint(run_forward, condition)\n",
    "        return outputs\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=64, z_channels=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        nn.init.normal_(self.embedding.weight, 0.0, 0.1)\n",
    "\n",
    "        self.preprocess = nn.Conv1d(embedding_dim, 768, kernel_size=3, padding=1)\n",
    "        self.gblocks = nn.ModuleList([\n",
    "            GBlock(768, 768, z_channels, 5),\n",
    "            GBlock(768, 768, z_channels, 4),\n",
    "            GBlock(768, 384, z_channels, 4),\n",
    "            GBlock(384, 384, z_channels, 4),\n",
    "            GBlock(384, 192, z_channels, 2),\n",
    "        ])\n",
    "        self.postprocess = nn.Sequential(\n",
    "            nn.Conv1d(192, 1, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, conditions_emb, z):\n",
    "        # conditions_emb: [B, embedding_dim, T]\n",
    "        outputs = self.preprocess(conditions_emb)\n",
    "        for layer in self.gblocks:\n",
    "            outputs = layer(outputs, z)\n",
    "        outputs = self.postprocess(outputs)\n",
    "        return outputs\n",
    "\n",
    "class DBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample_factor):\n",
    "        super(DBlock, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.AvgPool1d(downsample_factor, stride=downsample_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        )\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1),\n",
    "            nn.AvgPool1d(downsample_factor, stride=downsample_factor)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.layers(inputs) + self.residual(inputs)\n",
    "        return outputs\n",
    "\n",
    "class CondDBlock(nn.Module):\n",
    "    def __init__(self, in_channels, lc_channels, upsample_factor):\n",
    "        super(CondDBlock, self).__init__()\n",
    "        self.lc_conv1d = nn.Conv1d(lc_channels, in_channels, kernel_size=1)\n",
    "        self.start = nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.end = nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.residual = nn.Conv1d(in_channels, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, inputs, conditions):\n",
    "        conditions = self.lc_conv1d(conditions)\n",
    "        outputs = self.start(inputs) + conditions\n",
    "        outputs = self.end(outputs)\n",
    "        residual_outputs = self.residual(inputs)\n",
    "        return outputs + residual_outputs\n",
    "\n",
    "class ConditionalDBlocks(nn.Module):\n",
    "    def __init__(self, in_channels, lc_channels, factors=(2,2,2), out_channels=(128,256)):\n",
    "        super(ConditionalDBlocks, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.lc_channels = lc_channels\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(DBlock(in_channels, 64, 1))\n",
    "        in_channels = 64\n",
    "        for i, channel in enumerate(out_channels):\n",
    "            self.layers.append(DBlock(in_channels, channel, factors[i]))\n",
    "            in_channels = channel\n",
    "        self.cond_layer = CondDBlock(in_channels, lc_channels, factors[-1])\n",
    "        self.adjust_channels = nn.Conv1d(in_channels, 512, kernel_size=1)\n",
    "        self.post_process = nn.ModuleList([DBlock(512, 512, 1), DBlock(512, 512, 1)])\n",
    "\n",
    "    def forward(self, inputs, conditions):\n",
    "        batch_size = inputs.size(0)\n",
    "        outputs = inputs.view(batch_size, self.in_channels, -1)\n",
    "        for layer in self.layers:\n",
    "            outputs = layer(outputs)\n",
    "        # conditions: [B, lc_channels, T'] -> pool to 1 then expand\n",
    "        conditions_pooled = F.adaptive_avg_pool1d(conditions, 1)\n",
    "        conditions_pooled = conditions_pooled.expand(-1, self.lc_channels, outputs.size(-1))\n",
    "        outputs = self.cond_layer(outputs, conditions_pooled)\n",
    "        outputs = self.adjust_channels(outputs)\n",
    "        for layer in self.post_process:\n",
    "            outputs = layer(outputs)\n",
    "        return outputs\n",
    "\n",
    "class UnConditionalDBlocks(nn.Module):\n",
    "    def __init__(self, in_channels, factors=(5, 3), out_channels=(128, 256)):\n",
    "        super(UnConditionalDBlocks, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(DBlock(in_channels, 64, 1))\n",
    "        in_channels = 64\n",
    "        for (i, factor) in enumerate(factors):\n",
    "            self.layers.append(DBlock(in_channels, out_channels[i], factor))\n",
    "            in_channels = out_channels[i]\n",
    "        self.layers.append(DBlock(in_channels, in_channels, 1))\n",
    "        self.layers.append(DBlock(in_channels, in_channels, 1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        outputs = inputs.view(batch_size, self.in_channels, -1)\n",
    "        for layer in self.layers:\n",
    "            outputs = layer(outputs)\n",
    "        return outputs\n",
    "\n",
    "class Multiple_Random_Window_Discriminators(nn.Module):\n",
    "    def __init__(self, lc_channels, window_size=(2,4,8,16,30), upsample_factor=120):\n",
    "        super(Multiple_Random_Window_Discriminators, self).__init__()\n",
    "        self.lc_channels = lc_channels\n",
    "        self.window_size = window_size\n",
    "        self.upsample_factor = upsample_factor\n",
    "\n",
    "        self.udiscriminators = nn.ModuleList([\n",
    "            UnConditionalDBlocks(in_channels=1, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=2, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=4, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=8, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=15, factors=(2, 2), out_channels=(128, 256)),\n",
    "        ])\n",
    "\n",
    "        self.discriminators = nn.ModuleList([\n",
    "            ConditionalDBlocks(in_channels=1, lc_channels=lc_channels,\n",
    "                               factors=(5, 3, 2, 2, 2), out_channels=(128, 128, 256, 256)),\n",
    "            ConditionalDBlocks(in_channels=2, lc_channels=lc_channels,\n",
    "                               factors=(5, 3, 2, 2), out_channels=(128, 256, 256)),\n",
    "            ConditionalDBlocks(in_channels=4, lc_channels=lc_channels,\n",
    "                               factors=(5, 3, 2), out_channels=(128, 256)),\n",
    "            ConditionalDBlocks(in_channels=8, lc_channels=lc_channels,\n",
    "                               factors=(5, 3), out_channels=(256,)),\n",
    "            ConditionalDBlocks(in_channels=15, lc_channels=lc_channels,\n",
    "                               factors=(2, 2, 2), out_channels=(128, 256)),\n",
    "        ])\n",
    "\n",
    "    def forward(self, real_samples, fake_samples, conditions):\n",
    "        real_outputs, fake_outputs = [], []\n",
    "        # Unconditional\n",
    "        for (size, layer) in zip(self.window_size, self.udiscriminators):\n",
    "            size = size * self.upsample_factor\n",
    "            index = np.random.randint(0, real_samples.size(-1) - size + 1)\n",
    "            real_slice = real_samples[:, :, index: index + size]\n",
    "            fake_slice = fake_samples[:, :, index: index + size]\n",
    "            real_output = layer(real_slice)\n",
    "            fake_output = layer(fake_slice)\n",
    "            real_outputs.append(real_output)\n",
    "            fake_outputs.append(fake_output)\n",
    "\n",
    "        # Conditional\n",
    "        for (size, layer) in zip(self.window_size, self.discriminators):\n",
    "            lc_index = np.random.randint(0, conditions.size(-1) - size + 1)\n",
    "            sample_index = lc_index * self.upsample_factor\n",
    "            real_x = real_samples[:, :, sample_index: (lc_index + size)*self.upsample_factor]\n",
    "            fake_x = fake_samples[:, :, sample_index: (lc_index + size)*self.upsample_factor]\n",
    "            lc = conditions[:, :, lc_index: lc_index + size]\n",
    "            real_output = layer(real_x, lc)\n",
    "            fake_output = layer(fake_x, lc)\n",
    "            real_outputs.append(real_output)\n",
    "            fake_outputs.append(fake_output)\n",
    "\n",
    "        return real_outputs, fake_outputs\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, audio_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.audio_length = audio_length\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###################################\n",
    "# Training Functions\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pretrain_single_generator(\n",
    "    num_epochs, z_dim, lr_gen, lr_disc, batch_size, seed,\n",
    "    audio_length, output_dir, train_dataset, checkpoint_path=\"checkpoint.pth\",\n",
    "    resume=False, vocab_size=100, embedding_dim=64\n",
    "):\n",
    "    set_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    phoneme_embedding = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "    nn.init.normal_(phoneme_embedding.weight, 0.0, 0.1)\n",
    "\n",
    "    generator = Generator(vocab_size=vocab_size, embedding_dim=embedding_dim, z_channels=z_dim).to(device)\n",
    "    discriminator = Multiple_Random_Window_Discriminators(lc_channels=embedding_dim).to(device)\n",
    "\n",
    "    optimizer_gen = optim.Adam(generator.parameters(), lr=lr_gen, betas=(0.5, 0.9))\n",
    "    optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "    scaler_gen = GradScaler()\n",
    "    scaler_disc = GradScaler()\n",
    "\n",
    "    start_epoch = 0\n",
    "    if resume and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        optimizer_gen.load_state_dict(checkpoint['optimizer_gen_state_dict'])\n",
    "        optimizer_disc.load_state_dict(checkpoint['optimizer_disc_state_dict'])\n",
    "        scaler_gen.load_state_dict(checkpoint['scaler_gen'])\n",
    "        scaler_disc.load_state_dict(checkpoint['scaler_disc'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        for batch_idx, (real_audio, features_ids) in enumerate(train_loader):\n",
    "            real_audio = real_audio.to(device)  # [B,1,T]\n",
    "            features_ids = features_ids.to(device) # [B, feature_length]\n",
    "            # Embed features\n",
    "            features_emb = phoneme_embedding(features_ids) # [B, feature_length, emb_dim]\n",
    "            features_emb = features_emb.transpose(1, 2)     # [B, emb_dim, feature_length]\n",
    "\n",
    "            # Train Discriminator\n",
    "            for _ in range(5):  # Critic steps\n",
    "                optimizer_disc.zero_grad()\n",
    "                noise = generate_noise(real_audio.size(0), z_dim, device)\n",
    "\n",
    "                with autocast(device_type='cuda'):\n",
    "                    fake_audio = generator(features_emb, noise).detach()\n",
    "                    real_outputs, fake_outputs = discriminator(real_audio, fake_audio, features_emb)\n",
    "                    \n",
    "                    # Compute WGAN loss for discriminator\n",
    "                    loss_disc = sum(torch.mean(f) - torch.mean(r) for r, f in zip(real_outputs, fake_outputs))\n",
    "                    \n",
    "                    # Gradient Penalty\n",
    "                    gradient_penalty = compute_gradient_penalty(discriminator, real_audio, fake_audio, features_emb, device)\n",
    "                    loss_disc += 10 * gradient_penalty\n",
    "\n",
    "                # Backward and step\n",
    "                try:\n",
    "                    scaler_disc.scale(loss_disc).backward(retain_graph=True)\n",
    "                except RuntimeError as e:\n",
    "                    print(\"Backward failed:\", e)\n",
    "                    exit(1)\n",
    "\n",
    "                scaler_disc.step(optimizer_disc)\n",
    "                scaler_disc.update()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_gen.zero_grad()\n",
    "            # In Generator Training\n",
    "            noise = generate_noise(real_audio.size(0), z_dim, device)\n",
    "            with autocast(device_type='cuda'):\n",
    "                fake_audio = generator(features_emb, noise)  # Do not detach here\n",
    "                fake_outputs = discriminator(fake_audio, fake_audio, features_emb)[1]\n",
    "                fake_outputs = pad_tensors_to_match(fake_outputs)\n",
    "                loss_gen = -torch.mean(fake_outputs)\n",
    "\n",
    "            # Backward and step\n",
    "            scaler_gen.scale(loss_gen).backward(retain_graph=True)  # Ensure no re-use of graph\n",
    "            scaler_gen.step(optimizer_gen)\n",
    "            scaler_gen.update()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {loss_disc.item():.4f}, Loss G: {loss_gen.item():.4f}\")\n",
    "\n",
    "        # Save samples\n",
    "        visualize_and_save_generated_waveforms([generator], z_dim, features_emb, num_waveforms=5, device=device, epoch=epoch, sample_rate=16000, output_dir=output_dir)\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch+1,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_gen_state_dict': optimizer_gen.state_dict(),\n",
    "            'optimizer_disc_state_dict': optimizer_disc.state_dict(),\n",
    "            'scaler_gen': scaler_gen.state_dict(),\n",
    "            'scaler_disc': scaler_disc.state_dict(),\n",
    "            'z_dim': z_dim,\n",
    "            'lr_gen': lr_gen,\n",
    "            'lr_disc': lr_disc,\n",
    "            'batch_size': batch_size,\n",
    "            'seed': seed,\n",
    "            'audio_length': audio_length,\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "        torch.save(generator.state_dict(), os.path.join(output_dir, f\"pretrained_generator_epoch{epoch+1}.pth\"))\n",
    "\n",
    "    print(\"Pretraining complete.\")\n",
    "    return generator, phoneme_embedding\n",
    "\n",
    "def train_gan_with_pretrained_generators(\n",
    "    pretrained_generator, phoneme_embedding,\n",
    "    num_epochs, z_dim, lr_gen, lr_disc, batch_size, train_dataset,\n",
    "    num_generators, seed, audio_length, output_dir, checkpoint_path=\"multi_gan_checkpoint.pth\",\n",
    "    resume=False\n",
    "):\n",
    "    set_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize multiple generators with pretrained weights\n",
    "    generators = []\n",
    "    for _ in range(num_generators):\n",
    "        gen = Generator(vocab_size=phoneme_embedding.num_embeddings, embedding_dim=phoneme_embedding.embedding_dim, z_channels=z_dim).to(device)\n",
    "        gen.load_state_dict(pretrained_generator.state_dict())\n",
    "        generators.append(gen)\n",
    "\n",
    "    discriminator = Multiple_Random_Window_Discriminators(lc_channels=phoneme_embedding.embedding_dim).to(device)\n",
    "    encoder = Encoder(audio_length).to(device)\n",
    "\n",
    "    optimizer_gens = [optim.Adam(gen.parameters(), lr=lr_gen, betas=(0.5, 0.9)) for gen in generators]\n",
    "    optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "    optimizer_encoder = optim.Adam(encoder.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "    scaler_gens = [GradScaler() for _ in range(num_generators)]\n",
    "    scaler_disc = GradScaler()\n",
    "    scaler_encoder = GradScaler()\n",
    "\n",
    "    start_epoch = 0\n",
    "    if resume and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        for idx, gen in enumerate(generators):\n",
    "            gen.load_state_dict(checkpoint['generator_state_dicts'][idx])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "        for idx, opt_gen in enumerate(optimizer_gens):\n",
    "            opt_gen.load_state_dict(checkpoint['optimizer_gen_state_dicts'][idx])\n",
    "        optimizer_disc.load_state_dict(checkpoint['optimizer_disc_state_dict'])\n",
    "        optimizer_encoder.load_state_dict(checkpoint['optimizer_encoder_state_dict'])\n",
    "        print(f\"Resumed training from epoch {start_epoch}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    lambda_gp = 10\n",
    "    lambda_ortho = 0.1\n",
    "    num_critic = 5\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        for batch_idx, (real_audio, features_ids) in enumerate(train_loader):\n",
    "            real_audio = real_audio.to(device)\n",
    "            features_ids = features_ids.to(device)\n",
    "            features_emb = phoneme_embedding(features_ids).transpose(1,2) # [B, emb_dim, length]\n",
    "\n",
    "            # Train Discriminator\n",
    "            for _ in range(num_critic):\n",
    "                optimizer_disc.zero_grad()\n",
    "                noises = [generate_noise(real_audio.size(0), z_dim, device) for _ in range(num_generators)]\n",
    "\n",
    "                with autocast(device_type='cuda'):\n",
    "                    fakes = [gen(features_emb, noises[i]).detach() for i, gen in enumerate(generators)]\n",
    "                    # We must stack fakes in a way to pass them to the discriminator.\n",
    "                    # Discriminator expects two sets: real and fake.\n",
    "                    # It processes each window independently. Just call discriminator on real_audio and \"first\" fake:\n",
    "                    real_outputs, fake_outputs = discriminator(real_audio, fakes[0], features_emb)\n",
    "                    # If we intended multiple sets, we could combine them, but code is from original snippet.\n",
    "                    # We'll just consider the single fakes[0] for gradient penalty and WGAN loss.\n",
    "                    loss_disc = sum(torch.mean(fake) - torch.mean(real) for real, fake in zip(real_outputs, fake_outputs))\n",
    "                    gradient_penalty = compute_gradient_penalty(discriminator, real_audio, fakes[0], features_emb, device)\n",
    "                    loss_disc += lambda_gp * gradient_penalty\n",
    "\n",
    "                scaler_disc.scale(loss_disc).backward()\n",
    "                scaler_disc.step(optimizer_disc)\n",
    "                scaler_disc.update()\n",
    "\n",
    "            # Train Generators and Encoder\n",
    "            for idx, gen in enumerate(generators):\n",
    "                optimizer_gens[idx].zero_grad()\n",
    "                optimizer_encoder.zero_grad()\n",
    "\n",
    "                noise = generate_noise(real_audio.size(0), z_dim, device)\n",
    "                with autocast(device_type='cuda'):\n",
    "                    fake = gen(features_emb, noise)\n",
    "                    fake_outputs = discriminator(fake, fake, features_emb)[1]\n",
    "                    fake_outputs = pad_tensors_to_match(fake_outputs)\n",
    "                    loss_gen = -torch.mean(fake_outputs)\n",
    "\n",
    "                    # Orthogonal loss\n",
    "                    gen_feature = encoder(fake)\n",
    "                    ortho_loss_val = 0\n",
    "                    for other_idx, other_gen in enumerate(generators):\n",
    "                        if idx != other_idx:\n",
    "                            other_noise = generate_noise(real_audio.size(0), z_dim, device)\n",
    "                            other_fake = other_gen(features_emb, other_noise)\n",
    "                            other_feature = encoder(other_fake)\n",
    "                            ortho_loss_val += orthogonal_loss(gen_feature, other_feature)\n",
    "                    ortho_loss_val /= (num_generators - 1)\n",
    "                    total_loss_gen = loss_gen + lambda_ortho * ortho_loss_val\n",
    "\n",
    "                scaler_gens[idx].scale(total_loss_gen).backward()\n",
    "                scaler_gens[idx].step(optimizer_gens[idx])\n",
    "                scaler_gens[idx].update()\n",
    "                scaler_encoder.step(optimizer_encoder)\n",
    "                scaler_encoder.update()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {loss_disc.item():.4f}, Loss G: {loss_gen.item():.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'generator_state_dicts': [gen.state_dict() for gen in generators],\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'encoder_state_dict': encoder.state_dict(),\n",
    "            'optimizer_gen_state_dicts': [opt.state_dict() for opt in optimizer_gens],\n",
    "            'optimizer_disc_state_dict': optimizer_disc.state_dict(),\n",
    "            'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return generators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################\n",
    "# Main Execution\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename=\"training_log.txt\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "set_seed(42)\n",
    "audio_length = 64000\n",
    "z_dim = 128\n",
    "lr_gen = 0.0002\n",
    "lr_disc = 0.0002\n",
    "batch_size = 4\n",
    "num_epochs = 50\n",
    "root_dir = \"./data\"\n",
    "sample_rate = 16000\n",
    "num_generators = 5\n",
    "output_dir = 'generated_audio'\n",
    "\n",
    "# Create dataset (no longer loading all into RAM)\n",
    "train_dataset = MyAudioDataset(root_dir, target_sr=sample_rate, target_length=audio_length)\n",
    "vocab_size = len(train_dataset.phoneme_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    clear_all_memory()\n",
    "    pretrained_generator, phoneme_embedding = pretrain_single_generator(\n",
    "        num_epochs=20,\n",
    "        z_dim=z_dim,\n",
    "        lr_gen=lr_gen,\n",
    "        lr_disc=lr_disc,\n",
    "        batch_size=batch_size,\n",
    "        seed=42,\n",
    "        audio_length=audio_length,\n",
    "        output_dir='waveform_pre',\n",
    "        train_dataset=train_dataset,\n",
    "        checkpoint_path=\"gan_single_check.pth\",\n",
    "        embedding_dim=64,\n",
    "        vocab_size=vocab_size\n",
    "    )\n",
    "\n",
    "    train_gan_with_pretrained_generators(\n",
    "        pretrained_generator, phoneme_embedding,\n",
    "        num_epochs=num_epochs,\n",
    "        z_dim=z_dim,\n",
    "        lr_gen=lr_gen,\n",
    "        lr_disc=lr_disc,\n",
    "        batch_size=batch_size,\n",
    "        train_dataset=train_dataset,\n",
    "        num_generators=num_generators,\n",
    "        seed=42,\n",
    "        audio_length=audio_length,\n",
    "        output_dir=output_dir,\n",
    "        checkpoint_path='my_checkpoints/multi_gan_checkpoint.pth',\n",
    "        resume=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
