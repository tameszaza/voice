{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from glob import glob\n",
    "from scipy.signal import butter, filtfilt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_normalize(file_path, target_sr=16000):\n",
    "    \"\"\"Load a FLAC file, resample to 16kHz, and normalize.\"\"\"\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
    "    audio = audio / np.max(np.abs(audio))  # Normalize to [-1, 1]\n",
    "    return audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_trim(audio, target_length=32000):\n",
    "    \"\"\"Pad or trim audio to the target length.\"\"\"\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:target_length]\n",
    "    return audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass_filter(audio, lowcut, highcut, sr, order=5):\n",
    "    \"\"\"Apply a bandpass filter to isolate specific frequency ranges.\"\"\"\n",
    "    nyquist = 0.5 * sr\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, audio)\n",
    "\n",
    "# Example: Split into low and high frequency bands\n",
    "def split_into_bands(audio, sr=16000):\n",
    "    low_band = bandpass_filter(audio, 20, 2000, sr)\n",
    "    high_band = bandpass_filter(audio, 2000, 8000, sr)\n",
    "    return low_band, high_band\n",
    "def segment_audio(audio, segment_size=8000, hop_size=4000):\n",
    "    \"\"\"Segment audio into overlapping chunks.\"\"\"\n",
    "    segments = []\n",
    "    for i in range(0, len(audio) - segment_size + 1, hop_size):\n",
    "        segments.append(audio[i:i + segment_size])\n",
    "    return np.array(segments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(root_dir, target_sr=16000, target_length=32000, split_bands=False, segment=False):\n",
    "    \"\"\"Load and preprocess all audio files in the dataset.\"\"\"\n",
    "    \n",
    "    # Update to use os.path.join for compatibility across systems\n",
    "    flac_files = glob(os.path.join(root_dir, '**', '*.flac'), recursive=True)\n",
    "    \n",
    "    # Check if files are found\n",
    "    if not flac_files:\n",
    "        print(\"No FLAC files found. Check the directory path.\")\n",
    "    \n",
    "    dataset = []\n",
    "    for file in flac_files:\n",
    "        audio = load_and_normalize(file, target_sr)\n",
    "        audio = pad_or_trim(audio, target_length)\n",
    "        \n",
    "        if split_bands:\n",
    "            # Split into frequency bands for multiple generators\n",
    "            low_band, high_band = split_into_bands(audio, target_sr)\n",
    "            dataset.append((low_band, high_band))\n",
    "        elif segment:\n",
    "            # Segment audio for time-segmented generators\n",
    "            audio_segments = segment_audio(audio)\n",
    "            dataset.extend(audio_segments)\n",
    "        else:\n",
    "            # Standard single generator processing\n",
    "            dataset.append(audio)\n",
    "\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'voice/data/LibriSpeech/dev-clean'\n",
    "flac_files = glob(os.path.join(data_dir, '**', '*.flac'), recursive=True)\n",
    "print(flac_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_tensor(audio_list):\n",
    "    \"\"\"Convert the audio list to PyTorch tensors.\"\"\"\n",
    "    if isinstance(audio_list[0], tuple):\n",
    "        # For split bands (tuple of bands)\n",
    "        low_band_tensors = [torch.tensor(x[0], dtype=torch.float32) for x in audio_list]\n",
    "        high_band_tensors = [torch.tensor(x[1], dtype=torch.float32) for x in audio_list]\n",
    "        return torch.stack(low_band_tensors), torch.stack(high_band_tensors)\n",
    "    else:\n",
    "        # For standard or segmented audio\n",
    "        audio_tensors = [torch.tensor(audio, dtype=torch.float32) for audio in audio_list]\n",
    "        return torch.stack(audio_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(audio_tensors, batch_size=32):\n",
    "    \"\"\"Create a DataLoader for batch processing.\"\"\"\n",
    "    if isinstance(audio_tensors, tuple):\n",
    "        # If using frequency bands\n",
    "        dataset = TensorDataset(audio_tensors[0], audio_tensors[1])\n",
    "    else:\n",
    "        dataset = TensorDataset(audio_tensors)\n",
    "        \n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoice\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mLibriSpeech\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdev-clean\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m audio_dataset \u001b[38;5;241m=\u001b[39m preprocess_dataset(data_dir, split_bands\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Using frequency bands\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m audio_tensors \u001b[38;5;241m=\u001b[39m \u001b[43maudio_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m create_dataloader(audio_tensors, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m, in \u001b[0;36maudio_to_tensor\u001b[1;34m(audio_list)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maudio_to_tensor\u001b[39m(audio_list):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert the audio list to PyTorch tensors.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43maudio_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;66;03m# For split bands (tuple of bands)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m         low_band_tensors \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(x[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m audio_list]\n\u001b[0;32m      6\u001b[0m         high_band_tensors \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(x[\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m audio_list]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "data_dir = 'voice\\data\\LibriSpeech\\dev-clean'\n",
    "audio_dataset = preprocess_dataset(data_dir, split_bands=True)  # Using frequency bands\n",
    "\n",
    "audio_tensors = audio_to_tensor(audio_dataset)\n",
    "\n",
    "\n",
    "dataloader = create_dataloader(audio_tensors, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
