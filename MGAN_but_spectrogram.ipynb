{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from torch.utils.data import DataLoader, TensorDataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 6789\n",
    "torch.manual_seed(manualSeed)\n",
    "np.random.seed(manualSeed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "# Generator shared layers\n",
    "# Generator shared layers\n",
    "class GeneratorSharedLayers(nn.Module):\n",
    "    def __init__(self, ngf, nc, mel_bins, time_frames):\n",
    "        super(GeneratorSharedLayers, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: (ngf * 8, mel_bins // 8, time_frames // 8)\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),  # Upsample to (mel_bins // 4, time_frames // 4)\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),  # Upsample to (mel_bins // 2, time_frames // 2)\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 2, nc, 4, 2, 1, bias=False),       # Upsample to (mel_bins, time_frames)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Generator with unique input layer and shared layers\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc, shared_layers, mel_bins, time_frames):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngf = ngf\n",
    "        self.mel_bins = mel_bins\n",
    "        self.time_frames = time_frames\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(nz, ngf * 8 * (mel_bins // 8) * (time_frames // 8)),\n",
    "            nn.BatchNorm1d(ngf * 8 * (mel_bins // 8) * (time_frames // 8)),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.shared_layers = shared_layers\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.input_layer(input)\n",
    "        x = x.view(-1, self.ngf * 8, self.mel_bins // 8, self.time_frames // 8)\n",
    "        x = self.shared_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Discriminator shared layers\n",
    "# Discriminator shared layers\n",
    "class DiscriminatorSharedLayers(nn.Module):\n",
    "    def __init__(self, ndf, nc):\n",
    "        super(DiscriminatorSharedLayers, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: (nc, mel_bins, time_frames)\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),          # Output: (ndf, 32, 64)\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),     # Output: (ndf*2, 16, 32)\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False), # Output: (ndf*4, 8, 16)\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ndf, nc, shared_layers, num_gens, mel_bins, time_frames):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.shared_layers = shared_layers\n",
    "        self.num_gens = num_gens\n",
    "\n",
    "        # The spatial dimensions after the shared layers are (8, 16)\n",
    "        self.output_bin = nn.Sequential(\n",
    "            nn.Conv2d(ndf * 4, 1, kernel_size=(8, 16), stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.output_mul = nn.Sequential(\n",
    "            nn.Conv2d(ndf * 4, num_gens, kernel_size=(8, 16), stride=1, padding=0, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.shared_layers(input)\n",
    "        # x has shape: (batch_size, ndf*4, 8, 16)\n",
    "\n",
    "        # Apply output layers\n",
    "        output_bin = self.output_bin(x)  # Shape: (batch_size, 1, 1, 1)\n",
    "        output_bin = output_bin.view(-1)  # Shape: (batch_size,)\n",
    "\n",
    "        output_mul = self.output_mul(x)  # Shape: (batch_size, num_gens, 1, 1)\n",
    "        output_mul = output_mul.view(-1, self.num_gens)  # Shape: (batch_size, num_gens)\n",
    "\n",
    "        return output_bin, output_mul\n",
    "\n",
    "\n",
    "\n",
    "# MGAN class encapsulating the training loop\n",
    "class MGAN:\n",
    "    def __init__(self, num_z, beta, num_gens, batch_size, z_prior, learning_rate,\n",
    "                 num_epochs, img_size, num_gen_feature_maps, num_dis_feature_maps,\n",
    "                 sample_dir, device):\n",
    "        self.num_z = num_z\n",
    "        self.beta = beta\n",
    "        self.num_gens = num_gens\n",
    "        self.batch_size = batch_size\n",
    "        self.z_prior = z_prior\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.img_size = img_size\n",
    "        self.ngf = num_gen_feature_maps\n",
    "        self.ndf = num_dis_feature_maps\n",
    "        self.sample_dir = sample_dir\n",
    "        self.device = device\n",
    "\n",
    "        self.mel_bins, self.num_frames, self.num_channels = img_size\n",
    "        self.history = {'d_loss': [], 'g_loss': []}\n",
    "\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        mel_bins, time_frames, num_channels = self.img_size\n",
    "\n",
    "        self.shared_gen_layers = GeneratorSharedLayers(self.ngf, num_channels,mel_bins,time_frames).to(self.device)\n",
    "        self.generators = nn.ModuleList([\n",
    "            Generator(self.num_z, self.ngf, num_channels, self.shared_gen_layers, mel_bins, time_frames).to(self.device)\n",
    "            for _ in range(self.num_gens)\n",
    "        ])\n",
    "        self.shared_dis_layers = DiscriminatorSharedLayers(self.ndf, num_channels).to(self.device)\n",
    "        self.discriminator = Discriminator(self.ndf, num_channels, self.shared_dis_layers, self.num_gens, mel_bins, time_frames).to(self.device)\n",
    "\n",
    "        self.optimizerD = optim.Adam(\n",
    "            list(self.discriminator.parameters()) + list(self.shared_dis_layers.parameters()),\n",
    "            lr=self.learning_rate, betas=(0.5, 0.999)\n",
    "        )\n",
    "        gen_params = [param for gen in self.generators for param in gen.input_layer.parameters()]\n",
    "        gen_params += list(self.shared_gen_layers.parameters())\n",
    "        self.optimizerG = optim.Adam(gen_params, lr=self.learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "        self.criterion_bin = nn.BCELoss()\n",
    "        self.criterion_mul = nn.CrossEntropyLoss()\n",
    "\n",
    "    def fit(self, trainloader):\n",
    "        fixed_noise = self._sample_z(self.num_gens * 16).to(self.device)\n",
    "\n",
    "        real_label = 1.0\n",
    "        fake_label = 0.0\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(f\"Epoch {epoch+1}/{self.num_epochs}\")\n",
    "\n",
    "            for i, data in enumerate(trainloader):\n",
    "                print(f\"Batch {i+1}\")\n",
    "\n",
    "                ############################\n",
    "                # (1) Update Discriminator #\n",
    "                ############################\n",
    "                real_images = data[0].to(self.device)\n",
    "                b_size = real_images.size(0)\n",
    "                print(f\"Real images shape: {real_images.shape}\")\n",
    "                label_real = torch.full((b_size,), real_label, device=self.device)\n",
    "\n",
    "                # Forward pass real images through the discriminator\n",
    "                output_bin_real, _ = self.discriminator(real_images)\n",
    "                print(f\"Output binary shape (real): {output_bin_real.shape}\")\n",
    "                d_bin_real_loss = self.criterion_bin(output_bin_real, label_real)\n",
    "\n",
    "                # Generate fake images and corresponding labels\n",
    "                fake_images = []\n",
    "                gen_labels = []\n",
    "\n",
    "                # Ensure batch_per_gen is at least 2\n",
    "                batch_per_gen = max(2, b_size // self.num_gens)  \n",
    "\n",
    "                if batch_per_gen < b_size // self.num_gens:\n",
    "                    print(f\"Warning: Batch size {b_size} is too small for {self.num_gens} generators. Adjusting batch_per_gen to {batch_per_gen}.\")\n",
    "\n",
    "                for idx, gen in enumerate(self.generators):\n",
    "                    if idx * batch_per_gen >= b_size:\n",
    "                        break  # Prevent overflow when batch size is smaller than expected\n",
    "                    # Adjust z sampling to match batch_per_gen\n",
    "                    z = self._sample_z(batch_per_gen).to(self.device)\n",
    "                    fake_imgs = gen(z)\n",
    "                    fake_images.append(fake_imgs)\n",
    "                    gen_labels.append(torch.full((fake_imgs.size(0),), idx, dtype=torch.long, device=self.device))\n",
    "\n",
    "                # Concatenate fake images and labels\n",
    "                fake_images = torch.cat(fake_images, 0)  # Shape: [total_fake_batch_size, channels, mel_bins, time_frames]\n",
    "                gen_labels = torch.cat(gen_labels, 0)    # Shape: [total_fake_batch_size]\n",
    "\n",
    "                print(f\"Fake images shape: {fake_images.shape}\")\n",
    "                print(f\"Gen labels shape: {gen_labels.shape}\")\n",
    "\n",
    "                # Forward pass fake images through the discriminator\n",
    "                output_bin_fake, output_mul_fake = self.discriminator(fake_images.detach())\n",
    "                print(f\"Output binary shape (fake): {output_bin_fake.shape}\")\n",
    "                print(f\"Output multi-class shape: {output_mul_fake.shape}\")\n",
    "\n",
    "                # Ensure shapes are compatible for loss computation\n",
    "                output_mul_fake = output_mul_fake.view(-1, self.num_gens)  # Shape: [total_fake_batch_size, num_gens]\n",
    "                gen_labels = gen_labels.view(-1)                          # Shape: [total_fake_batch_size]\n",
    "                d_mul_loss = self.criterion_mul(output_mul_fake, gen_labels)\n",
    "\n",
    "\n",
    "                if output_mul_fake.shape[0] != gen_labels.shape[0]:\n",
    "                    print(f\"Shape mismatch! output_mul_fake: {output_mul_fake.shape}, gen_labels: {gen_labels.shape}\")\n",
    "                    raise ValueError(\"Mismatch in shapes for discriminator multi-class output and labels.\")\n",
    "\n",
    "                # Compute discriminator losses\n",
    "                label_fake = torch.full((output_bin_fake.size(0),), fake_label, device=self.device)\n",
    "                d_bin_fake_loss = self.criterion_bin(output_bin_fake, label_fake)\n",
    "                d_mul_loss = self.criterion_mul(output_mul_fake, gen_labels)\n",
    "\n",
    "                d_loss = d_bin_real_loss + d_bin_fake_loss + d_mul_loss * self.beta\n",
    "                d_loss.backward()\n",
    "                self.optimizerD.step()\n",
    "\n",
    "                ##########################\n",
    "                # (2) Update Generators #\n",
    "                ##########################\n",
    "                for gen in self.generators:\n",
    "                    gen.zero_grad()\n",
    "                self.shared_gen_layers.zero_grad()\n",
    "\n",
    "                label_real = torch.full((output_bin_fake.size(0),), real_label, device=self.device)\n",
    "                output_bin_fake, output_mul_fake = self.discriminator(fake_images)\n",
    "                g_bin_loss = self.criterion_bin(output_bin_fake, label_real)\n",
    "                g_mul_loss = self.criterion_mul(output_mul_fake.view(-1, self.num_gens), gen_labels) * self.beta\n",
    "\n",
    "                g_loss = g_bin_loss + g_mul_loss\n",
    "                g_loss.backward()\n",
    "                self.optimizerG.step()\n",
    "\n",
    "                # Save losses for plotting\n",
    "                self.history['d_loss'].append(d_loss.item())\n",
    "                self.history['g_loss'].append(g_loss.item())\n",
    "\n",
    "            print(f\"[{epoch+1}/{self.num_epochs}] d_loss: {d_loss.item():.4f} | g_loss: {g_loss.item():.4f}\")\n",
    "\n",
    "            # Save samples every few epochs\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self._save_samples(epoch + 1, fixed_noise)\n",
    "\n",
    "        # Plot loss history after training\n",
    "        self._plot_history()\n",
    "\n",
    "\n",
    "\n",
    "    def _sample_z(self, size):\n",
    "        if self.z_prior == \"uniform\":\n",
    "            return torch.rand(size, self.num_z) * 2 - 1\n",
    "        return torch.randn(size, self.num_z)\n",
    "\n",
    "    def _save_samples(self, epoch, fixed_noise):\n",
    "        with torch.no_grad():\n",
    "            fake_images = []\n",
    "            for idx, gen in enumerate(self.generators):\n",
    "                noise = fixed_noise[idx * 16:(idx + 1) * 16].to(self.device)\n",
    "                gen.eval()\n",
    "                fake_imgs = gen(noise)\n",
    "                gen.train()\n",
    "                fake_images.append(fake_imgs)\n",
    "\n",
    "            fake_images = torch.cat(fake_images, 0)\n",
    "            fake_images = (fake_images + 1) / 2.0\n",
    "            os.makedirs(self.sample_dir, exist_ok=True)\n",
    "            sample_path = os.path.join(self.sample_dir, f\"epoch_{epoch:04d}.png\")\n",
    "            vutils.save_image(fake_images, sample_path, nrow=16, padding=2, normalize=True)\n",
    "            print(f\"Saved samples to {sample_path}\")\n",
    "\n",
    "    def _plot_history(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.history['d_loss'], label=\"D Loss\")\n",
    "        plt.plot(self.history['g_loss'], label=\"G Loss\")\n",
    "        plt.title(\"Loss During Training\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import librosa\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def librispeech_to_mel(root_dir, target_sr=16000, n_mels=64, n_fft=1024, hop_length=512, target_length=128):\n",
    "    \"\"\"\n",
    "    Convert raw LibriSpeech audio files to Mel spectrograms and return a TensorDataset.\n",
    "    \"\"\"\n",
    "    flac_files = glob.glob(os.path.join(root_dir, '**', '*.flac'), recursive=True)\n",
    "    mel_spectrograms = []  # Initialize list to store Mel spectrograms\n",
    "\n",
    "    if len(flac_files) == 0:\n",
    "        raise ValueError(f\"No FLAC files found in directory: {root_dir}\")\n",
    "\n",
    "    for file in flac_files:\n",
    "        try:\n",
    "            # Load the audio file\n",
    "            audio, sr = librosa.load(file, sr=target_sr)\n",
    "            # Normalize audio to range [-1, 1]\n",
    "            audio = audio / np.max(np.abs(audio))\n",
    "            \n",
    "            # Convert to Mel spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "            \n",
    "            # Convert to dB scale\n",
    "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            \n",
    "            # Ensure the Mel spectrogram has a fixed length\n",
    "            if mel_spec_db.shape[1] < target_length:\n",
    "                # Pad if shorter\n",
    "                pad_width = target_length - mel_spec_db.shape[1]\n",
    "                mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)), mode='constant')\n",
    "            else:\n",
    "                # Trim if longer\n",
    "                mel_spec_db = mel_spec_db[:, :target_length]\n",
    "\n",
    "            # Add channel dimension and append to list\n",
    "            mel_tensor = torch.tensor(mel_spec_db, dtype=torch.float32).unsqueeze(0)  # Add channel dim\n",
    "            mel_spectrograms.append(mel_tensor)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    if len(mel_spectrograms) == 0:\n",
    "        raise ValueError(\"No valid audio files were processed into Mel spectrograms.\")\n",
    "\n",
    "    # Stack into a single tensor\n",
    "    mel_spectrograms = torch.stack(mel_spectrograms)  # Shape: [num_samples, 1, n_mels, target_length]\n",
    "    return TensorDataset(mel_spectrograms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from model import MGAN  # Import the MGAN class\n",
    "from dataset_load import librispeech_to_mel  # Your dataset preprocessing function\n",
    "import glob\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    num_z = 100\n",
    "    beta = 0.5\n",
    "    num_gens = 10\n",
    "    batch_size = 16\n",
    "    z_prior = \"gaussian\"\n",
    "    learning_rate = 0.0002\n",
    "    num_epochs = 50\n",
    "\n",
    "    # Spectrogram dimensions\n",
    "    mel_bins = 64\n",
    "    num_frames = 128\n",
    "    num_channels = 1\n",
    "    img_size = (mel_bins, num_frames, num_channels)\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Dataset preprocessing\n",
    "    data_dir = r'data\\LibriSpeech\\LibriSpeech\\dev-clean'\n",
    "    print(\"Preprocessing dataset into Mel spectrograms...\")\n",
    "    mel_dataset = librispeech_to_mel(data_dir, target_sr=16000, n_mels=mel_bins, target_length=num_frames)\n",
    "    print(f\"Dataset size: {len(mel_dataset)}\")\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(mel_dataset, batch_size=batch_size, shuffle=True)\n",
    "    if len(mel_dataset) == 0:\n",
    "        raise ValueError(\"The dataset is empty. Please check your data directory and preprocessing function.\")\n",
    "\n",
    "    # Initialize MGAN\n",
    "    mgan_model = MGAN(\n",
    "        num_z=num_z,\n",
    "        beta=beta,\n",
    "        num_gens=num_gens,\n",
    "        batch_size=batch_size,\n",
    "        z_prior=z_prior,\n",
    "        learning_rate=learning_rate,\n",
    "        num_epochs=num_epochs,\n",
    "        img_size=img_size,\n",
    "        num_gen_feature_maps=64,\n",
    "        num_dis_feature_maps=64,\n",
    "        sample_dir=\"samples\",\n",
    "        device=device\n",
    "    )\n",
    "    if os.path.exists(\"mgan_model.pth\"):\n",
    "        mgan_model.load_state_dict(torch.load(\"mgan_model.pth\"))\n",
    "        print(\"Model loaded from mgan_model.pth\")\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    mgan_model.fit(dataloader)\n",
    "    print(\"Training completed.\")\n",
    "    torch.save(mgan_model.state_dict(), \"mgan_model.pth\")\n",
    "    print(\"Model saved to mgan_model.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
