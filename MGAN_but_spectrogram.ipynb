{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": null,
>>>>>>> d03adc8a092c0446f4d36399e88aa5d66e0ab999
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from torch.utils.data import DataLoader, TensorDataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 6789\n",
    "torch.manual_seed(manualSeed)\n",
    "np.random.seed(manualSeed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "# Generator shared layers\n",
    "# Generator shared layers\n",
    "class GeneratorSharedLayers(nn.Module):\n",
    "    def __init__(self, ngf, nc, mel_bins, time_frames):\n",
    "        super(GeneratorSharedLayers, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: (ngf * 8, mel_bins // 8, time_frames // 8)\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),  # Upsample to (mel_bins // 4, time_frames // 4)\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),  # Upsample to (mel_bins // 2, time_frames // 2)\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 2, nc, 4, 2, 1, bias=False),       # Upsample to (mel_bins, time_frames)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Generator with unique input layer and shared layers\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc, shared_layers, mel_bins, time_frames):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngf = ngf\n",
    "        self.mel_bins = mel_bins\n",
    "        self.time_frames = time_frames\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(nz, ngf * 8 * (mel_bins // 8) * (time_frames // 8)),\n",
    "            nn.BatchNorm1d(ngf * 8 * (mel_bins // 8) * (time_frames // 8)),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.shared_layers = shared_layers\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.input_layer(input)\n",
    "        x = x.view(-1, self.ngf * 8, self.mel_bins // 8, self.time_frames // 8)\n",
    "        x = self.shared_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Discriminator shared layers\n",
    "# Discriminator shared layers\n",
    "class DiscriminatorSharedLayers(nn.Module):\n",
    "    def __init__(self, ndf, nc):\n",
    "        super(DiscriminatorSharedLayers, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: (nc, mel_bins, time_frames)\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),          # Output: (ndf, 32, 64)\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),     # Output: (ndf*2, 16, 32)\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False), # Output: (ndf*4, 8, 16)\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ndf, nc, shared_layers, num_gens, mel_bins, time_frames):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.shared_layers = shared_layers\n",
    "        self.num_gens = num_gens\n",
    "\n",
    "        # The spatial dimensions after the shared layers are (8, 16)\n",
    "        self.output_bin = nn.Sequential(\n",
    "            nn.Conv2d(ndf * 4, 1, kernel_size=(8, 16), stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.output_mul = nn.Sequential(\n",
    "            nn.Conv2d(ndf * 4, num_gens, kernel_size=(8, 16), stride=1, padding=0, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.shared_layers(input)\n",
    "        # x has shape: (batch_size, ndf*4, 8, 16)\n",
    "\n",
    "        # Apply output layers\n",
    "        output_bin = self.output_bin(x)  # Shape: (batch_size, 1, 1, 1)\n",
    "        output_bin = output_bin.view(-1)  # Shape: (batch_size,)\n",
    "\n",
    "        output_mul = self.output_mul(x)  # Shape: (batch_size, num_gens, 1, 1)\n",
    "        output_mul = output_mul.view(-1, self.num_gens)  # Shape: (batch_size, num_gens)\n",
    "\n",
    "        return output_bin, output_mul\n",
    "\n",
    "\n",
    "\n",
    "# MGAN class encapsulating the training loop\n",
    "class MGAN:\n",
    "    def __init__(self, num_z, beta, num_gens, batch_size, z_prior, learning_rate,\n",
    "                 num_epochs, img_size, num_gen_feature_maps, num_dis_feature_maps,\n",
    "                 sample_dir, device):\n",
    "        self.num_z = num_z\n",
    "        self.beta = beta\n",
    "        self.num_gens = num_gens\n",
    "        self.batch_size = batch_size\n",
    "        self.z_prior = z_prior\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.img_size = img_size\n",
    "        self.ngf = num_gen_feature_maps\n",
    "        self.ndf = num_dis_feature_maps\n",
    "        self.sample_dir = sample_dir\n",
    "        self.device = device\n",
    "\n",
    "        self.mel_bins, self.num_frames, self.num_channels = img_size\n",
    "        self.history = {'d_loss': [], 'g_loss': []}\n",
    "\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        mel_bins, time_frames, num_channels = self.img_size\n",
    "\n",
    "        self.shared_gen_layers = GeneratorSharedLayers(self.ngf, num_channels,mel_bins,time_frames).to(self.device)\n",
    "        self.generators = nn.ModuleList([\n",
    "            Generator(self.num_z, self.ngf, num_channels, self.shared_gen_layers, mel_bins, time_frames).to(self.device)\n",
    "            for _ in range(self.num_gens)\n",
    "        ])\n",
    "        self.shared_dis_layers = DiscriminatorSharedLayers(self.ndf, num_channels).to(self.device)\n",
    "        self.discriminator = Discriminator(self.ndf, num_channels, self.shared_dis_layers, self.num_gens, mel_bins, time_frames).to(self.device)\n",
    "\n",
    "        self.optimizerD = optim.Adam(\n",
    "            list(self.discriminator.parameters()) + list(self.shared_dis_layers.parameters()),\n",
    "            lr=self.learning_rate, betas=(0.5, 0.999)\n",
    "        )\n",
    "        gen_params = [param for gen in self.generators for param in gen.input_layer.parameters()]\n",
    "        gen_params += list(self.shared_gen_layers.parameters())\n",
    "        self.optimizerG = optim.Adam(gen_params, lr=self.learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "        self.criterion_bin = nn.BCELoss()\n",
    "        self.criterion_mul = nn.CrossEntropyLoss()\n",
    "\n",
    "    def fit(self, trainloader):\n",
    "        fixed_noise = self._sample_z(self.num_gens * 16).to(self.device)\n",
    "\n",
    "        real_label = 1.0\n",
    "        fake_label = 0.0\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(f\"Epoch {epoch+1}/{self.num_epochs}\")\n",
    "\n",
    "            for i, data in enumerate(trainloader):\n",
    "                print(f\"Batch {i+1}\")\n",
    "\n",
    "                ############################\n",
    "                # (1) Update Discriminator #\n",
    "                ############################\n",
    "                real_images = data[0].to(self.device)\n",
    "                b_size = real_images.size(0)\n",
    "                print(f\"Real images shape: {real_images.shape}\")\n",
    "                label_real = torch.full((b_size,), real_label, device=self.device)\n",
    "\n",
    "                # Forward pass real images through the discriminator\n",
    "                output_bin_real, _ = self.discriminator(real_images)\n",
    "                print(f\"Output binary shape (real): {output_bin_real.shape}\")\n",
    "                d_bin_real_loss = self.criterion_bin(output_bin_real, label_real)\n",
    "\n",
    "                # Generate fake images and corresponding labels\n",
    "                fake_images = []\n",
    "                gen_labels = []\n",
    "\n",
    "                # Ensure batch_per_gen is at least 2\n",
    "                batch_per_gen = max(2, b_size // self.num_gens)  \n",
    "\n",
    "                if batch_per_gen < b_size // self.num_gens:\n",
    "                    print(f\"Warning: Batch size {b_size} is too small for {self.num_gens} generators. Adjusting batch_per_gen to {batch_per_gen}.\")\n",
    "\n",
    "                for idx, gen in enumerate(self.generators):\n",
    "                    if idx * batch_per_gen >= b_size:\n",
    "                        break  # Prevent overflow when batch size is smaller than expected\n",
    "                    # Adjust z sampling to match batch_per_gen\n",
    "                    z = self._sample_z(batch_per_gen).to(self.device)\n",
    "                    fake_imgs = gen(z)\n",
    "                    fake_images.append(fake_imgs)\n",
    "                    gen_labels.append(torch.full((fake_imgs.size(0),), idx, dtype=torch.long, device=self.device))\n",
    "\n",
    "                # Concatenate fake images and labels\n",
    "                fake_images = torch.cat(fake_images, 0)  # Shape: [total_fake_batch_size, channels, mel_bins, time_frames]\n",
    "                gen_labels = torch.cat(gen_labels, 0)    # Shape: [total_fake_batch_size]\n",
    "\n",
    "                print(f\"Fake images shape: {fake_images.shape}\")\n",
    "                print(f\"Gen labels shape: {gen_labels.shape}\")\n",
    "\n",
    "                # Forward pass fake images through the discriminator\n",
    "                output_bin_fake, output_mul_fake = self.discriminator(fake_images.detach())\n",
    "                print(f\"Output binary shape (fake): {output_bin_fake.shape}\")\n",
    "                print(f\"Output multi-class shape: {output_mul_fake.shape}\")\n",
    "\n",
    "                # Ensure shapes are compatible for loss computation\n",
    "                output_mul_fake = output_mul_fake.view(-1, self.num_gens)  # Shape: [total_fake_batch_size, num_gens]\n",
    "                gen_labels = gen_labels.view(-1)                          # Shape: [total_fake_batch_size]\n",
    "                d_mul_loss = self.criterion_mul(output_mul_fake, gen_labels)\n",
    "\n",
    "\n",
    "                if output_mul_fake.shape[0] != gen_labels.shape[0]:\n",
    "                    print(f\"Shape mismatch! output_mul_fake: {output_mul_fake.shape}, gen_labels: {gen_labels.shape}\")\n",
    "                    raise ValueError(\"Mismatch in shapes for discriminator multi-class output and labels.\")\n",
    "\n",
    "                # Compute discriminator losses\n",
    "                label_fake = torch.full((output_bin_fake.size(0),), fake_label, device=self.device)\n",
    "                d_bin_fake_loss = self.criterion_bin(output_bin_fake, label_fake)\n",
    "                d_mul_loss = self.criterion_mul(output_mul_fake, gen_labels)\n",
    "\n",
    "                d_loss = d_bin_real_loss + d_bin_fake_loss + d_mul_loss * self.beta\n",
    "                d_loss.backward()\n",
    "                self.optimizerD.step()\n",
    "\n",
    "                ##########################\n",
    "                # (2) Update Generators #\n",
    "                ##########################\n",
    "                for gen in self.generators:\n",
    "                    gen.zero_grad()\n",
    "                self.shared_gen_layers.zero_grad()\n",
    "\n",
    "                label_real = torch.full((output_bin_fake.size(0),), real_label, device=self.device)\n",
    "                output_bin_fake, output_mul_fake = self.discriminator(fake_images)\n",
    "                g_bin_loss = self.criterion_bin(output_bin_fake, label_real)\n",
    "                g_mul_loss = self.criterion_mul(output_mul_fake.view(-1, self.num_gens), gen_labels) * self.beta\n",
    "\n",
    "                g_loss = g_bin_loss + g_mul_loss\n",
    "                g_loss.backward()\n",
    "                self.optimizerG.step()\n",
    "\n",
    "                # Save losses for plotting\n",
    "                self.history['d_loss'].append(d_loss.item())\n",
    "                self.history['g_loss'].append(g_loss.item())\n",
    "\n",
    "            print(f\"[{epoch+1}/{self.num_epochs}] d_loss: {d_loss.item():.4f} | g_loss: {g_loss.item():.4f}\")\n",
    "\n",
    "            # Save samples every few epochs\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self._save_samples(epoch + 1, fixed_noise)\n",
    "\n",
    "        # Plot loss history after training\n",
    "        self._plot_history()\n",
    "\n",
    "\n",
    "\n",
    "    def _sample_z(self, size):\n",
    "        if self.z_prior == \"uniform\":\n",
    "            return torch.rand(size, self.num_z) * 2 - 1\n",
    "        return torch.randn(size, self.num_z)\n",
    "\n",
    "    def _save_samples(self, epoch, fixed_noise):\n",
    "        with torch.no_grad():\n",
    "            fake_images = []\n",
    "            for idx, gen in enumerate(self.generators):\n",
    "                noise = fixed_noise[idx * 16:(idx + 1) * 16].to(self.device)\n",
    "                gen.eval()\n",
    "                fake_imgs = gen(noise)\n",
    "                gen.train()\n",
    "                fake_images.append(fake_imgs)\n",
    "\n",
    "            fake_images = torch.cat(fake_images, 0)\n",
    "            fake_images = (fake_images + 1) / 2.0\n",
    "            os.makedirs(self.sample_dir, exist_ok=True)\n",
    "            sample_path = os.path.join(self.sample_dir, f\"epoch_{epoch:04d}.png\")\n",
    "            vutils.save_image(fake_images, sample_path, nrow=16, padding=2, normalize=True)\n",
    "            print(f\"Saved samples to {sample_path}\")\n",
    "\n",
    "    def _plot_history(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.history['d_loss'], label=\"D Loss\")\n",
    "        plt.plot(self.history['g_loss'], label=\"G Loss\")\n",
    "        plt.title(\"Loss During Training\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": null,
>>>>>>> d03adc8a092c0446f4d36399e88aa5d66e0ab999
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import librosa\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def librispeech_to_mel(root_dir, target_sr=16000, n_mels=64, n_fft=1024, hop_length=512, target_length=128):\n",
    "    \"\"\"\n",
    "    Convert raw LibriSpeech audio files to Mel spectrograms and return a TensorDataset.\n",
    "    \"\"\"\n",
    "    flac_files = glob.glob(os.path.join(root_dir, '**', '*.flac'), recursive=True)\n",
    "    mel_spectrograms = []  # Initialize list to store Mel spectrograms\n",
    "\n",
    "    if len(flac_files) == 0:\n",
    "        raise ValueError(f\"No FLAC files found in directory: {root_dir}\")\n",
    "\n",
    "    for file in flac_files:\n",
    "        try:\n",
    "            # Load the audio file\n",
    "            audio, sr = librosa.load(file, sr=target_sr)\n",
    "            # Normalize audio to range [-1, 1]\n",
    "            audio = audio / np.max(np.abs(audio))\n",
    "            \n",
    "            # Convert to Mel spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "            \n",
    "            # Convert to dB scale\n",
    "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            \n",
    "            # Ensure the Mel spectrogram has a fixed length\n",
    "            if mel_spec_db.shape[1] < target_length:\n",
    "                # Pad if shorter\n",
    "                pad_width = target_length - mel_spec_db.shape[1]\n",
    "                mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)), mode='constant')\n",
    "            else:\n",
    "                # Trim if longer\n",
    "                mel_spec_db = mel_spec_db[:, :target_length]\n",
    "\n",
    "            # Add channel dimension and append to list\n",
    "            mel_tensor = torch.tensor(mel_spec_db, dtype=torch.float32).unsqueeze(0)  # Add channel dim\n",
    "            mel_spectrograms.append(mel_tensor)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    if len(mel_spectrograms) == 0:\n",
    "        raise ValueError(\"No valid audio files were processed into Mel spectrograms.\")\n",
    "\n",
    "    # Stack into a single tensor\n",
    "    mel_spectrograms = torch.stack(mel_spectrograms)  # Shape: [num_samples, 1, n_mels, target_length]\n",
    "    return TensorDataset(mel_spectrograms)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": null,
>>>>>>> d03adc8a092c0446f4d36399e88aa5d66e0ab999
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "def mel_to_audio(mel_spectrogram, sr=16000, n_fft=1024, hop_length=512, n_iter=100):\n",
    "    \"\"\"\n",
    "    Convert a Mel spectrogram back to an audio waveform.\n",
    "    \n",
    "    Parameters:\n",
    "    - mel_spectrogram (np.ndarray): The Mel spectrogram (in dB or power format).\n",
    "    - sr (int): Sample rate of the audio.\n",
    "    - n_fft (int): Number of FFT components.\n",
    "    - hop_length (int): Hop length for STFT.\n",
    "    - n_iter (int): Number of iterations for Griffin-Lim algorithm.\n",
    "    \n",
    "    Returns:\n",
    "    - audio (np.ndarray): Reconstructed audio waveform.\n",
    "    \"\"\"\n",
    "    # Ensure the spectrogram is in power format (if it's in dB, convert it back)\n",
    "    if mel_spectrogram.max() > 1.0:  # Assuming input is in dB format\n",
    "        mel_spectrogram = librosa.db_to_power(mel_spectrogram)\n",
    "\n",
    "    # Convert Mel spectrogram to linear spectrogram\n",
    "    mel_basis = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=mel_spectrogram.shape[0])\n",
    "    inv_mel_basis = np.linalg.pinv(mel_basis)  # Invert the Mel filter\n",
    "    linear_spectrogram = np.dot(inv_mel_basis, mel_spectrogram)\n",
    "\n",
    "    # Reconstruct the audio waveform using Griffin-Lim algorithm\n",
    "    audio = librosa.griffinlim(linear_spectrogram, hop_length=hop_length, n_iter=n_iter)\n",
    "\n",
    "    return audio\n",
    "\n",
    "def save_audio_from_spectrogram(mel_spectrogram, output_path, sr=16000, n_fft=1024, hop_length=512, n_iter=100):\n",
    "    \"\"\"\n",
    "    Convert a Mel spectrogram to an audio file and save it.\n",
    "    \n",
    "    Parameters:\n",
    "    - mel_spectrogram (np.ndarray): The Mel spectrogram (in dB or power format).\n",
    "    - output_path (str): Path to save the audio file.\n",
    "    - sr (int): Sample rate of the audio.\n",
    "    - n_fft (int): Number of FFT components.\n",
    "    - hop_length (int): Hop length for STFT.\n",
    "    - n_iter (int): Number of iterations for Griffin-Lim algorithm.\n",
    "    \"\"\"\n",
    "    audio = mel_to_audio(mel_spectrogram, sr=sr, n_fft=n_fft, hop_length=hop_length, n_iter=n_iter)\n",
    "    sf.write(output_path, audio, sr)\n",
    "    print(f\"Audio saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Preprocessing dataset into Mel spectrograms...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 70\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved to mgan_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 70\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 35\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mLibriSpeech\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mLibriSpeech\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdev-clean\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing dataset into Mel spectrograms...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m mel_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mlibrispeech_to_mel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_sr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmel_bins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(mel_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# DataLoader\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 26\u001b[0m, in \u001b[0;36mlibrispeech_to_mel\u001b[1;34m(root_dir, target_sr, n_mels, n_fft, hop_length, target_length)\u001b[0m\n\u001b[0;32m     23\u001b[0m audio \u001b[38;5;241m=\u001b[39m audio \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39mabs(audio))\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Convert to Mel spectrogram\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m mel_spec \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmelspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_mels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Convert to dB scale\u001b[39;00m\n\u001b[0;32m     29\u001b[0m mel_spec_db \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mpower_to_db(mel_spec, ref\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmax)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\feature\\spectral.py:2143\u001b[0m, in \u001b[0;36mmelspectrogram\u001b[1;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[0m\n\u001b[0;32m   2130\u001b[0m S, n_fft \u001b[38;5;241m=\u001b[39m _spectrogram(\n\u001b[0;32m   2131\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   2132\u001b[0m     S\u001b[38;5;241m=\u001b[39mS,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2139\u001b[0m     pad_mode\u001b[38;5;241m=\u001b[39mpad_mode,\n\u001b[0;32m   2140\u001b[0m )\n\u001b[0;32m   2142\u001b[0m \u001b[38;5;66;03m# Build a Mel filter\u001b[39;00m\n\u001b[1;32m-> 2143\u001b[0m mel_basis \u001b[38;5;241m=\u001b[39m \u001b[43mfilters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2145\u001b[0m melspec: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...ft,mf->...mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, S, mel_basis, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m melspec\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\filters.py:239\u001b[0m, in \u001b[0;36mmel\u001b[1;34m(sr, n_fft, n_mels, fmin, fmax, htk, norm, dtype)\u001b[0m\n\u001b[0;32m    236\u001b[0m     upper \u001b[38;5;241m=\u001b[39m ramps[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m/\u001b[39m fdiff[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# .. then intersect them with each other and zero\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m     weights[i] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(norm, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslaney\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;66;03m# Slaney-style mel is scaled to be approx constant energy per channel\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
=======
>>>>>>> d03adc8a092c0446f4d36399e88aa5d66e0ab999
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python 3",
=======
   "display_name": ".venv",
>>>>>>> d03adc8a092c0446f4d36399e88aa5d66e0ab999
   "language": "python",
   "name": "python3"
  },
  "language_info": {
<<<<<<< HEAD
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
=======
   "name": "python",
   "version": "3.12.3"
>>>>>>> d03adc8a092c0446f4d36399e88aa5d66e0ab999
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
