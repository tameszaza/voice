{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  orthogonal loss \n",
    "This model work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from glob import glob\n",
    "from phonemizer import phonemize\n",
    "from torch.nn.utils import spectral_norm\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a FLAC audio file and resample it to the target sample rate\n",
    "def load_flac(file_path, target_sr=16000):\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
    "    max_val = np.max(np.abs(audio))\n",
    "    if max_val > 0:\n",
    "        audio = audio / max_val  # Normalize to [-1, 1]\n",
    "    return audio\n",
    "\n",
    "# Pad or trim audio to the target length\n",
    "def pad_or_trim(audio, target_length=64000):\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:target_length]\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the transcription for a given audio file from the corresponding .trans.txt file\n",
    "def get_transcription(file_path):\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "    base_name = os.path.basename(file_path)\n",
    "    file_id = os.path.splitext(base_name)[0]\n",
    "\n",
    "    # Locate the transcription file\n",
    "    transcription_file = None\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith(\".trans.txt\"):\n",
    "            transcription_file = os.path.join(dir_path, file)\n",
    "            break\n",
    "\n",
    "    if not transcription_file:\n",
    "        raise FileNotFoundError(f\"No transcription file found in {dir_path}\")\n",
    "\n",
    "    # Read the transcription file and find the transcription for the current audio file\n",
    "    with open(transcription_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\" \", 1)\n",
    "            if parts[0] == file_id:\n",
    "                transcription = parts[1]\n",
    "                return transcription\n",
    "\n",
    "    raise ValueError(f\"No transcription found for file {file_id}\")\n",
    "\n",
    "# Extract phonetic features from transcription\n",
    "def get_phonetic_features(transcription, max_length=100):\n",
    "    phonemes = phonemize(transcription, backend=\"espeak\", language=\"en-us\")\n",
    "    phoneme_to_id = {char: idx for idx, char in enumerate(sorted(set(phonemes)))}\n",
    "    phonetic_features = [phoneme_to_id[p] for p in phonemes]\n",
    "\n",
    "    # Convert to tensor and pad/truncate\n",
    "    phonetic_features = torch.tensor(phonetic_features, dtype=torch.float32)\n",
    "    if len(phonetic_features) < max_length:\n",
    "        phonetic_features = nn.functional.pad(phonetic_features, (0, max_length - len(phonetic_features)))\n",
    "    else:\n",
    "        phonetic_features = phonetic_features[:max_length]\n",
    "    return phonetic_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset and create a TensorDataset\n",
    "def preprocess_dataset(root_dir, target_sr=16000, target_length=64000, feature_length=100):\n",
    "    flac_files = glob(os.path.join(root_dir, '**', '*.flac'), recursive=True)\n",
    "    print(f\"Found {len(flac_files)} .flac files in {root_dir}.\")\n",
    "    if len(flac_files) == 0:\n",
    "        print(\"No .flac files found. Please check the root_dir path.\")\n",
    "    audio_dataset = []\n",
    "    feature_dataset = []\n",
    "    for file in flac_files:\n",
    "        try:\n",
    "            audio = load_flac(file, target_sr)\n",
    "            audio = pad_or_trim(audio, target_length)\n",
    "            transcription = get_transcription(file)\n",
    "            phonetic_features = get_phonetic_features(transcription, max_length=feature_length)\n",
    "            audio_dataset.append(audio)\n",
    "            feature_dataset.append(phonetic_features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "    audio_dataset = torch.tensor(audio_dataset, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
    "    feature_dataset = torch.stack(feature_dataset)  # Stack tensors\n",
    "    return TensorDataset(audio_dataset, feature_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a waveform to an audio file\n",
    "def save_waveform_to_audio(waveform, sample_rate, filename):\n",
    "    if isinstance(waveform, torch.Tensor):\n",
    "        waveform = waveform.detach().cpu().numpy()\n",
    "    waveform = np.squeeze(waveform)\n",
    "    max_val = np.max(np.abs(waveform))\n",
    "    if max_val > 0:\n",
    "        waveform = waveform / max_val\n",
    "    sf.write(filename, waveform, sample_rate)\n",
    "\n",
    "# Verify waveform-to-audio conversion using preprocessed dataset\n",
    "def verify_waveform_to_audio(root_dir, sample_rate=16000, target_length=64000, output_dir=\"verified_audio\"):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    dataset = preprocess_dataset(root_dir, sample_rate, target_length)\n",
    "    num_samples_to_verify = min(5, len(dataset))\n",
    "    for idx in range(num_samples_to_verify):\n",
    "        waveform = dataset[idx][0]  # Access audio data\n",
    "        filename = os.path.join(output_dir, f\"example_waveform_{idx+1}.wav\")\n",
    "        save_waveform_to_audio(waveform, sample_rate, filename)\n",
    "        print(f\"Waveform saved to {filename}\")\n",
    "        # Plot the waveform\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(waveform.numpy().squeeze())\n",
    "        plt.title(f\"Waveform {idx+1}\")\n",
    "        plt.xlabel(\"Sample Index\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate noise for the generator\n",
    "def generate_noise(batch_size, z_dim, device):\n",
    "    return torch.randn(batch_size, z_dim).to(device)\n",
    "\n",
    "# Orthogonal loss function\n",
    "def orthogonal_loss(feature1, feature2):\n",
    "    inner_product = torch.sum(feature1 * feature2, dim=1)\n",
    "    norm1 = torch.norm(feature1, dim=1)\n",
    "    norm2 = torch.norm(feature2, dim=1)\n",
    "    cosine_similarity = inner_product / (norm1 * norm2 + 1e-8)\n",
    "    return torch.mean(cosine_similarity**2)  # Minimize the cosine similarity to make vectors orthogonal\n",
    "\n",
    "# Compute gradient penalty for WGAN-GP\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, conditions, device):\n",
    "    batch_size = real_samples.size(0)\n",
    "    epsilon = torch.rand(batch_size, 1, 1, device=device)  # Shape: [batch_size, 1, 1]\n",
    "    epsilon = epsilon.expand_as(real_samples)  # Expand to match `real_samples` shape\n",
    "\n",
    "    # Interpolate between real and fake samples\n",
    "    interpolates = (epsilon * real_samples + (1 - epsilon) * fake_samples).requires_grad_(True)\n",
    "\n",
    "    # Pass interpolates, fake_samples, and conditions to the discriminator\n",
    "    real_outputs, fake_outputs = discriminator(interpolates, fake_samples, conditions)\n",
    "\n",
    "    # Compute gradient penalty for each output\n",
    "    gradient_penalties = []\n",
    "    for real_output in real_outputs:\n",
    "        grad_outputs = torch.ones_like(real_output, device=device)\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=real_output,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=grad_outputs,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "\n",
    "        # Reshape gradients and compute L2 norm\n",
    "        gradients = gradients.view(batch_size, -1)  # Flatten\n",
    "        gradient_norm = gradients.norm(2, dim=1)  # L2 norm\n",
    "        gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "        gradient_penalties.append(gradient_penalty)\n",
    "\n",
    "    # Average all gradient penalties\n",
    "    return sum(gradient_penalties) / len(gradient_penalties)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save_generated_waveforms(generators, z_dim, features, num_waveforms, device, epoch, sample_rate=16000, output_dir='generated_audio'):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for idx, gen in enumerate(generators):\n",
    "        gen.eval()\n",
    "        with torch.no_grad():\n",
    "            noise = generate_noise(num_waveforms, z_dim, device)\n",
    "            fake_waveforms = gen(features[:num_waveforms].to(device), noise).cpu()\n",
    "            \n",
    "            # Adjust num_waveforms to the actual size of fake_waveforms\n",
    "            num_available_waveforms = fake_waveforms.size(0)\n",
    "            if num_available_waveforms < num_waveforms:\n",
    "                print(f\"Warning: Requested {num_waveforms} waveforms, but generator produced {num_available_waveforms}\")\n",
    "                num_waveforms = num_available_waveforms\n",
    "\n",
    "            for i in range(num_waveforms):\n",
    "                waveform = fake_waveforms[i]\n",
    "                filename = f'epoch{epoch+1}_gen{idx+1}_sample{i+1}.wav'\n",
    "                filepath = os.path.join(output_dir, filename)\n",
    "                save_waveform_to_audio(waveform, sample_rate, filepath)\n",
    "                print(f\"Saved {filepath}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels=1, z_channels=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.z_channels = z_channels\n",
    "\n",
    "        self.preprocess = nn.Conv1d(1, 768, kernel_size=3, padding=1)\n",
    "\n",
    "        self.gblocks = nn.ModuleList([\n",
    "            GBlock(768, 768, z_channels, 5),  # Upsample by 4\n",
    "            GBlock(768, 768, z_channels, 4),  # Upsample by 4\n",
    "            GBlock(768, 384, z_channels, 4),  # Upsample by 4\n",
    "            GBlock(384, 384, z_channels, 4),  # Upsample by 2\n",
    "            GBlock(384, 192, z_channels, 2),  # Upsample by 2\n",
    "        ])\n",
    "        self.postprocess = nn.Sequential(\n",
    "            nn.Conv1d(192, 1, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, inputs, z):\n",
    "        # print(f\"Input shape: {inputs.shape}\")  # Debug input shape\n",
    "        inputs = self.preprocess(inputs)\n",
    "        outputs = inputs\n",
    "        for i, layer in enumerate(self.gblocks):\n",
    "            outputs = layer(outputs, z)\n",
    "            # print(f\"After GBlock {i}: {outputs.shape}\")  # Debug output shape\n",
    "        outputs = self.postprocess(outputs)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "class GBlock(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, z_channels, upsample_factor):\n",
    "        super(GBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.z_channels = z_channels\n",
    "        self.upsample_factor = upsample_factor\n",
    "\n",
    "        # GroupNorm for memory efficiency\n",
    "        self.condition_norm1 = nn.GroupNorm(32, in_channels)\n",
    "        self.first_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            UpsampleNet(in_channels, in_channels, upsample_factor),\n",
    "            nn.Conv1d(in_channels, hidden_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.condition_norm2 = nn.GroupNorm(32, hidden_channels)\n",
    "        self.second_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, dilation=1, padding=1)\n",
    "        )\n",
    "        self.residual1 = nn.Sequential(\n",
    "            UpsampleNet(in_channels, in_channels, upsample_factor),\n",
    "            nn.Conv1d(in_channels, hidden_channels, kernel_size=1)\n",
    "        )\n",
    "        self.condition_norm3 = nn.GroupNorm(32, hidden_channels)\n",
    "        self.third_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        )\n",
    "        self.condition_norm4 = nn.GroupNorm(32, hidden_channels)\n",
    "        self.fourth_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, condition, z):\n",
    "        def run_forward(inputs):\n",
    "            outputs = self.condition_norm1(inputs)\n",
    "            outputs = self.first_stack(outputs)\n",
    "            outputs = self.condition_norm2(outputs)\n",
    "            outputs = self.second_stack(outputs)\n",
    "            residual_outputs = self.residual1(inputs) + outputs\n",
    "            outputs = self.condition_norm3(residual_outputs)\n",
    "            outputs = self.third_stack(outputs)\n",
    "            outputs = self.condition_norm4(outputs)\n",
    "            outputs = self.fourth_stack(outputs)\n",
    "            outputs = outputs + residual_outputs\n",
    "            return outputs\n",
    "\n",
    "        # Gradient checkpointing to save memory\n",
    "        outputs = checkpoint.checkpoint(run_forward, condition)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, upsample_factor):\n",
    "        super(UpsampleNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.upsample_factor = upsample_factor\n",
    "\n",
    "        layer = nn.ConvTranspose1d(input_size, output_size, upsample_factor * 2,\n",
    "                                   upsample_factor, padding=upsample_factor // 2)\n",
    "        nn.init.orthogonal_(layer.weight)\n",
    "        self.layer = spectral_norm(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.layer(inputs)\n",
    "        outputs = outputs[:, :, : inputs.size(-1) * self.upsample_factor]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalBatchNorm1d(nn.Module):\n",
    "    \"\"\"Conditional Batch Normalization\"\"\"\n",
    "    def __init__(self, num_features, z_channels=128):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.z_channels = z_channels\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features, affine=False)\n",
    "        self.layer = spectral_norm(nn.Linear(z_channels, num_features * 2))\n",
    "        self.layer.weight.data.normal_(1, 0.02)\n",
    "        self.layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, inputs, noise):\n",
    "        outputs = self.batch_norm(inputs)\n",
    "        gamma, beta = self.layer(noise).chunk(2, 1)\n",
    "        gamma = gamma.view(-1, self.num_features, 1)\n",
    "        beta = beta.view(-1, self.num_features, 1)\n",
    "        outputs = gamma * outputs + beta\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator architecture using Conv1d layers\n",
    "class Multiple_Random_Window_Discriminators(nn.Module):\n",
    "    def __init__(self, lc_channels, window_size=(2, 4, 8, 16, 30), upsample_factor=120):\n",
    "        super(Multiple_Random_Window_Discriminators, self).__init__()\n",
    "        self.lc_channels = lc_channels\n",
    "        self.window_size = window_size\n",
    "        self.upsample_factor = upsample_factor\n",
    "\n",
    "        self.udiscriminators = nn.ModuleList([\n",
    "            UnConditionalDBlocks(in_channels=1, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=2, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=4, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=8, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=15, factors=(2, 2), out_channels=(128, 256)),\n",
    "        ])\n",
    "\n",
    "        self.discriminators = nn.ModuleList([\n",
    "            ConditionalDBlocks(in_channels=1, lc_channels=lc_channels,\n",
    "                               factors=(5, 3, 2, 2, 2), out_channels=(128, 128, 256, 256)),\n",
    "            ConditionalDBlocks(in_channels=2, lc_channels=lc_channels,\n",
    "                               factors=(5, 3, 2, 2), out_channels=(128, 256, 256)),\n",
    "            ConditionalDBlocks(in_channels=4, lc_channels=lc_channels,\n",
    "                               factors=(5, 3, 2), out_channels=(128, 256)),\n",
    "            ConditionalDBlocks(in_channels=8, lc_channels=lc_channels,\n",
    "                               factors=(5, 3), out_channels=(256,)),\n",
    "            ConditionalDBlocks(in_channels=15, lc_channels=lc_channels,\n",
    "                               factors=(2, 2, 2), out_channels=(128, 256)),\n",
    "        ])\n",
    "\n",
    "    def forward(self, real_samples, fake_samples, conditions):\n",
    "        real_outputs, fake_outputs = [], []\n",
    "\n",
    "        # Unconditional discriminator\n",
    "        for (size, layer) in zip(self.window_size, self.udiscriminators):\n",
    "            size = size * self.upsample_factor\n",
    "            if real_samples.size(-1) < size:\n",
    "                raise ValueError(f\"Window size {size} is too large for input with length {real_samples.size(-1)}\")\n",
    "\n",
    "            index = np.random.randint(0, real_samples.size(-1) - size + 1)\n",
    "            #print(f\"Unconditional index: {index}, size: {size}, slice: {index}:{index + size}\")\n",
    "\n",
    "            real_slice = real_samples[:, :, index: index + size]\n",
    "            fake_slice = fake_samples[:, :, index: index + size]\n",
    "\n",
    "            if real_slice.size(-1) == 0 or fake_slice.size(-1) == 0:\n",
    "                raise ValueError(f\"Generated slice has zero length: real_slice.shape={real_slice.shape}, fake_slice.shape={fake_slice.shape}\")\n",
    "\n",
    "            real_output = layer(real_slice)\n",
    "            fake_output = layer(fake_slice)\n",
    "\n",
    "            real_outputs.append(real_output)\n",
    "            fake_outputs.append(fake_output)\n",
    "\n",
    "        # Conditional discriminator\n",
    "        for (size, layer) in zip(self.window_size, self.discriminators):\n",
    "            lc_index = np.random.randint(0, conditions.size(-1) - size + 1)\n",
    "            sample_index = lc_index * self.upsample_factor\n",
    "\n",
    "            if real_samples.size(-1) < (lc_index + size) * self.upsample_factor:\n",
    "                raise ValueError(f\"Window size exceeds input size for conditional discriminator.\")\n",
    "\n",
    "            real_x = real_samples[:, :, sample_index: (lc_index + size) * self.upsample_factor]\n",
    "            fake_x = fake_samples[:, :, sample_index: (lc_index + size) * self.upsample_factor]\n",
    "            lc = conditions[:, :, lc_index: lc_index + size]\n",
    "\n",
    "            if real_x.size(-1) == 0 or fake_x.size(-1) == 0:\n",
    "                raise ValueError(f\"Generated slice has zero length: real_x.shape={real_x.shape}, fake_x.shape={fake_x.shape}\")\n",
    "\n",
    "            real_output = layer(real_x, lc)\n",
    "            fake_output = layer(fake_x, lc)\n",
    "\n",
    "            real_outputs.append(real_output)\n",
    "            fake_outputs.append(fake_output)\n",
    "\n",
    "        return real_outputs, fake_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample_factor):\n",
    "        super(DBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.AvgPool1d(downsample_factor, stride=downsample_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        )\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1),\n",
    "            nn.AvgPool1d(downsample_factor, stride=downsample_factor)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.layers(inputs) + self.residual(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondDBlock(nn.Module):\n",
    "    def __init__(self, in_channels, lc_channels, upsample_factor):\n",
    "        super(CondDBlock, self).__init__()\n",
    "        self.lc_conv1d = nn.Conv1d(lc_channels, in_channels, kernel_size=1)\n",
    "        self.start = nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.end = nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.residual = nn.Conv1d(in_channels, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, inputs, conditions):\n",
    "        # Ensure conditions are processed to match inputs\n",
    "        conditions = self.lc_conv1d(conditions)\n",
    "        # print(f\"Conditions shape after lc_conv1d: {conditions.shape}\")  # Debug\n",
    "        outputs = self.start(inputs) + conditions\n",
    "        outputs = self.end(outputs)\n",
    "        residual_outputs = self.residual(inputs)\n",
    "        return outputs + residual_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalDBlocks(nn.Module):\n",
    "    def __init__(self, in_channels, lc_channels, factors=(2, 2, 2), out_channels=(128, 256)):\n",
    "        super(ConditionalDBlocks, self).__init__()\n",
    "        assert len(factors) == len(out_channels) + 1\n",
    "        self.in_channels = in_channels\n",
    "        self.lc_channels = lc_channels\n",
    "        self.factors = factors\n",
    "        self.out_channels = out_channels\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(DBlock(in_channels, 64, 1))\n",
    "        in_channels = 64\n",
    "        for i, channel in enumerate(out_channels):\n",
    "            self.layers.append(DBlock(in_channels, channel, factors[i]))\n",
    "            in_channels = channel\n",
    "        self.cond_layer = CondDBlock(in_channels, lc_channels, factors[-1])\n",
    "        \n",
    "        # New adjustment layer to match post_process input\n",
    "        self.adjust_channels = nn.Conv1d(in_channels, 512, kernel_size=1)\n",
    "\n",
    "        self.post_process = nn.ModuleList([\n",
    "            DBlock(512, 512, 1),\n",
    "            DBlock(512, 512, 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, inputs, conditions):\n",
    "        batch_size = inputs.size(0)\n",
    "        # print(f\"Initial inputs shape: {inputs.shape}, conditions shape: {conditions.shape}\")  # Debug\n",
    "\n",
    "        # Reshape inputs\n",
    "        outputs = inputs.view(batch_size, self.in_channels, -1)\n",
    "        # print(f\"After view: {outputs.shape}\")  # Debug\n",
    "\n",
    "        # Process through layers\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            outputs = layer(outputs)\n",
    "            # print(f\"After layer {i}: {outputs.shape}\")  # Debug\n",
    "\n",
    "        # Adjust conditions to match temporal dimension\n",
    "        conditions = F.adaptive_avg_pool1d(conditions, output_size=1)  # Reduce temporal dimension to 1\n",
    "        # print(f\"Conditions shape after pooling: {conditions.shape}\")  # Debug\n",
    "\n",
    "        conditions = conditions.expand(-1, self.lc_channels, outputs.size(-1))  # Expand to match outputs\n",
    "        # print(f\"Conditions shape after expand: {conditions.shape}\")  # Debug\n",
    "\n",
    "        # Apply cond_layer\n",
    "        outputs = self.cond_layer(outputs, conditions)\n",
    "        #print(f\"After cond_layer: {outputs.shape}\")  # Debug\n",
    "\n",
    "        # Adjust channels\n",
    "        outputs = self.adjust_channels(outputs)\n",
    "        #print(f\"After adjust_channels: {outputs.shape}\")  # Debug\n",
    "\n",
    "        # Post-process\n",
    "        for i, layer in enumerate(self.post_process):\n",
    "            outputs = layer(outputs)\n",
    "            #print(f\"After post_process layer {i}: {outputs.shape}\")  # Debug\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnConditionalDBlocks(nn.Module):\n",
    "    def __init__(self, in_channels, factors=(5, 3), out_channels=(128, 256)):\n",
    "        super(UnConditionalDBlocks, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.factors = factors\n",
    "        self.out_channels = out_channels\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(DBlock(in_channels, 64, 1))\n",
    "        in_channels = 64\n",
    "        for (i, factor) in enumerate(factors):\n",
    "            self.layers.append(DBlock(in_channels, out_channels[i], factor))\n",
    "            in_channels = out_channels[i]\n",
    "        self.layers.append(DBlock(in_channels, in_channels, 1))\n",
    "        self.layers.append(DBlock(in_channels, in_channels, 1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size()[0]\n",
    "        outputs = inputs.view(batch_size, self.in_channels, -1)\n",
    "        for layer in self.layers:\n",
    "            outputs = layer(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, audio_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.audio_length = audio_length\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=5, stride=2, padding=2),  # Downsample\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=2),  # Downsample\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2),  # Downsample\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool1d(1)  # Reduce to (batch_size, 128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)  # Shape: (batch_size, 128)\n",
    "\n",
    "\n",
    "def orthogonal_loss(features1, features2):\n",
    "    \"\"\"Compute orthogonal loss between two feature sets.\"\"\"\n",
    "    return torch.mean(torch.sum(features1 * features2, dim=1)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_with_pretrained_generators(\n",
    "    pretrained_generator, num_epochs, z_dim, lr_gen, lr_disc, batch_size, train_dataset,\n",
    "    num_generators, seed, audio_length, output_dir, lambda_gp=10, lambda_ortho=0.1, num_critic=5,\n",
    "    checkpoint_path=\"checkpoint.pth\", resume=False\n",
    "):\n",
    "    set_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize multiple generators with pretrained weights\n",
    "    generators = []\n",
    "    for _ in range(num_generators):\n",
    "        gen = Generator(z_dim, audio_length).to(device)\n",
    "        gen.load_state_dict(pretrained_generator.state_dict())\n",
    "        generators.append(gen)\n",
    "\n",
    "    # Initialize Discriminator and Encoder\n",
    "    discriminator = Multiple_Random_Window_Discriminators(lc_channels=1).to(device)\n",
    "    encoder = Encoder(audio_length).to(device)\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_gens = [optim.Adam(gen.parameters(), lr=lr_gen, betas=(0.5, 0.9)) for gen in generators]\n",
    "    optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "    optimizer_encoder = optim.Adam(encoder.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "    # Mixed precision scalers\n",
    "    scaler_gens = [GradScaler() for _ in range(num_generators)]\n",
    "    scaler_disc = GradScaler()\n",
    "    scaler_encoder = GradScaler()\n",
    "\n",
    "    # Load checkpoint if resuming\n",
    "    start_epoch = 0\n",
    "    if resume and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        for idx, gen in enumerate(generators):\n",
    "            gen.load_state_dict(checkpoint['generator_state_dicts'][idx])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "        for idx, opt_gen in enumerate(optimizer_gens):\n",
    "            opt_gen.load_state_dict(checkpoint['optimizer_gen_state_dicts'][idx])\n",
    "        optimizer_disc.load_state_dict(checkpoint['optimizer_disc_state_dict'])\n",
    "        optimizer_encoder.load_state_dict(checkpoint['optimizer_encoder_state_dict'])\n",
    "        print(f\"Resumed training from epoch {start_epoch}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        for batch_idx, (real_audio, features) in enumerate(train_loader):\n",
    "            real_audio = real_audio.to(device)\n",
    "            features = features.unsqueeze(1).to(device)\n",
    "            batch_size = real_audio.size(0)\n",
    "\n",
    "            # Train Discriminator\n",
    "            for _ in range(num_critic):\n",
    "                optimizer_disc.zero_grad()\n",
    "                noises = [generate_noise(batch_size, z_dim, device) for _ in range(num_generators)]\n",
    "\n",
    "                with autocast(device_type='cuda'):\n",
    "                    fakes = [gen(features, noises[idx]).detach() for idx, gen in enumerate(generators)]\n",
    "                    real_outputs, fake_outputs = discriminator(real_audio, torch.stack(fakes), features)\n",
    "                    loss_disc = sum(torch.mean(fake) - torch.mean(real) for real, fake in zip(real_outputs, fake_outputs))\n",
    "                    gradient_penalty = compute_gradient_penalty(discriminator, real_audio, fakes[0], features, device)\n",
    "                    loss_disc += lambda_gp * gradient_penalty\n",
    "\n",
    "                scaler_disc.scale(loss_disc).backward()\n",
    "                scaler_disc.step(optimizer_disc)\n",
    "                scaler_disc.update()\n",
    "\n",
    "            # Train Generators and Encoder\n",
    "            for idx, gen in enumerate(generators):\n",
    "                optimizer_gens[idx].zero_grad()\n",
    "                optimizer_encoder.zero_grad()\n",
    "\n",
    "                noise = generate_noise(batch_size, z_dim, device)\n",
    "                with autocast(device_type='cuda'):\n",
    "                    fake = gen(features, noise)\n",
    "                    fake_outputs = discriminator(fake, fake, features)[1]\n",
    "                    loss_gen = -torch.mean(fake_outputs)\n",
    "\n",
    "                    # Orthogonal loss\n",
    "                    gen_feature = encoder(fake)\n",
    "                    ortho_loss = 0\n",
    "                    for other_idx, other_gen in enumerate(generators):\n",
    "                        if idx != other_idx:\n",
    "                            other_noise = generate_noise(batch_size, z_dim, device)\n",
    "                            other_fake = other_gen(features, other_noise)\n",
    "                            other_feature = encoder(other_fake)\n",
    "                            ortho_loss += orthogonal_loss(gen_feature, other_feature)\n",
    "                    ortho_loss /= (num_generators - 1)\n",
    "                    total_loss_gen = loss_gen + lambda_ortho * ortho_loss\n",
    "\n",
    "                scaler_gens[idx].scale(total_loss_gen).backward()\n",
    "                scaler_gens[idx].step(optimizer_gens[idx])\n",
    "                scaler_gens[idx].update()\n",
    "                scaler_encoder.step(optimizer_encoder)\n",
    "                scaler_encoder.update()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {loss_disc.item():.4f}, Loss G: {loss_gen.item():.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'generator_state_dicts': [gen.state_dict() for gen in generators],\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'encoder_state_dict': encoder.state_dict(),\n",
    "            'optimizer_gen_state_dicts': [opt.state_dict() for opt in optimizer_gens],\n",
    "            'optimizer_disc_state_dict': optimizer_disc.state_dict(),\n",
    "            'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensors_to_match(tensor_list):\n",
    "    # Find the maximum size in each dimension\n",
    "    max_shape = list(max(tensor.size(dim) for tensor in tensor_list) for dim in range(tensor_list[0].dim()))\n",
    "\n",
    "    padded_tensors = []\n",
    "    for tensor in tensor_list:\n",
    "        # Calculate the padding needed for each dimension\n",
    "        pad = []\n",
    "        for dim, max_dim in enumerate(max_shape[::-1]):\n",
    "            pad.extend([0, max_dim - tensor.size(dim)])\n",
    "        pad = pad[::-1]  # Reverse padding list\n",
    "        # Apply padding\n",
    "        padded_tensor = torch.nn.functional.pad(tensor, pad)\n",
    "        padded_tensors.append(padded_tensor)\n",
    "    \n",
    "    return torch.stack(padded_tensors, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "def pretrain_single_generator(\n",
    "    num_epochs, z_dim, lr_gen, lr_disc, batch_size, seed,\n",
    "    audio_length, output_dir, train_dataset, checkpoint_path=\"checkpoint.pth\",\n",
    "    resume=False\n",
    "):\n",
    "    set_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define models and move to device\n",
    "    generator = Generator(in_channels=train_dataset[0][1].shape[0], z_channels=z_dim).to(device)\n",
    "    discriminator = Multiple_Random_Window_Discriminators(lc_channels=train_dataset[0][1].shape[0]).to(device)\n",
    "\n",
    "    optimizer_gen = optim.Adam(generator.parameters(), lr=lr_gen, betas=(0.5, 0.9))\n",
    "    optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "    # Mixed precision scaler\n",
    "    scaler_gen = GradScaler()\n",
    "    scaler_disc = GradScaler()\n",
    "\n",
    "    start_epoch = 0\n",
    "    if resume and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        optimizer_gen.load_state_dict(checkpoint['optimizer_gen_state_dict'])\n",
    "        optimizer_disc.load_state_dict(checkpoint['optimizer_disc_state_dict'])\n",
    "        scaler_gen.load_state_dict(checkpoint['scaler_gen'])\n",
    "        scaler_disc.load_state_dict(checkpoint['scaler_disc'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        for batch_idx, (real_audio, features) in enumerate(train_loader):\n",
    "            real_audio = real_audio.to(device)\n",
    "            features = features.unsqueeze(1).to(device)\n",
    "            batch_size = real_audio.size(0)\n",
    "\n",
    "            # Train Discriminator\n",
    "            for _ in range(5):\n",
    "                optimizer_disc.zero_grad()\n",
    "                noise = generate_noise(batch_size, z_dim, device)\n",
    "\n",
    "                with autocast(device_type='cuda'):\n",
    "                    # Detach fake_audio to avoid grad on generator when training disc\n",
    "                    fake_audio = generator(features, noise).detach()\n",
    "                    real_outputs, fake_outputs = discriminator(real_audio, fake_audio, features)\n",
    "                    loss_disc = sum(torch.mean(fake) - torch.mean(real) for real, fake in zip(real_outputs, fake_outputs))\n",
    "                    gradient_penalty = compute_gradient_penalty(discriminator, real_audio, fake_audio, features, device)\n",
    "                    loss_disc += 10 * gradient_penalty\n",
    "\n",
    "                scaler_disc.scale(loss_disc).backward()\n",
    "                scaler_disc.step(optimizer_disc)\n",
    "                scaler_disc.update()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_gen.zero_grad()\n",
    "            noise = generate_noise(batch_size, z_dim, device)\n",
    "\n",
    "            with autocast(device_type='cuda'):\n",
    "                fake_audio = generator(features, noise)\n",
    "                fake_outputs = discriminator(fake_audio, fake_audio, features)[1]\n",
    "                # Adjust if needed, e.g. if fake_outputs is a list of tensors\n",
    "                # Ensure pad_tensors_to_match is defined and imports are correct\n",
    "                fake_outputs = pad_tensors_to_match(fake_outputs)\n",
    "                loss_gen = -torch.mean(fake_outputs)\n",
    "\n",
    "            scaler_gen.scale(loss_gen).backward()\n",
    "            scaler_gen.step(optimizer_gen)\n",
    "            scaler_gen.update()\n",
    "\n",
    "        # Open the file in append mode\n",
    "        with open(\"output.txt\", \"a\") as file:\n",
    "            # Write the output to the file\n",
    "            file.write(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {loss_disc.item():.4f}, Loss G: {loss_gen.item():.4f}\\n\")\n",
    "\n",
    "\n",
    "        # Save model output\n",
    "        visualize_and_save_generated_waveforms(\n",
    "            [generator], z_dim, features, num_waveforms=5, device=device, epoch=epoch, sample_rate=16000, output_dir=output_dir\n",
    "        )\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch+1,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_gen_state_dict': optimizer_gen.state_dict(),\n",
    "            'optimizer_disc_state_dict': optimizer_disc.state_dict(),\n",
    "            'scaler_gen': scaler_gen.state_dict(),\n",
    "            'scaler_disc': scaler_disc.state_dict(),\n",
    "            'z_dim': z_dim,\n",
    "            'lr_gen': lr_gen,\n",
    "            'lr_disc': lr_disc,\n",
    "            'batch_size': batch_size,\n",
    "            'seed': seed,\n",
    "            'audio_length': audio_length,\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "        torch.save(generator.state_dict(), os.path.join(output_dir, f\"pretrained_generator_epoch{epoch+1}.pth\"))\n",
    "\n",
    "    print(\"Pretraining complete.\")\n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pretrain_single_generator(num_epochs, z_dim, lr_gen, lr_disc, batch_size, seed, audio_length, output_dir, train_dataset):\n",
    "#     set_seed(seed)\n",
    "#     # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     # # Define the single generator and discriminator\n",
    "#     # generator = Generator(in_channels=1, z_channels=z_dim).to(device)\n",
    "\n",
    "#     # discriminator = Multiple_Random_Window_Discriminators(lc_channels=1)\n",
    "\n",
    "# # In CondDBlock (and related blocks), ensure lc_conv1d is defined as:\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "#     # Move models to device\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     generator = Generator(in_channels=train_dataset[0][1].shape[0], z_channels=z_dim).to(device)\n",
    "#     discriminator = Multiple_Random_Window_Discriminators(lc_channels=train_dataset[0][1].shape[0]).to(device)\n",
    "#     optimizer_gen = optim.Adam(generator.parameters(), lr=lr_gen, betas=(0.5, 0.9))\n",
    "#     optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "# # Training loop\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         torch.cuda.empty_cache()\n",
    "#         for batch_idx, (real_audio, features) in enumerate(train_loader):\n",
    "#             real_audio = real_audio.to(device)\n",
    "#             features = features.unsqueeze(1).to(device)\n",
    "#             print(f\"real_audio.shape: {real_audio.shape}\")\n",
    "#             print(f\"features.shape: {features.shape}\")\n",
    "            \n",
    "\n",
    "\n",
    "#             batch_size = real_audio.size(0)\n",
    "\n",
    "#             # Train Discriminator\n",
    "#             for _ in range(5):\n",
    "#                 optimizer_disc.zero_grad()\n",
    "#                 noise = generate_noise(batch_size, z_dim, device)\n",
    "#                 fake_audio = generator(features, noise).detach()\n",
    "#                 real_outputs, fake_outputs = discriminator(real_audio, fake_audio, features)\n",
    "#                 loss_disc = sum([torch.mean(fake) - torch.mean(real) for real, fake in zip(real_outputs, fake_outputs)])\n",
    "#                 gradient_penalty = compute_gradient_penalty(discriminator, real_audio, fake_audio, features, device)\n",
    "#                 loss_disc += 10 * gradient_penalty\n",
    "#                 loss_disc.backward()\n",
    "#                 optimizer_disc.step()\n",
    "\n",
    "#             # Train Generator\n",
    "#             optimizer_gen.zero_grad()\n",
    "#             noise = generate_noise(batch_size, z_dim, device)\n",
    "#             fake_audio = generator(features, noise)\n",
    "#             print(f\"fake_audio.shape: {fake_audio.shape}\")\n",
    "#             fake_outputs = discriminator(fake_audio, fake_audio, features)[1]\n",
    "#             fake_outputs = pad_tensors_to_match(fake_outputs)  # Ensure uniform tensor size\n",
    "#             loss_gen = -torch.mean(fake_outputs)\n",
    "#             loss_gen.backward()\n",
    "#             optimizer_gen.step()\n",
    "\n",
    "#         print(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {loss_disc.item():.4f}, Loss G: {loss_gen.item():.4f}\")\n",
    "\n",
    "#         visualize_and_save_generated_waveforms(\n",
    "#             [generator], z_dim, features, num_waveforms=5, device=device, epoch=epoch, sample_rate=16000, output_dir=output_dir\n",
    "#         )\n",
    "#         torch.save(generator.state_dict(), os.path.join(output_dir, f\"pretrained_generator_epoch{epoch+1}.pth\"))\n",
    "\n",
    "#     print(\"Pretraining complete.\")\n",
    "#     return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "audio_length = 64000\n",
    "z_dim = 128\n",
    "lr_gen = 0.0002\n",
    "lr_disc = 0.0002\n",
    "batch_size = 12\n",
    "num_epochs = 50\n",
    "root_dir = \"./data\"\n",
    "sample_rate = 16000\n",
    "num_generators = 5\n",
    "output_dir = 'generated_audio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2703 .flac files in ./data.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_sr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_length\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m, in \u001b[0;36mpreprocess_dataset\u001b[0;34m(root_dir, target_sr, target_length, feature_length)\u001b[0m\n\u001b[1;32m     12\u001b[0m audio \u001b[38;5;241m=\u001b[39m pad_or_trim(audio, target_length)\n\u001b[1;32m     13\u001b[0m transcription \u001b[38;5;241m=\u001b[39m get_transcription(file)\n\u001b[0;32m---> 14\u001b[0m phonetic_features \u001b[38;5;241m=\u001b[39m \u001b[43mget_phonetic_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranscription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m audio_dataset\u001b[38;5;241m.\u001b[39mappend(audio)\n\u001b[1;32m     16\u001b[0m feature_dataset\u001b[38;5;241m.\u001b[39mappend(phonetic_features)\n",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m, in \u001b[0;36mget_phonetic_features\u001b[0;34m(transcription, max_length)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_phonetic_features\u001b[39m(transcription, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m---> 29\u001b[0m     phonemes \u001b[38;5;241m=\u001b[39m \u001b[43mphonemize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranscription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mespeak\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men-us\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     phoneme_to_id \u001b[38;5;241m=\u001b[39m {char: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, char \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(phonemes)))}\n\u001b[1;32m     31\u001b[0m     phonetic_features \u001b[38;5;241m=\u001b[39m [phoneme_to_id[p] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m phonemes]\n",
      "File \u001b[0;32m~/Documents/myenv/lib/python3.12/site-packages/phonemizer/phonemize.py:206\u001b[0m, in \u001b[0;36mphonemize\u001b[0;34m(text, language, backend, separator, strip, prepend_text, preserve_empty_lines, preserve_punctuation, punctuation_marks, with_stress, tie, language_switch, words_mismatch, njobs, logger)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# initialize the phonemization backend\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mespeak\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 206\u001b[0m     phonemizer \u001b[38;5;241m=\u001b[39m \u001b[43mBACKENDS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpunctuation_marks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpunctuation_marks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreserve_punctuation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_punctuation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_stress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_stress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtie\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtie\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguage_switch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage_switch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwords_mismatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwords_mismatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mespeak-mbrola\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    216\u001b[0m     phonemizer \u001b[38;5;241m=\u001b[39m BACKENDS[backend](\n\u001b[1;32m    217\u001b[0m         language,\n\u001b[1;32m    218\u001b[0m         logger\u001b[38;5;241m=\u001b[39mlogger)\n",
      "File \u001b[0;32m~/Documents/myenv/lib/python3.12/site-packages/phonemizer/backend/espeak/espeak.py:45\u001b[0m, in \u001b[0;36mEspeakBackend.__init__\u001b[0;34m(self, language, punctuation_marks, preserve_punctuation, with_stress, tie, language_switch, words_mismatch, logger)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, language: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     38\u001b[0m              punctuation_marks: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Pattern]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     39\u001b[0m              preserve_punctuation: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m              words_mismatch: WordMismatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     44\u001b[0m              logger: Optional[Logger] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpunctuation_marks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpunctuation_marks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreserve_punctuation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_punctuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_espeak\u001b[38;5;241m.\u001b[39mset_voice(language)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_stress \u001b[38;5;241m=\u001b[39m with_stress\n",
      "File \u001b[0;32m~/Documents/myenv/lib/python3.12/site-packages/phonemizer/backend/espeak/base.py:39\u001b[0m, in \u001b[0;36mBaseEspeakBackend.__init__\u001b[0;34m(self, language, punctuation_marks, preserve_punctuation, logger)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, language: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     36\u001b[0m              punctuation_marks: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Pattern]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m              preserve_punctuation: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     38\u001b[0m              logger: Optional[Logger] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpunctuation_marks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpunctuation_marks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreserve_punctuation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_punctuation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_espeak \u001b[38;5;241m=\u001b[39m EspeakWrapper()\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloaded \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_espeak\u001b[38;5;241m.\u001b[39mlibrary_path)\n",
      "File \u001b[0;32m~/Documents/myenv/lib/python3.12/site-packages/phonemizer/backend/base.py:76\u001b[0m, in \u001b[0;36mBaseBackend.__init__\u001b[0;34m(self, language, punctuation_marks, preserve_punctuation, logger)\u001b[0m\n\u001b[1;32m     73\u001b[0m     logger \u001b[38;5;241m=\u001b[39m get_logger()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# ensure the backend is installed on the system\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not installed on your system\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname()))\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger \u001b[38;5;241m=\u001b[39m logger\n",
      "File \u001b[0;32m~/Documents/myenv/lib/python3.12/site-packages/phonemizer/backend/espeak/base.py:87\u001b[0m, in \u001b[0;36mBaseEspeakBackend.is_available\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_available\u001b[39m(\u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m         \u001b[43mEspeakWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/myenv/lib/python3.12/site-packages/phonemizer/backend/espeak/wrapper.py:60\u001b[0m, in \u001b[0;36mEspeakWrapper.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_voice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# load the espeak API\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_espeak \u001b[38;5;241m=\u001b[39m \u001b[43mEspeakAPI\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# lazy loading of attributes only required for the synthetize method\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libc_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/myenv/lib/python3.12/site-packages/phonemizer/backend/espeak/api.py:86\u001b[0m, in \u001b[0;36mEspeakAPI.__init__\u001b[0;34m(self, library)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_library \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mcdll\u001b[38;5;241m.\u001b[39mLoadLibrary(\u001b[38;5;28mstr\u001b[39m(espeak_copy))\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_library\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mespeak_Initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0x02\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to initialize espeak shared library\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = preprocess_dataset(root_dir, target_sr=sample_rate, target_length=audio_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_all_memory():\n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()  # Clear the cache\n",
    "        torch.cuda.synchronize()  # Ensure all CUDA operations are finished\n",
    "        print(\"Cleared GPU memory cache.\")\n",
    "\n",
    "    # Delete all tensors\n",
    "    gc.collect()  # Run garbage collection to free up Python memory\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
    "                del obj\n",
    "        except Exception as e:\n",
    "            pass  # Skip over objects that can't be deleted\n",
    "\n",
    "    # Re-run garbage collection and clear the cache again\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Final GPU cleanup complete.\")\n",
    "\n",
    "# Call the function to clear all memory\n",
    "clear_all_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_generator = pretrain_single_generator(\n",
    "        num_epochs=20,\n",
    "        z_dim=z_dim,\n",
    "        lr_gen=lr_gen,\n",
    "        lr_disc=lr_disc,\n",
    "        batch_size=batch_size,\n",
    "        seed=42,\n",
    "        audio_length=audio_length,\n",
    "        output_dir='waveform_pre',\n",
    "        train_dataset=train_dataset,\n",
    "        checkpoint_path=\"gan_single_check.pth\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan_with_pretrained_generators(\n",
    "        pretrained_generator,\n",
    "        num_epochs=num_epochs,\n",
    "        z_dim=z_dim,\n",
    "        lr_gen=lr_gen,\n",
    "        lr_disc=lr_disc,\n",
    "        batch_size=batch_size,\n",
    "        train_dataset=train_dataset,\n",
    "        num_generators=num_generators,\n",
    "        seed=42,\n",
    "        audio_length=audio_length,\n",
    "        output_dir=output_dir,\n",
    "        checkpoint_dir='my_checkpoints',\n",
    "        resume=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
