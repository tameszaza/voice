{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements\n",
    "\n",
    "```markdown\n",
    "# Import necessary libraries and modules for the project\n",
    "#\n",
    "# This section imports various Python libraries and modules required for:\n",
    "# - File and directory operations (os)\n",
    "# - Random number generation (random, numpy)\n",
    "# - Mathematical operations (math, numpy)\n",
    "# - PyTorch for deep learning (torch and its submodules)\n",
    "# - Audio processing (soundfile, librosa)\n",
    "# - File globbing (glob)\n",
    "# - Text-to-phoneme conversion (phonemizer)\n",
    "# - Data loading and processing (torch.utils.data)\n",
    "# - Neural network normalization (torch.nn.utils)\n",
    "# - Gradient checkpointing (torch.utils.checkpoint)\n",
    "# - Plotting (matplotlib)\n",
    "# - Garbage collection (gc)\n",
    "# - Logging (logging)\n",
    "# - Automatic mixed precision training (torch.amp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from glob import glob\n",
    "from phonemizer import phonemize\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils import spectral_norm\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import logging\n",
    "from torch.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################\n",
    "# Utility and Setup Functions\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def load_flac(file_path, target_sr=16000):\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
    "    max_val = np.max(np.abs(audio))\n",
    "    if max_val > 0:\n",
    "        audio = audio / max_val  # Normalize to [-1, 1]\n",
    "    return audio\n",
    "\n",
    "def pad_or_trim(audio, target_length=64000):\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:target_length]\n",
    "    return audio\n",
    "\n",
    "def get_transcription(file_path):\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "    base_name = os.path.basename(file_path)\n",
    "    file_id = os.path.splitext(base_name)[0]\n",
    "    transcription_file = None\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith(\".trans.txt\"):\n",
    "            transcription_file = os.path.join(dir_path, file)\n",
    "            break\n",
    "    if not transcription_file:\n",
    "        raise FileNotFoundError(f\"No transcription file found in {dir_path}\")\n",
    "\n",
    "    with open(transcription_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\" \", 1)\n",
    "            if parts[0] == file_id:\n",
    "                transcription = parts[1]\n",
    "                return transcription\n",
    "    raise ValueError(f\"No transcription found for file {file_id}\")\n",
    "\n",
    "def save_waveform_to_audio(waveform, sample_rate, filename):\n",
    "    if isinstance(waveform, torch.Tensor):\n",
    "        waveform = waveform.detach().cpu().numpy()\n",
    "    waveform = np.squeeze(waveform)\n",
    "    max_val = np.max(np.abs(waveform))\n",
    "    if max_val > 0:\n",
    "        waveform = waveform / max_val\n",
    "    sf.write(filename, waveform, sample_rate)\n",
    "\n",
    "def generate_noise(batch_size, z_dim, device):\n",
    "    return torch.randn(batch_size, z_dim, device=device)\n",
    "\n",
    "def orthogonal_loss(feature1, feature2):\n",
    "    inner_product = torch.sum(feature1 * feature2, dim=1)\n",
    "    norm1 = torch.norm(feature1, dim=1)\n",
    "    norm2 = torch.norm(feature2, dim=1)\n",
    "    cosine_similarity = inner_product / (norm1 * norm2 + 1e-8)\n",
    "    return torch.mean(cosine_similarity**2)  # Minimize similarity to enforce orthogonality\n",
    "\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, conditions, device):\n",
    "    batch_size = real_samples.size(0)\n",
    "    epsilon = torch.rand(batch_size, 1, 1, device=device)\n",
    "    epsilon = epsilon.expand_as(real_samples)\n",
    "    interpolates = (epsilon * real_samples + (1 - epsilon) * fake_samples).requires_grad_(True)\n",
    "\n",
    "    real_outputs, fake_outputs = discriminator(interpolates, fake_samples, conditions)\n",
    "    # Assuming just one set of outputs for simplicity:\n",
    "    real_output = real_outputs[0]\n",
    "\n",
    "    grad_outputs = torch.ones_like(real_output, device=device)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=real_output,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=grad_outputs,\n",
    "        create_graph=True,    # Make sure this is True\n",
    "        retain_graph=True      # Consider adding this if needed\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "def visualize_and_save_generated_waveforms(generators, z_dim, features_emb, num_waveforms, device, epoch, sample_rate=16000, output_dir='generated_audio'):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for idx, gen in enumerate(generators):\n",
    "        gen.eval()\n",
    "        with torch.no_grad():\n",
    "            noise = generate_noise(num_waveforms, z_dim, device)\n",
    "            fake_waveforms = gen(features_emb[:num_waveforms], noise).cpu()\n",
    "            num_available_waveforms = fake_waveforms.size(0)\n",
    "            if num_available_waveforms < num_waveforms:\n",
    "                logger.info(f\"Warning: Requested {num_waveforms} waveforms, got {num_available_waveforms}\")\n",
    "\n",
    "            for i in range(num_available_waveforms):\n",
    "                waveform = fake_waveforms[i]\n",
    "                filename = f'epoch{epoch+1}_gen{idx+1}_sample{i+1}.wav'\n",
    "                filepath = os.path.join(output_dir, filename)\n",
    "                save_waveform_to_audio(waveform, sample_rate, filepath)\n",
    "                logger.info(f\"Saved {filepath}\")\n",
    "\n",
    "def pad_tensors_to_match(tensor_list):\n",
    "    # Pad a list of tensors along the last dimension to match their sizes\n",
    "    max_len = max(t.size(-1) for t in tensor_list)\n",
    "    max_channels = max(t.size(1) for t in tensor_list)\n",
    "    padded_tensors = []\n",
    "    for t in tensor_list:\n",
    "        diff_len = max_len - t.size(-1)\n",
    "        diff_channels = max_channels - t.size(1)\n",
    "        if diff_len > 0 or diff_channels > 0:\n",
    "            t = F.pad(t, (0, diff_len, 0, diff_channels))\n",
    "        padded_tensors.append(t)\n",
    "    return torch.stack(padded_tensors, dim=0)\n",
    "\n",
    "def clear_all_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        logger.info(\"Cleared GPU memory cache.\")\n",
    "    gc.collect()\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
    "                del obj\n",
    "        except:\n",
    "            pass\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        logger.info(\"Final GPU cleanup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###################################\n",
    "# Custom Dataset\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, target_sr=16000, target_length=64000, feature_length=100):\n",
    "        self.root_dir = root_dir\n",
    "        self.target_sr = target_sr\n",
    "        self.target_length = target_length\n",
    "        self.feature_length = feature_length\n",
    "        self.flac_files = glob(os.path.join(root_dir, '**', '*.flac'), recursive=True)\n",
    "        logger.info(f\"Found {len(self.flac_files)} .flac files in {root_dir}.\")\n",
    "        if len(self.flac_files) == 0:\n",
    "            raise ValueError(\"No .flac files found.\")\n",
    "        \n",
    "        # Build a global phoneme-to-id dictionary by scanning all files\n",
    "        all_phonemes = set()\n",
    "        for f in self.flac_files:\n",
    "            try:\n",
    "                transcription = get_transcription(f)\n",
    "                phonemes = phonemize(transcription, backend=\"espeak\", language=\"en-us\")\n",
    "                all_phonemes.update(list(phonemes))\n",
    "            except Exception as e:\n",
    "                logger.info(f\"Error processing file {f}: {e}\")\n",
    "        \n",
    "        self.phoneme_to_id = {p: i for i, p in enumerate(sorted(all_phonemes))}\n",
    "        logger.info(f\"Phoneme vocabulary size: {len(self.phoneme_to_id)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.flac_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.flac_files[idx]\n",
    "        try:\n",
    "            audio = load_flac(file, self.target_sr)\n",
    "            audio = pad_or_trim(audio, self.target_length)\n",
    "            transcription = get_transcription(file)\n",
    "            if not transcription or not transcription.strip():\n",
    "                raise ValueError(f\"Empty or invalid transcription for file: {file}\")\n",
    "\n",
    "            phonemes = phonemize(transcription, backend=\"espeak\", language=\"en-us\")\n",
    "            phonetic_features = [self.phoneme_to_id[p] for p in phonemes]\n",
    "\n",
    "            # Convert to tensor and pad/truncate\n",
    "            phonetic_features = torch.tensor(phonetic_features, dtype=torch.long)\n",
    "            if len(phonetic_features) < self.feature_length:\n",
    "                phonetic_features = F.pad(phonetic_features, (0, self.feature_length - len(phonetic_features)))\n",
    "            else:\n",
    "                phonetic_features = phonetic_features[:self.feature_length]\n",
    "\n",
    "            audio = torch.tensor(audio, dtype=torch.float32).unsqueeze(0)  # [B,1,T]\n",
    "            # features is [feature_length], long tensor of phoneme IDs\n",
    "            return audio, phonetic_features\n",
    "        except Exception as e:\n",
    "            # If any error occurs, return a dummy sample or raise\n",
    "            logger.info(f\"Error reading {file}: {e}\")\n",
    "            # Return dummy data (should rarely happen)\n",
    "            dummy_audio = torch.zeros((1, self.target_length), dtype=torch.float32)\n",
    "            dummy_features = torch.zeros(self.feature_length, dtype=torch.long)\n",
    "            return dummy_audio, dummy_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################\n",
    "# Models\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, upsample_factor):\n",
    "        super(UpsampleNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.upsample_factor = upsample_factor  # Add this line\n",
    "\n",
    "        layer = nn.ConvTranspose1d(input_size, output_size, upsample_factor * 2,\n",
    "                                   upsample_factor, padding=upsample_factor // 2)\n",
    "        nn.init.orthogonal_(layer.weight)\n",
    "        self.layer = spectral_norm(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.layer(inputs)\n",
    "        outputs = outputs[:, :, : inputs.size(-1) * self.upsample_factor]\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class GBlock(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, z_channels, upsample_factor):\n",
    "        super(GBlock, self).__init__()\n",
    "        self.condition_norm1 = nn.GroupNorm(32, in_channels)\n",
    "        self.first_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            UpsampleNet(in_channels, in_channels, upsample_factor),\n",
    "            nn.Conv1d(in_channels, hidden_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.condition_norm2 = nn.GroupNorm(32, hidden_channels)\n",
    "        self.second_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, dilation=1, padding=1)\n",
    "        )\n",
    "        self.residual1 = nn.Sequential(\n",
    "            UpsampleNet(in_channels, in_channels, upsample_factor),\n",
    "            nn.Conv1d(in_channels, hidden_channels, kernel_size=1)\n",
    "        )\n",
    "        self.condition_norm3 = nn.GroupNorm(32, hidden_channels)\n",
    "        self.third_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        )\n",
    "        self.condition_norm4 = nn.GroupNorm(32, hidden_channels)\n",
    "        self.fourth_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, condition, z):\n",
    "        def run_forward(inputs):\n",
    "            outputs = self.condition_norm1(inputs)\n",
    "            outputs = self.first_stack(outputs)\n",
    "            outputs = self.condition_norm2(outputs)\n",
    "            outputs = self.second_stack(outputs)\n",
    "            residual_outputs = self.residual1(inputs) + outputs\n",
    "            outputs = self.condition_norm3(residual_outputs)\n",
    "            outputs = self.third_stack(outputs)\n",
    "            outputs = self.condition_norm4(outputs)\n",
    "            outputs = self.fourth_stack(outputs)\n",
    "            outputs = outputs + residual_outputs\n",
    "            return outputs\n",
    "        outputs = checkpoint.checkpoint(run_forward, condition)\n",
    "        return outputs\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=64, z_channels=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        nn.init.normal_(self.embedding.weight, 0.0, 0.1)\n",
    "\n",
    "        self.preprocess = nn.Conv1d(embedding_dim, 768, kernel_size=3, padding=1)\n",
    "        self.gblocks = nn.ModuleList([\n",
    "            GBlock(768, 768, z_channels, 5),\n",
    "            GBlock(768, 768, z_channels, 4),\n",
    "            GBlock(768, 384, z_channels, 4),\n",
    "            GBlock(384, 384, z_channels, 4),\n",
    "            GBlock(384, 192, z_channels, 2),\n",
    "        ])\n",
    "        self.postprocess = nn.Sequential(\n",
    "            nn.Conv1d(192, 1, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, conditions_emb, z):\n",
    "        # conditions_emb: [B, embedding_dim, T]\n",
    "        outputs = self.preprocess(conditions_emb)\n",
    "        for layer in self.gblocks:\n",
    "            outputs = layer(outputs, z)\n",
    "        outputs = self.postprocess(outputs)\n",
    "        return outputs\n",
    "\n",
    "class DBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample_factor):\n",
    "        super(DBlock, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.AvgPool1d(downsample_factor, stride=downsample_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        )\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1),\n",
    "            nn.AvgPool1d(downsample_factor, stride=downsample_factor)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.layers(inputs) + self.residual(inputs)\n",
    "        return outputs\n",
    "\n",
    "class CondDBlock(nn.Module):\n",
    "    def __init__(self, in_channels, lc_channels, upsample_factor):\n",
    "        super(CondDBlock, self).__init__()\n",
    "        self.lc_conv1d = nn.Conv1d(lc_channels, in_channels, kernel_size=1)\n",
    "        self.start = nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.end = nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.residual = nn.Conv1d(in_channels, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, inputs, conditions):\n",
    "        conditions = self.lc_conv1d(conditions)\n",
    "        outputs = self.start(inputs) + conditions\n",
    "        outputs = self.end(outputs)\n",
    "        residual_outputs = self.residual(inputs)\n",
    "        return outputs + residual_outputs\n",
    "\n",
    "class ConditionalDBlocks(nn.Module):\n",
    "    def __init__(self, in_channels, lc_channels, factors=(2,2,2), out_channels=(128,256)):\n",
    "        super(ConditionalDBlocks, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.lc_channels = lc_channels\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(DBlock(in_channels, 64, 1))\n",
    "        in_channels = 64\n",
    "        for i, channel in enumerate(out_channels):\n",
    "            self.layers.append(DBlock(in_channels, channel, factors[i]))\n",
    "            in_channels = channel\n",
    "        self.cond_layer = CondDBlock(in_channels, lc_channels, factors[-1])\n",
    "        self.adjust_channels = nn.Conv1d(in_channels, 512, kernel_size=1)\n",
    "        self.post_process = nn.ModuleList([DBlock(512, 512, 1), DBlock(512, 512, 1)])\n",
    "\n",
    "    def forward(self, inputs, conditions):\n",
    "        batch_size = inputs.size(0)\n",
    "        outputs = inputs.view(batch_size, self.in_channels, -1)\n",
    "        for layer in self.layers:\n",
    "            outputs = layer(outputs)\n",
    "        # conditions: [B, lc_channels, T'] -> pool to 1 then expand\n",
    "        conditions_pooled = F.adaptive_avg_pool1d(conditions, 1)\n",
    "        conditions_pooled = conditions_pooled.expand(-1, self.lc_channels, outputs.size(-1))\n",
    "        outputs = self.cond_layer(outputs, conditions_pooled)\n",
    "        outputs = self.adjust_channels(outputs)\n",
    "        for layer in self.post_process:\n",
    "            outputs = layer(outputs)\n",
    "        return outputs\n",
    "\n",
    "class UnConditionalDBlocks(nn.Module):\n",
    "    def __init__(self, in_channels, factors=(5, 3), out_channels=(128, 256)):\n",
    "        super(UnConditionalDBlocks, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(DBlock(in_channels, 64, 1))\n",
    "        in_channels = 64\n",
    "        for (i, factor) in enumerate(factors):\n",
    "            self.layers.append(DBlock(in_channels, out_channels[i], factor))\n",
    "            in_channels = out_channels[i]\n",
    "        self.layers.append(DBlock(in_channels, in_channels, 1))\n",
    "        self.layers.append(DBlock(in_channels, in_channels, 1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        outputs = inputs.view(batch_size, self.in_channels, -1)\n",
    "        for layer in self.layers:\n",
    "            outputs = layer(outputs)\n",
    "        return outputs\n",
    "\n",
    "class Multiple_Random_Window_Discriminators(nn.Module):\n",
    "    def __init__(self, lc_channels, window_size=(2,4,8,16,30), upsample_factor=120):\n",
    "        super(Multiple_Random_Window_Discriminators, self).__init__()\n",
    "        self.lc_channels = lc_channels\n",
    "        self.window_size = window_size\n",
    "        self.upsample_factor = upsample_factor\n",
    "\n",
    "        self.udiscriminators = nn.ModuleList([\n",
    "            UnConditionalDBlocks(in_channels=1, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=2, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=4, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=8, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=15, factors=(2, 2), out_channels=(128, 256)),\n",
    "        ])\n",
    "\n",
    "        self.discriminators = nn.ModuleList([\n",
    "            ConditionalDBlocks(in_channels=1, lc_channels=lc_channels,\n",
    "                               factors=(5, 3, 2, 2, 2), out_channels=(128, 128, 256, 256)),\n",
    "            ConditionalDBlocks(in_channels=2, lc_channels=lc_channels,\n",
    "                               factors=(5, 3, 2, 2), out_channels=(128, 256, 256)),\n",
    "            ConditionalDBlocks(in_channels=4, lc_channels=lc_channels,\n",
    "                               factors=(5, 3, 2), out_channels=(128, 256)),\n",
    "            ConditionalDBlocks(in_channels=8, lc_channels=lc_channels,\n",
    "                               factors=(5, 3), out_channels=(256,)),\n",
    "            ConditionalDBlocks(in_channels=15, lc_channels=lc_channels,\n",
    "                               factors=(2, 2, 2), out_channels=(128, 256)),\n",
    "        ])\n",
    "\n",
    "    def forward(self, real_samples, fake_samples, conditions):\n",
    "        real_outputs, fake_outputs = [], []\n",
    "        # Unconditional\n",
    "        for (size, layer) in zip(self.window_size, self.udiscriminators):\n",
    "            size = size * self.upsample_factor\n",
    "            index = np.random.randint(0, real_samples.size(-1) - size + 1)\n",
    "            real_slice = real_samples[:, :, index: index + size]\n",
    "            fake_slice = fake_samples[:, :, index: index + size]\n",
    "            real_output = layer(real_slice)\n",
    "            fake_output = layer(fake_slice)\n",
    "            real_outputs.append(real_output)\n",
    "            fake_outputs.append(fake_output)\n",
    "\n",
    "        # Conditional\n",
    "        for (size, layer) in zip(self.window_size, self.discriminators):\n",
    "            lc_index = np.random.randint(0, conditions.size(-1) - size + 1)\n",
    "            sample_index = lc_index * self.upsample_factor\n",
    "            real_x = real_samples[:, :, sample_index: (lc_index + size)*self.upsample_factor]\n",
    "            fake_x = fake_samples[:, :, sample_index: (lc_index + size)*self.upsample_factor]\n",
    "            lc = conditions[:, :, lc_index: lc_index + size]\n",
    "            real_output = layer(real_x, lc)\n",
    "            fake_output = layer(fake_x, lc)\n",
    "            real_outputs.append(real_output)\n",
    "            fake_outputs.append(fake_output)\n",
    "\n",
    "        return real_outputs, fake_outputs\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, audio_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.audio_length = audio_length\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###################################\n",
    "# Training Functions\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pretrain_single_generator(\n",
    "    num_epochs, z_dim, lr_gen, lr_disc, batch_size, seed,\n",
    "    audio_length, output_dir, train_dataset, checkpoint_path=\"checkpoint.pth\",\n",
    "    resume=False, vocab_size=100, embedding_dim=64\n",
    "):\n",
    "    set_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    phoneme_embedding = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "    nn.init.normal_(phoneme_embedding.weight, 0.0, 0.1)\n",
    "\n",
    "    generator = Generator(vocab_size=vocab_size, embedding_dim=embedding_dim, z_channels=z_dim).to(device)\n",
    "    discriminator = Multiple_Random_Window_Discriminators(lc_channels=embedding_dim).to(device)\n",
    "\n",
    "    optimizer_gen = optim.Adam(generator.parameters(), lr=lr_gen, betas=(0.5, 0.9))\n",
    "    optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "    scaler_gen = GradScaler()\n",
    "    scaler_disc = GradScaler()\n",
    "\n",
    "    start_epoch = 0\n",
    "    if resume and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        optimizer_gen.load_state_dict(checkpoint['optimizer_gen_state_dict'])\n",
    "        optimizer_disc.load_state_dict(checkpoint['optimizer_disc_state_dict'])\n",
    "        scaler_gen.load_state_dict(checkpoint['scaler_gen'])\n",
    "        scaler_disc.load_state_dict(checkpoint['scaler_disc'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        logger.info(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        for batch_idx, (real_audio, features_ids) in enumerate(train_loader):\n",
    "            real_audio = real_audio.to(device)  # [B,1,T]\n",
    "            features_ids = features_ids.to(device) # [B, feature_length]\n",
    "            # Embed features\n",
    "            features_emb = phoneme_embedding(features_ids) # [B, feature_length, emb_dim]\n",
    "            features_emb = features_emb.transpose(1, 2)     # [B, emb_dim, feature_length]\n",
    "\n",
    "            # Train Discriminator\n",
    "            for _ in range(5):  # Critic steps\n",
    "                optimizer_disc.zero_grad()\n",
    "                noise = generate_noise(real_audio.size(0), z_dim, device)\n",
    "\n",
    "                with autocast(device_type='cuda'):\n",
    "                    fake_audio = generator(features_emb, noise).detach()\n",
    "                    real_outputs, fake_outputs = discriminator(real_audio, fake_audio, features_emb)\n",
    "                    \n",
    "                    # Compute WGAN loss for discriminator\n",
    "                    loss_disc = sum(torch.mean(f) - torch.mean(r) for r, f in zip(real_outputs, fake_outputs))\n",
    "                    \n",
    "                    # Gradient Penalty\n",
    "                    gradient_penalty = compute_gradient_penalty(discriminator, real_audio, fake_audio, features_emb, device)\n",
    "                    loss_disc += 10 * gradient_penalty\n",
    "\n",
    "                # Backward and step\n",
    "                try:\n",
    "                    scaler_disc.scale(loss_disc).backward(retain_graph=True)\n",
    "                except RuntimeError as e:\n",
    "                    logger.info(\"Backward failed:\", e)\n",
    "                    exit(1)\n",
    "\n",
    "                scaler_disc.step(optimizer_disc)\n",
    "                scaler_disc.update()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_gen.zero_grad()\n",
    "            # In Generator Training\n",
    "            noise = generate_noise(real_audio.size(0), z_dim, device)\n",
    "            with autocast(device_type='cuda'):\n",
    "                fake_audio = generator(features_emb, noise)  # Do not detach here\n",
    "                fake_outputs = discriminator(fake_audio, fake_audio, features_emb)[1]\n",
    "                fake_outputs = pad_tensors_to_match(fake_outputs)\n",
    "                loss_gen = -torch.mean(fake_outputs)\n",
    "\n",
    "            # Backward and step\n",
    "            scaler_gen.scale(loss_gen).backward(retain_graph=True)  # Ensure no re-use of graph\n",
    "            scaler_gen.step(optimizer_gen)\n",
    "            scaler_gen.update()\n",
    "\n",
    "        logger.info(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {loss_disc.item():.4f}, Loss G: {loss_gen.item():.4f}\")\n",
    "\n",
    "        # Save samples\n",
    "        visualize_and_save_generated_waveforms([generator], z_dim, features_emb, num_waveforms=5, device=device, epoch=epoch, sample_rate=16000, output_dir=output_dir)\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch+1,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_gen_state_dict': optimizer_gen.state_dict(),\n",
    "            'optimizer_disc_state_dict': optimizer_disc.state_dict(),\n",
    "            'scaler_gen': scaler_gen.state_dict(),\n",
    "            'scaler_disc': scaler_disc.state_dict(),\n",
    "            'z_dim': z_dim,\n",
    "            'lr_gen': lr_gen,\n",
    "            'lr_disc': lr_disc,\n",
    "            'batch_size': batch_size,\n",
    "            'seed': seed,\n",
    "            'audio_length': audio_length,\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        logger.info(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "        torch.save(generator.state_dict(), os.path.join(output_dir, f\"pretrained_generator_epoch{epoch+1}.pth\"))\n",
    "\n",
    "    logger.info(\"Pretraining complete.\")\n",
    "    return generator, phoneme_embedding\n",
    "\n",
    "def train_gan_with_pretrained_generators(\n",
    "    pretrained_generator, phoneme_embedding,\n",
    "    num_epochs, z_dim, lr_gen, lr_disc, batch_size, train_dataset,\n",
    "    num_generators, seed, audio_length, output_dir, checkpoint_path=\"multi_gan_checkpoint.pth\",\n",
    "    resume=False\n",
    "):\n",
    "    set_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize multiple generators with pretrained weights\n",
    "    generators = []\n",
    "    for _ in range(num_generators):\n",
    "        gen = Generator(vocab_size=phoneme_embedding.num_embeddings, embedding_dim=phoneme_embedding.embedding_dim, z_channels=z_dim).to(device)\n",
    "        gen.load_state_dict(pretrained_generator.state_dict())\n",
    "        generators.append(gen)\n",
    "\n",
    "    discriminator = Multiple_Random_Window_Discriminators(lc_channels=phoneme_embedding.embedding_dim).to(device)\n",
    "    encoder = Encoder(audio_length).to(device)\n",
    "\n",
    "    optimizer_gens = [optim.Adam(gen.parameters(), lr=lr_gen, betas=(0.5, 0.9)) for gen in generators]\n",
    "    optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "    optimizer_encoder = optim.Adam(encoder.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "    scaler_gens = [GradScaler() for _ in range(num_generators)]\n",
    "    scaler_disc = GradScaler()\n",
    "    scaler_encoder = GradScaler()\n",
    "\n",
    "    start_epoch = 0\n",
    "    if resume and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        for idx, gen in enumerate(generators):\n",
    "            gen.load_state_dict(checkpoint['generator_state_dicts'][idx])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "        for idx, opt_gen in enumerate(optimizer_gens):\n",
    "            opt_gen.load_state_dict(checkpoint['optimizer_gen_state_dicts'][idx])\n",
    "        optimizer_disc.load_state_dict(checkpoint['optimizer_disc_state_dict'])\n",
    "        optimizer_encoder.load_state_dict(checkpoint['optimizer_encoder_state_dict'])\n",
    "        logger.info(f\"Resumed training from epoch {start_epoch}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    lambda_gp = 10\n",
    "    lambda_ortho = 0.1\n",
    "    num_critic = 5\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        for batch_idx, (real_audio, features_ids) in enumerate(train_loader):\n",
    "            real_audio = real_audio.to(device)\n",
    "            features_ids = features_ids.to(device)\n",
    "            features_emb = phoneme_embedding(features_ids).transpose(1,2) # [B, emb_dim, length]\n",
    "\n",
    "            # Train Discriminator\n",
    "            for _ in range(num_critic):\n",
    "                optimizer_disc.zero_grad()\n",
    "                noises = [generate_noise(real_audio.size(0), z_dim, device) for _ in range(num_generators)]\n",
    "\n",
    "                with autocast(device_type='cuda'):\n",
    "                    fakes = [gen(features_emb, noises[i]).detach() for i, gen in enumerate(generators)]\n",
    "                    # We must stack fakes in a way to pass them to the discriminator.\n",
    "                    # Discriminator expects two sets: real and fake.\n",
    "                    # It processes each window independently. Just call discriminator on real_audio and \"first\" fake:\n",
    "                    real_outputs, fake_outputs = discriminator(real_audio, fakes[0], features_emb)\n",
    "                    # If we intended multiple sets, we could combine them, but code is from original snippet.\n",
    "                    # We'll just consider the single fakes[0] for gradient penalty and WGAN loss.\n",
    "                    loss_disc = sum(torch.mean(fake) - torch.mean(real) for real, fake in zip(real_outputs, fake_outputs))\n",
    "                    gradient_penalty = compute_gradient_penalty(discriminator, real_audio, fakes[0], features_emb, device)\n",
    "                    loss_disc += lambda_gp * gradient_penalty\n",
    "\n",
    "                scaler_disc.scale(loss_disc).backward()\n",
    "                scaler_disc.step(optimizer_disc)\n",
    "                scaler_disc.update()\n",
    "\n",
    "            # Train Generators and Encoder\n",
    "            for idx, gen in enumerate(generators):\n",
    "                optimizer_gens[idx].zero_grad()\n",
    "                optimizer_encoder.zero_grad()\n",
    "\n",
    "                noise = generate_noise(real_audio.size(0), z_dim, device)\n",
    "                with autocast(device_type='cuda'):\n",
    "                    fake = gen(features_emb, noise)\n",
    "                    fake_outputs = discriminator(fake, fake, features_emb)[1]\n",
    "                    fake_outputs = pad_tensors_to_match(fake_outputs)\n",
    "                    loss_gen = -torch.mean(fake_outputs)\n",
    "\n",
    "                    # Orthogonal loss\n",
    "                    gen_feature = encoder(fake)\n",
    "                    ortho_loss_val = 0\n",
    "                    for other_idx, other_gen in enumerate(generators):\n",
    "                        if idx != other_idx:\n",
    "                            other_noise = generate_noise(real_audio.size(0), z_dim, device)\n",
    "                            other_fake = other_gen(features_emb, other_noise)\n",
    "                            other_feature = encoder(other_fake)\n",
    "                            ortho_loss_val += orthogonal_loss(gen_feature, other_feature)\n",
    "                    ortho_loss_val /= (num_generators - 1)\n",
    "                    total_loss_gen = loss_gen + lambda_ortho * ortho_loss_val\n",
    "\n",
    "                scaler_gens[idx].scale(total_loss_gen).backward()\n",
    "                scaler_gens[idx].step(optimizer_gens[idx])\n",
    "                scaler_gens[idx].update()\n",
    "                scaler_encoder.step(optimizer_encoder)\n",
    "                scaler_encoder.update()\n",
    "\n",
    "        logger.info(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {loss_disc.item():.4f}, Loss G: {loss_gen.item():.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'generator_state_dicts': [gen.state_dict() for gen in generators],\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'encoder_state_dict': encoder.state_dict(),\n",
    "            'optimizer_gen_state_dicts': [opt.state_dict() for opt in optimizer_gens],\n",
    "            'optimizer_disc_state_dict': optimizer_disc.state_dict(),\n",
    "            'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        logger.info(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    logger.info(\"Training complete.\")\n",
    "    return generators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################\n",
    "# Main Execution\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 21:27:26,658 - INFO - Logging system initialized\n",
      "2024-12-12 21:27:26,825 - INFO - Found 148688 .flac files in ./data.\n",
      "2024-12-12 21:27:26,858 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:26,969 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:26,991 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,018 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,066 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,090 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,133 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,155 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,179 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,202 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,227 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,249 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,272 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,296 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,317 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,340 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,364 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,437 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,481 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,505 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,551 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,669 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,694 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,742 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,812 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,835 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,882 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,904 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,927 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:27,970 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,038 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,062 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,212 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,260 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,283 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,327 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,350 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,372 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,416 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,440 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,489 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,512 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,556 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,579 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,624 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,648 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,693 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,716 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,759 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,782 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,806 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,853 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,901 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:28,973 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,024 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,045 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,089 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,136 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,185 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,209 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,232 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,255 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,278 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,302 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,348 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,371 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,421 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,443 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,467 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,489 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,513 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,536 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,562 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,584 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,606 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,626 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,650 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,676 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,721 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,770 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,796 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,821 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,868 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,893 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,915 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,941 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:29,965 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:30,041 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:30,088 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:30,113 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:30,136 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:30,161 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:30,238 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:30,285 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:30,307 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:30,357 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:30,429 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:30,545 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:30,592 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:30,618 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:30,643 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:30,712 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:30,761 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:30,784 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:30,808 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:30,903 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:30,951 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,026 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,050 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,073 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,098 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,123 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,168 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,214 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,269 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,319 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,343 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,368 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,392 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,416 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,440 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,465 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,488 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,509 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,605 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,653 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,679 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,704 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,730 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,827 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,851 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,898 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,921 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,942 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,965 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:31,991 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:32,014 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:32,036 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:32,060 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:32,084 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:32,132 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:32,181 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:32,204 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:32,229 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:32,254 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:32,347 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:32,463 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:32,490 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:32,516 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:32,540 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:32,605 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:32,629 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:32,675 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:32,700 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:32,746 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n",
      "2024-12-12 21:27:32,771 - WARNING - words count mismatch on 100.0% of the lines (1/1)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    filename=\"training_500.log\",  # Specify a file to write logs to\n",
    "    filemode=\"a\"  # Append mode\n",
    ")\n",
    "\n",
    "# Create a console handler to also logger.info logs to the console\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the console handler to the root logger\n",
    "logging.getLogger('').addHandler(console_handler)\n",
    "\n",
    "# Now you can use logging throughout your code\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Example usage:\n",
    "logger.info(\"Logging system initialized\")\n",
    "\n",
    "set_seed(42)\n",
    "audio_length = 64000\n",
    "z_dim = 128\n",
    "lr_gen = 0.0002\n",
    "lr_disc = 0.0002\n",
    "batch_size = 8\n",
    "num_epochs = 50\n",
    "root_dir = \"./data\"\n",
    "sample_rate = 16000\n",
    "num_generators = 5\n",
    "output_dir = 'generated_audio'\n",
    "\n",
    "# Create dataset (no longer loading all into RAM)\n",
    "train_dataset = MyAudioDataset(root_dir, target_sr=sample_rate, target_length=audio_length)\n",
    "vocab_size = len(train_dataset.phoneme_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    clear_all_memory()\n",
    "    pretrained_generator, phoneme_embedding = pretrain_single_generator(\n",
    "        num_epochs=20,\n",
    "        z_dim=z_dim,\n",
    "        lr_gen=lr_gen,\n",
    "        lr_disc=lr_disc,\n",
    "        batch_size=batch_size,\n",
    "        seed=42,\n",
    "        audio_length=audio_length,\n",
    "        output_dir='output/waveform_pre_linguistics_o1',\n",
    "        train_dataset=train_dataset,\n",
    "        checkpoint_path=\"output/gan_single_check.pth\",\n",
    "        embedding_dim=64,\n",
    "        vocab_size=vocab_size\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    train_gan_with_pretrained_generators(\n",
    "        pretrained_generator, phoneme_embedding,\n",
    "        num_epochs=num_epochs,\n",
    "        z_dim=z_dim,\n",
    "        lr_gen=lr_gen,\n",
    "        lr_disc=lr_disc,\n",
    "        batch_size=batch_size,\n",
    "        train_dataset=train_dataset,\n",
    "        num_generators=num_generators,\n",
    "        seed=42,\n",
    "        audio_length=audio_length,\n",
    "        output_dir=output_dir,\n",
    "        checkpoint_path='output/my_checkpoints/multi_gan_checkpoint.pth',\n",
    "        resume=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
