{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  orthogonal loss \n",
    "This model work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_flac(file_path, target_sr=16000):\n",
    "    \"\"\"Load a FLAC audio file and resample it to the target sample rate.\"\"\"\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
    "    max_val = np.max(np.abs(audio))\n",
    "    if max_val > 0:\n",
    "        audio = audio / max_val  # Normalize to [-1, 1]\n",
    "    return audio\n",
    "\n",
    "def pad_or_trim(audio, target_length=64000):\n",
    "    \"\"\"Pad or trim audio to the target length.\"\"\"\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:target_length]\n",
    "    return audio\n",
    "\n",
    "def preprocess_dataset(root_dir, target_sr=16000, target_length=64000):\n",
    "    flac_files = glob(os.path.join(root_dir, '**', '*.flac'), recursive=True)\n",
    "    print(f\"Found {len(flac_files)} .flac files in {root_dir}.\")\n",
    "    if len(flac_files) == 0:\n",
    "        print(\"No .flac files found. Please check the root_dir path.\")\n",
    "    dataset = []\n",
    "    fragmented_count = 0  # Count of fragmented audio files\n",
    "    sink = []  # Buffer to hold audio fragments less than target_length\n",
    "\n",
    "    for file in flac_files:\n",
    "        try:\n",
    "            audio = load_flac(file, target_sr)\n",
    "            audio_length = len(audio)\n",
    "            if audio_length == target_length:\n",
    "                # Audio is exactly target_length, add directly to dataset\n",
    "                dataset.append(audio)\n",
    "            elif audio_length < target_length:\n",
    "                # Audio is shorter than target_length, add to sink\n",
    "                fragmented_count += 1\n",
    "                sink.append(audio)\n",
    "            else:\n",
    "                # Audio is longer than target_length\n",
    "                # Split audio into chunks of target_length\n",
    "                num_full_chunks = audio_length // target_length\n",
    "                for i in range(num_full_chunks):\n",
    "                    chunk = audio[i * target_length : (i + 1) * target_length]\n",
    "                    dataset.append(chunk)\n",
    "                # Add any remaining part to the sink\n",
    "                remainder = audio[num_full_chunks * target_length :]\n",
    "                if len(remainder) > 0:\n",
    "                    sink.append(remainder)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    # Now, process the sink to create additional samples\n",
    "    print(f\"Number of fragmented audio fragments in sink: {len(sink)}\")\n",
    "    # Keep track of how many additional samples we create\n",
    "    num_additional_samples = 0\n",
    "\n",
    "    # Combine fragments in the sink to form new audio samples of target_length\n",
    "    current_audio = np.array([], dtype=np.float32)\n",
    "    for fragment in sink:\n",
    "        current_audio = np.concatenate((current_audio, fragment))\n",
    "        while len(current_audio) >= target_length:\n",
    "            # Extract a chunk of target_length\n",
    "            chunk = current_audio[:target_length]\n",
    "            dataset.append(chunk)\n",
    "            num_additional_samples += 1\n",
    "            # Remove the chunk from current_audio\n",
    "            current_audio = current_audio[target_length:]\n",
    "\n",
    "    # Optionally handle the last remaining fragment\n",
    "    # Decide whether to discard or pad the last fragment\n",
    "    if len(current_audio) > 0:\n",
    "        # You can choose to discard it or pad it to target_length\n",
    "        # Here we pad it and add it to the dataset\n",
    "        padded_audio = pad_or_trim(current_audio, target_length)\n",
    "        dataset.append(padded_audio)\n",
    "        num_additional_samples += 1\n",
    "\n",
    "    print(f\"Number of additional samples created from sink fragments: {num_additional_samples}\")\n",
    "    print(f\"Total number of audio samples in dataset: {len(dataset)}\")\n",
    "\n",
    "    dataset = torch.tensor(dataset, dtype=torch.float32)\n",
    "    return TensorDataset(dataset)\n",
    "\n",
    "def save_waveform_to_audio(waveform, sample_rate, filename):\n",
    "    \"\"\"\n",
    "    Save a waveform to an audio file.\n",
    "    \"\"\"\n",
    "    if isinstance(waveform, torch.Tensor):\n",
    "        waveform = waveform.detach().cpu().numpy()\n",
    "    \n",
    "    waveform = np.squeeze(waveform)\n",
    "    max_val = np.max(np.abs(waveform))\n",
    "    if max_val > 0:\n",
    "        waveform = waveform / max_val\n",
    "    \n",
    "    sf.write(filename, waveform, sample_rate)\n",
    "\n",
    "def verify_waveform_to_audio(root_dir, sample_rate=16000, target_length=64000, output_dir=\"verified_audio\"):\n",
    "    \"\"\"\n",
    "    Verify waveform-to-audio conversion using preprocessed dataset.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    dataset = preprocess_dataset(root_dir, sample_rate, target_length)\n",
    "    \n",
    "    num_samples_to_verify = min(5, len(dataset))\n",
    "    \n",
    "    for idx in range(num_samples_to_verify):\n",
    "        waveform = dataset.tensors[0][idx]\n",
    "        \n",
    "        filename = os.path.join(output_dir, f\"example_waveform_{idx+1}.wav\")\n",
    "        save_waveform_to_audio(waveform, sample_rate, filename)\n",
    "        print(f\"Waveform saved to {filename}\")\n",
    "        \n",
    "        # Plot the waveform\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(waveform.numpy())\n",
    "        plt.title(f\"Waveform {idx+1}\")\n",
    "        plt.xlabel(\"Sample Index\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_length = 64000  # Set this to your actual audio waveform length\n",
    "root_dir = \"./data\"  # Replace with your actual data directory\n",
    "# verify_waveform_to_audio(root_dir, sample_rate=16000, target_length=64000, output_dir=\"verified_audio\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2703 .flac files in ./data.\n",
      "Number of fragmented audio fragments in sink: 2699\n",
      "Number of additional samples created from sink fragments: 1422\n",
      "Total number of audio samples in dataset: 4850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12479/91943476.py:89: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  dataset = torch.tensor(dataset, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = preprocess_dataset(root_dir=\"./data\", target_sr=16000, target_length=audio_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.benchmark = False  # Turn off the cuDNN auto-tuner to avoid nondeterministic behavior\n",
    "\n",
    "\n",
    "# img_size = 28\n",
    "# img_channels = 1\n",
    "\n",
    "def get_dim_for_each_layer(z_dim, total_l, l, output_dim):\n",
    "    \"\"\"\n",
    "    Calculate the dimension for the l-th layer in the generator.\n",
    "\n",
    "    Parameters:\n",
    "    - z_dim: int, the input dimension (e.g., latent vector size).\n",
    "    - total_l: int, the total number of layers in the generator.\n",
    "    - l: int, the current layer index (1-indexed).\n",
    "    - output_dim: int, the final output dimension (e.g., audio length).\n",
    "\n",
    "    Returns:\n",
    "    - int: the calculated dimension for the l-th layer.\n",
    "    \"\"\"\n",
    "    if l < 1 or l > total_l:\n",
    "        raise ValueError(\"Layer index 'l' must be in the range [1, total_l].\")\n",
    "    if l == total_l:\n",
    "        return output_dim\n",
    "    if l == 1:\n",
    "        return z_dim\n",
    "    # Calculate the dimension change per layer\n",
    "    step = (output_dim - z_dim) / (total_l - 1)\n",
    "    \n",
    "    # Compute the dimension for the l-th layer\n",
    "    dim = z_dim + (l - 1) * step\n",
    "    return math.ceil(dim)  # Use math.ceil to round up to an integer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, audio_length):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.initial_length = audio_length // 256  # 64000 / 256 = 250\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(z_dim, 512 * self.initial_length),  # Output: (batch_size, 512 * 250)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Unflatten(1, (512, self.initial_length)),  # Shape: (batch_size, 512, 250)\n",
    "            nn.ConvTranspose1d(512, 256, kernel_size=25, stride=4, padding=11, output_padding=1),  # Output length: 1000\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=25, stride=4, padding=11, output_padding=1),  # Output length: 4000\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=25, stride=4, padding=11, output_padding=1),   # Output length: 16000\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose1d(64, 1, kernel_size=25, stride=4, padding=11, output_padding=1),     # Output length: 64000\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, audio_length):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=25, stride=4, padding=11),  # Output: (batch_size, 64, L1)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(64, 128, kernel_size=25, stride=4, padding=11),  # Output: (batch_size, 128, L2)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(128, 256, kernel_size=25, stride=4, padding=11),  # Output: (batch_size, 256, L3)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(256, 512, kernel_size=25, stride=4, padding=11),  # Output: (batch_size, 512, L4)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool1d(1),  # Output: (batch_size, 512, 1)\n",
    "            nn.Flatten(),             # Output: (batch_size, 512)\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Remove the unnecessary unsqueeze\n",
    "        # x = x.unsqueeze(1)  # This line is removed\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, audio_length):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool1d(1),  # Output: (batch_size, 128, 1)\n",
    "            nn.Flatten(),             # Output: (batch_size, 128)\n",
    "            nn.Linear(128, 64)        # Output: (batch_size, 64)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.unsqueeze(1)  # Remove if input x already has channel dimension\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "# z_dim = 100\n",
    "# audio_length = 64000\n",
    "\n",
    "# generator = Generator(z_dim, audio_length)\n",
    "# discriminator = Discriminator(audio_length)\n",
    "\n",
    "# # Generate fake audio\n",
    "# noise = torch.randn(batch_size, z_dim)\n",
    "# fake_audio = generator(noise)\n",
    "\n",
    "# # Pass fake audio through the discriminator\n",
    "# disc_output = discriminator(fake_audio)\n",
    "\n",
    "# print(f\"Fake audio shape: {fake_audio.shape}\")         # Expected: (batch_size, 1, 64000)\n",
    "# print(f\"Discriminator output shape: {disc_output.shape}\")  # Expected: (batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noise(batch_size, z_dim, device):\n",
    "    return torch.randn(batch_size, z_dim).to(device)\n",
    "\n",
    "# Orthogonal loss function\n",
    "def orthogonal_loss(feature1, feature2):\n",
    "    inner_product = torch.sum(feature1 * feature2, dim=1)\n",
    "    norm1 = torch.norm(feature1, dim=1)\n",
    "    norm2 = torch.norm(feature2, dim=1)\n",
    "    cosine_similarity = inner_product / (norm1 * norm2 + 1e-8)\n",
    "    return torch.mean(cosine_similarity**2)  # Minimize the cosine similarity to make vectors orthogonal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(critic, real_samples, fake_samples, device):\n",
    "    batch_size = real_samples.size(0)\n",
    "\n",
    "    # Sample epsilon uniformly in [0,1]\n",
    "    epsilon = torch.rand(batch_size, 1, 1, device=device)\n",
    "    epsilon = epsilon.expand_as(real_samples)\n",
    "\n",
    "    # Interpolate between real and fake samples\n",
    "    interpolates = (epsilon * real_samples + (1 - epsilon) * fake_samples).requires_grad_(True)\n",
    "    interpolates_output = critic(interpolates)\n",
    "\n",
    "    # Compute gradients\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=interpolates_output,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(interpolates_output),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "\n",
    "    # Reshape gradients\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_and_save_generated_waveforms(generators, z_dim, num_waveforms, device,epoch, sample_rate=16000, output_dir='generated_audio'):\n",
    "    import os\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    noise = generate_noise(num_waveforms, z_dim, device)\n",
    "    for idx, gen in enumerate(generators):\n",
    "        fake_waveforms = gen(noise).detach().cpu().numpy()\n",
    "        for i in range(num_waveforms):\n",
    "            waveform = fake_waveforms[i]\n",
    "            # Save each waveform to an audio file\n",
    "            filename = f'epoch{epoch+1}_gen{idx+1}_sample{i+1}.wav'\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            save_waveform_to_audio(waveform, sample_rate, filepath)\n",
    "            print(f\"Saved {filepath}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def pretrain_single_generator(num_epochs, z_dim, lr_gen, lr_disc, batch_size, seed, audio_length, output_dir, train_dataset):\n",
    "    # Set the seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    # Check for device (GPU or CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define the single generator and discriminator\n",
    "    generator = Generator(z_dim, audio_length).to(device)\n",
    "    discriminator = Discriminator(audio_length).to(device)\n",
    "\n",
    "    optimizer_gen = optim.Adam(generator.parameters(), lr=lr_gen, betas=(0.5, 0.9))\n",
    "    optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "    # Load and preprocess the audio dataset\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    lambda_gp = 10  # Gradient penalty coefficient\n",
    "    num_critic = 5  # Number of discriminator updates per generator update\n",
    "\n",
    "    # To track the losses\n",
    "    loss_disc_history = []\n",
    "    loss_gen_history = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        torch.cuda.empty_cache()  # Clear unused memory\n",
    "        loss_disc_epoch = 0\n",
    "        loss_gen_epoch = 0\n",
    "\n",
    "        for batch_idx, (real,) in enumerate(train_loader):\n",
    "            real = real.to(device)\n",
    "            batch_size = real.size(0)\n",
    "            if real.dim() == 2:\n",
    "                real = real.unsqueeze(1)  # Add channel dimension (batch_size, 1, audio_length)\n",
    "\n",
    "            #print(f\"real shape: {real.shape}\")\n",
    "\n",
    "            # Train Discriminator multiple times\n",
    "            for _ in range(num_critic):\n",
    "                optimizer_disc.zero_grad()\n",
    "\n",
    "                # Generate fake data\n",
    "                noise = generate_noise(batch_size, z_dim, device)\n",
    "                fake = generator(noise).detach()\n",
    "\n",
    "                # Compute discriminator outputs\n",
    "                disc_real = discriminator(real)\n",
    "                disc_fake = discriminator(fake)\n",
    "\n",
    "                # Compute Wasserstein loss\n",
    "                loss_disc_real = -torch.mean(disc_real)\n",
    "                loss_disc_fake = torch.mean(disc_fake)\n",
    "                loss_disc = loss_disc_real + loss_disc_fake\n",
    "\n",
    "                # Compute gradient penalty\n",
    "                gradient_penalty = compute_gradient_penalty(discriminator, real.data, fake.data, device)\n",
    "                loss_disc += lambda_gp * gradient_penalty\n",
    "\n",
    "                loss_disc.backward()\n",
    "                optimizer_disc.step()\n",
    "\n",
    "            loss_disc_epoch += loss_disc.item()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_gen.zero_grad()\n",
    "            noise = generate_noise(batch_size, z_dim, device)\n",
    "            fake = generator(noise)\n",
    "            disc_fake = discriminator(fake)\n",
    "            loss_gen = -torch.mean(disc_fake)\n",
    "            loss_gen.backward()\n",
    "            optimizer_gen.step()\n",
    "\n",
    "            loss_gen_epoch += loss_gen.item()\n",
    "\n",
    "        avg_loss_disc = loss_disc_epoch / len(train_loader)\n",
    "        avg_loss_gen = loss_gen_epoch / len(train_loader)\n",
    "\n",
    "        # Record the losses\n",
    "        loss_disc_history.append(avg_loss_disc)\n",
    "        loss_gen_history.append(avg_loss_gen)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {avg_loss_disc:.4f}, Loss G: {avg_loss_gen:.4f}\")\n",
    "\n",
    "        # Visualize generated waveforms\n",
    "        visualize_and_save_generated_waveforms(\n",
    "            [generator], z_dim, num_waveforms=5, device=device, epoch=epoch, sample_rate=16000, output_dir=output_dir\n",
    "        )\n",
    "\n",
    "    # Save the generator model\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    torch.save(generator.state_dict(), os.path.join(output_dir, \"pretrained_generator.pth\"))\n",
    "    print(f\"Pretrained generator model saved to {os.path.join(output_dir, 'pretrained_generator.pth')}\")\n",
    "\n",
    "    # Plot the learning curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(loss_disc_history, label=\"Discriminator Loss\")\n",
    "    plt.plot(loss_gen_history, label=\"Generator Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Learning Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, \"learning_curves.png\"))\n",
    "    plt.show()\n",
    "\n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import os\n",
    "from torch.nn.utils import spectral_norm\n",
    "import torch.autograd as autograd\n",
    "# Wasserstein loss\n",
    "def wasserstein_loss(y_pred, y_true):\n",
    "    return torch.mean(y_pred * y_true)\n",
    "\n",
    "def compute_gradient_penalty(discriminator, real_data, fake_data, device):\n",
    "    batch_size = real_data.size(0)\n",
    "    # Corrected epsilon shape to match real_data dimensions\n",
    "    epsilon = torch.rand(batch_size, 1, 1, device=device)  # Shape: [batch_size, 1, 1]\n",
    "    epsilon = epsilon.expand_as(real_data)  # Now expands to [batch_size, 1, audio_length]\n",
    "    \n",
    "    # Interpolate between real and fake data\n",
    "    interpolates = epsilon * real_data + (1 - epsilon) * fake_data\n",
    "    interpolates = interpolates.requires_grad_(True)\n",
    "    \n",
    "    # Compute discriminator output\n",
    "    disc_interpolates = discriminator(interpolates)\n",
    "    \n",
    "    # Compute gradients\n",
    "    grad_outputs = torch.ones_like(disc_interpolates)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=disc_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=grad_outputs,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    # Reshape gradients\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "def train_gan_with_pretrained_generators(\n",
    "    pretrained_generator, num_epochs, z_dim, lr_gen, lr_disc, batch_size, train_dataset,\n",
    "    num_generators, seed, audio_length, output_dir, lambda_gp=10, lambda_ortho=0.1, num_critic=5,\n",
    "    checkpoint_dir='checkpoints', resume=True\n",
    "):\n",
    "    import os\n",
    "\n",
    "    # Set the seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize multiple generators with the pretrained generator\n",
    "    generators = []\n",
    "    for _ in range(num_generators):\n",
    "        gen = Generator(z_dim, audio_length).to(device)\n",
    "        gen.load_state_dict(pretrained_generator.state_dict())\n",
    "        generators.append(gen)\n",
    "\n",
    "    # Initialize Discriminator\n",
    "    discriminator = Discriminator(audio_length).to(device)\n",
    "\n",
    "    optimizer_gens = [optim.Adam(gen.parameters(), lr=lr_gen, betas=(0.5, 0.9)) for gen in generators]\n",
    "    optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "    # Initialize Encoder only if lambda_ortho > 0\n",
    "    if lambda_ortho > 0:\n",
    "        encoder = Encoder(audio_length).to(device)\n",
    "        optimizer_encoder = optim.Adam(encoder.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "    else:\n",
    "        encoder = None\n",
    "        optimizer_encoder = None\n",
    "\n",
    "    # Load and preprocess the audio dataset\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Check for existing checkpoint\n",
    "    start_epoch = 0\n",
    "    if resume and os.path.exists(os.path.join(checkpoint_dir, 'checkpoint.pth')):\n",
    "        print(\"Resuming from checkpoint...\")\n",
    "        checkpoint = torch.load(os.path.join(checkpoint_dir, 'checkpoint.pth'), map_location=device)\n",
    "        start_epoch = checkpoint['epoch'] + 1  # Next epoch to start from\n",
    "\n",
    "        # Load models\n",
    "        for idx, gen in enumerate(generators):\n",
    "            gen.load_state_dict(checkpoint['generator_state_dicts'][idx])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        if lambda_ortho > 0:\n",
    "            encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "\n",
    "        # Load optimizers\n",
    "        for idx, optimizer_gen in enumerate(optimizer_gens):\n",
    "            optimizer_gen.load_state_dict(checkpoint['optimizer_gens_state_dicts'][idx])\n",
    "        optimizer_disc.load_state_dict(checkpoint['optimizer_disc_state_dict'])\n",
    "        if lambda_ortho > 0:\n",
    "            optimizer_encoder.load_state_dict(checkpoint['optimizer_encoder_state_dict'])\n",
    "\n",
    "        # Load loss histories\n",
    "        loss_disc_history = checkpoint['loss_disc_history']\n",
    "        loss_gens_history = checkpoint['loss_gens_history']\n",
    "\n",
    "    else:\n",
    "        print(\"Starting training from scratch.\")\n",
    "        start_epoch = 0\n",
    "        # Initialize loss histories\n",
    "        loss_disc_history = []\n",
    "        loss_gens_history = [[] for _ in range(num_generators)]\n",
    "\n",
    "    # Training loop\n",
    "    try:\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            torch.cuda.empty_cache()\n",
    "            loss_disc_epoch = 0\n",
    "            loss_gens_epoch = [0] * num_generators\n",
    "\n",
    "            for batch_idx, (real,) in enumerate(train_loader):\n",
    "                real = real.to(device)\n",
    "                if real.dim() == 2:\n",
    "                    real = real.unsqueeze(1)  # Add channel dimension (batch_size, 1, audio_length)\n",
    "                batch_size = real.size(0)\n",
    "                real_label = -torch.ones(batch_size, 1, device=device)\n",
    "                fake_label = torch.ones(batch_size, 1, device=device)\n",
    "\n",
    "                real = real + 0.001 * torch.randn_like(real)\n",
    "\n",
    "                # Train Discriminator multiple times\n",
    "                for _ in range(num_critic):\n",
    "                    optimizer_disc.zero_grad()\n",
    "\n",
    "                    disc_real = discriminator(real)\n",
    "\n",
    "                    noises = [generate_noise(batch_size, z_dim, device) for _ in range(num_generators)]\n",
    "                    fakes = [gen(noises[idx]).detach() for idx, gen in enumerate(generators)]\n",
    "\n",
    "                    for idx in range(num_generators):\n",
    "                        fakes[idx] = fakes[idx] + 0.001 * torch.randn_like(fakes[idx])\n",
    "\n",
    "                    disc_fakes = [discriminator(fake) for fake in fakes]\n",
    "\n",
    "                    # Average the fake losses\n",
    "                    loss_disc_fake = sum(wasserstein_loss(disc_fake, fake_label) for disc_fake in disc_fakes) / num_generators\n",
    "                    loss_disc_real = wasserstein_loss(disc_real, real_label)\n",
    "                    loss_disc = loss_disc_real + loss_disc_fake\n",
    "\n",
    "                    # Average gradient penalty\n",
    "                    gradient_penalty = sum(\n",
    "                        compute_gradient_penalty(discriminator, real, fake, device) for fake in fakes\n",
    "                    ) / num_generators\n",
    "                    loss_disc += lambda_gp * gradient_penalty\n",
    "\n",
    "                    loss_disc.backward()\n",
    "                    optimizer_disc.step()\n",
    "\n",
    "                loss_disc_epoch += loss_disc.item()\n",
    "\n",
    "                # Train Generators\n",
    "                for idx, gen in enumerate(generators):\n",
    "                    optimizer_gens[idx].zero_grad()\n",
    "                    if lambda_ortho > 0:\n",
    "                        optimizer_encoder.zero_grad()\n",
    "\n",
    "                    noise = generate_noise(batch_size, z_dim, device)\n",
    "                    fake = gen(noise)\n",
    "                    disc_fake = discriminator(fake)\n",
    "\n",
    "                    loss_gen = wasserstein_loss(disc_fake, real_label)\n",
    "\n",
    "                    if lambda_ortho > 0:\n",
    "                        # Compute orthogonal loss\n",
    "                        gen_feature = encoder(fake)\n",
    "                        ortho_loss_total = 0\n",
    "                        for other_idx, other_gen in enumerate(generators):\n",
    "                            if idx != other_idx:\n",
    "                                other_noise = generate_noise(batch_size, z_dim, device)\n",
    "                                other_fake = other_gen(other_noise)\n",
    "                                other_feature = encoder(other_fake)\n",
    "                                ortho_loss = orthogonal_loss(gen_feature, other_feature)\n",
    "                                ortho_loss_total += ortho_loss\n",
    "\n",
    "                        ortho_loss_total /= (num_generators - 1)\n",
    "                        total_loss_gen = loss_gen + lambda_ortho * ortho_loss_total\n",
    "                    else:\n",
    "                        total_loss_gen = loss_gen\n",
    "\n",
    "                    total_loss_gen.backward()\n",
    "                    optimizer_gens[idx].step()\n",
    "\n",
    "                    if lambda_ortho > 0:\n",
    "                        optimizer_encoder.step()\n",
    "\n",
    "                    loss_gens_epoch[idx] += total_loss_gen.item()\n",
    "\n",
    "            avg_loss_disc = loss_disc_epoch / len(train_loader)\n",
    "            avg_loss_gens = [loss / len(train_loader) for loss in loss_gens_epoch]\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {avg_loss_disc:.4f}\")\n",
    "            for idx in range(num_generators):\n",
    "                print(f\"Loss G{idx+1}: {avg_loss_gens[idx]:.4f}\")\n",
    "            print('-' * 50)\n",
    "\n",
    "            # Record the losses\n",
    "            loss_disc_history.append(avg_loss_disc)\n",
    "            for idx in range(num_generators):\n",
    "                loss_gens_history[idx].append(avg_loss_gens[idx])\n",
    "\n",
    "            # Visualize generated waveforms\n",
    "            visualize_and_save_generated_waveforms(\n",
    "                generators, z_dim, num_waveforms=5, device=device, epoch=epoch, sample_rate=16000, output_dir=output_dir\n",
    "            )\n",
    "\n",
    "            # Save checkpoint after each epoch\n",
    "            checkpoint = {\n",
    "                'epo  ch': epoch,\n",
    "                'generator_state_dicts': [gen.state_dict() for gen in generators],\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_gens_state_dicts': [opt.state_dict() for opt in optimizer_gens],\n",
    "                'optimizer_disc_state_dict': optimizer_disc.state_dict(),\n",
    "                'loss_disc_history': loss_disc_history,\n",
    "                'loss_gens_history': loss_gens_history\n",
    "            }\n",
    "            if lambda_ortho > 0:\n",
    "                checkpoint['encoder_state_dict'] = encoder.state_dict()\n",
    "                checkpoint['optimizer_encoder_state_dict'] = optimizer_encoder.state_dict()\n",
    "\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            torch.save(checkpoint, os.path.join(checkpoint_dir, 'checkpoint.pth'))\n",
    "            print(f\"Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"Training interrupted at epoch {epoch}. Saving checkpoint...\")\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dicts': [gen.state_dict() for gen in generators],\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_gens_state_dicts': [opt.state_dict() for opt in optimizer_gens],\n",
    "            'optimizer_disc_state_dict': optimizer_disc.state_dict(),\n",
    "            'loss_disc_history': loss_disc_history,\n",
    "            'loss_gens_history': loss_gens_history\n",
    "        }\n",
    "        if lambda_ortho > 0:\n",
    "            checkpoint['encoder_state_dict'] = encoder.state_dict()\n",
    "            checkpoint['optimizer_encoder_state_dict'] = optimizer_encoder.state_dict()\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, 'checkpoint.pth'))\n",
    "        print(f\"Checkpoint saved at epoch {epoch}\")\n",
    "        print(\"Exiting training early.\")\n",
    "        return generators\n",
    "\n",
    "    return generators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_multiple_generators(pretrained_generator, num_generators, z_dim):\n",
    "    # Initialize multiple generators from the pretrained generator's weights\n",
    "    generators = []\n",
    "    for _ in range(num_generators):\n",
    "        new_generator = Generator(z_dim).to(pretrained_generator.gen[0].weight.device)  # Ensure same device\n",
    "        new_generator.load_state_dict(pretrained_generator.state_dict())  # Copy weights\n",
    "        generators.append(new_generator)\n",
    "    return generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_generator = pretrain_single_generator(num_epochs=20, \n",
    "#                                                  z_dim=100, lr_disc=0.0002,lr_gen=0.0002, batch_size=64, seed=42, audio_length=64000, \n",
    "#                                                  output_dir='waveform_pre', train_dataset=train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_generator(z_dim, weight_path, device, audio_length):\n",
    "    \"\"\"\n",
    "    Load the pretrained generator model weights.\n",
    "\n",
    "    Args:\n",
    "        z_dim (int): Latent space dimension.\n",
    "        weight_path (str): Path to the saved model weights.\n",
    "        device (torch.device): Device to load the model onto.\n",
    "\n",
    "    Returns:\n",
    "        Generator: Loaded generator model.\n",
    "    \"\"\"\n",
    "    generator = Generator(z_dim, audio_length=audio_length).to(device)\n",
    "    generator.load_state_dict(torch.load(weight_path, map_location=device))\n",
    "    generator.eval()  # Set the model to evaluation mode\n",
    "    print(f\"Pretrained generator model loaded from {weight_path}\")\n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12479/3981401217.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  generator.load_state_dict(torch.load(weight_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained generator model loaded from waveform_pre/pretrained_generator.pth\n"
     ]
    }
   ],
   "source": [
    "pretrained_generator = load_pretrained_generator(\n",
    "    z_dim=100,\n",
    "    weight_path='waveform_pre/pretrained_generator.pth',  # Path to saved weights\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), audio_length = audio_length\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training with multiple generators initialized from the pretrained one\n",
    "# train_gan_with_pretrained_generators(pretrained_generator, num_epochs=200, z_dim=100, lr_disc=0.0002, batch_size=24, num_generators=3, seed=42, lr_gen=0.0002, output_dir='waveform_mul', audio_length=64000, train_dataset = train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Clear unused CUDA memory\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accual train code (resumable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12479/1655430510.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(os.path.join(checkpoint_dir, 'checkpoint.pth'), map_location=device)\n",
      "/media/tameszaza/My Passport/ml/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/200] Loss D: -6.6362\n",
      "Loss G1: 6.9490\n",
      "Loss G2: 7.3247\n",
      "Loss G3: 6.6776\n",
      "Loss G4: 6.8954\n",
      "Loss G5: 6.8117\n",
      "--------------------------------------------------\n",
      "Saved waveform_mal_2/epoch21_gen1_sample1.wav\n",
      "Saved waveform_mal_2/epoch21_gen1_sample2.wav\n",
      "Saved waveform_mal_2/epoch21_gen1_sample3.wav\n",
      "Saved waveform_mal_2/epoch21_gen1_sample4.wav\n",
      "Saved waveform_mal_2/epoch21_gen1_sample5.wav\n",
      "Saved waveform_mal_2/epoch21_gen2_sample1.wav\n",
      "Saved waveform_mal_2/epoch21_gen2_sample2.wav\n",
      "Saved waveform_mal_2/epoch21_gen2_sample3.wav\n",
      "Saved waveform_mal_2/epoch21_gen2_sample4.wav\n",
      "Saved waveform_mal_2/epoch21_gen2_sample5.wav\n",
      "Saved waveform_mal_2/epoch21_gen3_sample1.wav\n",
      "Saved waveform_mal_2/epoch21_gen3_sample2.wav\n",
      "Saved waveform_mal_2/epoch21_gen3_sample3.wav\n",
      "Saved waveform_mal_2/epoch21_gen3_sample4.wav\n",
      "Saved waveform_mal_2/epoch21_gen3_sample5.wav\n",
      "Saved waveform_mal_2/epoch21_gen4_sample1.wav\n",
      "Saved waveform_mal_2/epoch21_gen4_sample2.wav\n",
      "Saved waveform_mal_2/epoch21_gen4_sample3.wav\n",
      "Saved waveform_mal_2/epoch21_gen4_sample4.wav\n",
      "Saved waveform_mal_2/epoch21_gen4_sample5.wav\n",
      "Saved waveform_mal_2/epoch21_gen5_sample1.wav\n",
      "Saved waveform_mal_2/epoch21_gen5_sample2.wav\n",
      "Saved waveform_mal_2/epoch21_gen5_sample3.wav\n",
      "Saved waveform_mal_2/epoch21_gen5_sample4.wav\n",
      "Saved waveform_mal_2/epoch21_gen5_sample5.wav\n",
      "Checkpoint saved at epoch 20\n",
      "Epoch [22/200] Loss D: -6.4225\n",
      "Loss G1: 6.7469\n",
      "Loss G2: 6.8436\n",
      "Loss G3: 6.3975\n",
      "Loss G4: 6.6017\n",
      "Loss G5: 6.3336\n",
      "--------------------------------------------------\n",
      "Saved waveform_mal_2/epoch22_gen1_sample1.wav\n",
      "Saved waveform_mal_2/epoch22_gen1_sample2.wav\n",
      "Saved waveform_mal_2/epoch22_gen1_sample3.wav\n",
      "Saved waveform_mal_2/epoch22_gen1_sample4.wav\n",
      "Saved waveform_mal_2/epoch22_gen1_sample5.wav\n",
      "Saved waveform_mal_2/epoch22_gen2_sample1.wav\n",
      "Saved waveform_mal_2/epoch22_gen2_sample2.wav\n",
      "Saved waveform_mal_2/epoch22_gen2_sample3.wav\n",
      "Saved waveform_mal_2/epoch22_gen2_sample4.wav\n",
      "Saved waveform_mal_2/epoch22_gen2_sample5.wav\n",
      "Saved waveform_mal_2/epoch22_gen3_sample1.wav\n",
      "Saved waveform_mal_2/epoch22_gen3_sample2.wav\n",
      "Saved waveform_mal_2/epoch22_gen3_sample3.wav\n",
      "Saved waveform_mal_2/epoch22_gen3_sample4.wav\n",
      "Saved waveform_mal_2/epoch22_gen3_sample5.wav\n",
      "Saved waveform_mal_2/epoch22_gen4_sample1.wav\n",
      "Saved waveform_mal_2/epoch22_gen4_sample2.wav\n",
      "Saved waveform_mal_2/epoch22_gen4_sample3.wav\n",
      "Saved waveform_mal_2/epoch22_gen4_sample4.wav\n",
      "Saved waveform_mal_2/epoch22_gen4_sample5.wav\n",
      "Saved waveform_mal_2/epoch22_gen5_sample1.wav\n",
      "Saved waveform_mal_2/epoch22_gen5_sample2.wav\n",
      "Saved waveform_mal_2/epoch22_gen5_sample3.wav\n",
      "Saved waveform_mal_2/epoch22_gen5_sample4.wav\n",
      "Saved waveform_mal_2/epoch22_gen5_sample5.wav\n",
      "Checkpoint saved at epoch 21\n",
      "Epoch [23/200] Loss D: -6.2923\n",
      "Loss G1: 7.2518\n",
      "Loss G2: 7.7186\n",
      "Loss G3: 6.9967\n",
      "Loss G4: 7.2374\n",
      "Loss G5: 6.9533\n",
      "--------------------------------------------------\n",
      "Saved waveform_mal_2/epoch23_gen1_sample1.wav\n",
      "Saved waveform_mal_2/epoch23_gen1_sample2.wav\n",
      "Saved waveform_mal_2/epoch23_gen1_sample3.wav\n",
      "Saved waveform_mal_2/epoch23_gen1_sample4.wav\n",
      "Saved waveform_mal_2/epoch23_gen1_sample5.wav\n",
      "Saved waveform_mal_2/epoch23_gen2_sample1.wav\n",
      "Saved waveform_mal_2/epoch23_gen2_sample2.wav\n",
      "Saved waveform_mal_2/epoch23_gen2_sample3.wav\n",
      "Saved waveform_mal_2/epoch23_gen2_sample4.wav\n",
      "Saved waveform_mal_2/epoch23_gen2_sample5.wav\n",
      "Saved waveform_mal_2/epoch23_gen3_sample1.wav\n",
      "Saved waveform_mal_2/epoch23_gen3_sample2.wav\n",
      "Saved waveform_mal_2/epoch23_gen3_sample3.wav\n",
      "Saved waveform_mal_2/epoch23_gen3_sample4.wav\n",
      "Saved waveform_mal_2/epoch23_gen3_sample5.wav\n",
      "Saved waveform_mal_2/epoch23_gen4_sample1.wav\n",
      "Saved waveform_mal_2/epoch23_gen4_sample2.wav\n",
      "Saved waveform_mal_2/epoch23_gen4_sample3.wav\n",
      "Saved waveform_mal_2/epoch23_gen4_sample4.wav\n",
      "Saved waveform_mal_2/epoch23_gen4_sample5.wav\n",
      "Saved waveform_mal_2/epoch23_gen5_sample1.wav\n",
      "Saved waveform_mal_2/epoch23_gen5_sample2.wav\n",
      "Saved waveform_mal_2/epoch23_gen5_sample3.wav\n",
      "Saved waveform_mal_2/epoch23_gen5_sample4.wav\n",
      "Saved waveform_mal_2/epoch23_gen5_sample5.wav\n",
      "Checkpoint saved at epoch 22\n",
      "Epoch [24/200] Loss D: -6.1755\n",
      "Loss G1: 7.3752\n",
      "Loss G2: 7.9047\n",
      "Loss G3: 7.2341\n",
      "Loss G4: 7.2347\n",
      "Loss G5: 7.1832\n",
      "--------------------------------------------------\n",
      "Saved waveform_mal_2/epoch24_gen1_sample1.wav\n",
      "Saved waveform_mal_2/epoch24_gen1_sample2.wav\n",
      "Saved waveform_mal_2/epoch24_gen1_sample3.wav\n",
      "Saved waveform_mal_2/epoch24_gen1_sample4.wav\n",
      "Saved waveform_mal_2/epoch24_gen1_sample5.wav\n",
      "Saved waveform_mal_2/epoch24_gen2_sample1.wav\n",
      "Saved waveform_mal_2/epoch24_gen2_sample2.wav\n",
      "Saved waveform_mal_2/epoch24_gen2_sample3.wav\n",
      "Saved waveform_mal_2/epoch24_gen2_sample4.wav\n",
      "Saved waveform_mal_2/epoch24_gen2_sample5.wav\n",
      "Saved waveform_mal_2/epoch24_gen3_sample1.wav\n",
      "Saved waveform_mal_2/epoch24_gen3_sample2.wav\n",
      "Saved waveform_mal_2/epoch24_gen3_sample3.wav\n",
      "Saved waveform_mal_2/epoch24_gen3_sample4.wav\n",
      "Saved waveform_mal_2/epoch24_gen3_sample5.wav\n",
      "Saved waveform_mal_2/epoch24_gen4_sample1.wav\n",
      "Saved waveform_mal_2/epoch24_gen4_sample2.wav\n",
      "Saved waveform_mal_2/epoch24_gen4_sample3.wav\n",
      "Saved waveform_mal_2/epoch24_gen4_sample4.wav\n",
      "Saved waveform_mal_2/epoch24_gen4_sample5.wav\n",
      "Saved waveform_mal_2/epoch24_gen5_sample1.wav\n",
      "Saved waveform_mal_2/epoch24_gen5_sample2.wav\n",
      "Saved waveform_mal_2/epoch24_gen5_sample3.wav\n",
      "Saved waveform_mal_2/epoch24_gen5_sample4.wav\n",
      "Saved waveform_mal_2/epoch24_gen5_sample5.wav\n",
      "Checkpoint saved at epoch 23\n",
      "Epoch [25/200] Loss D: -5.9572\n",
      "Loss G1: 6.4520\n",
      "Loss G2: 6.5443\n",
      "Loss G3: 6.3132\n",
      "Loss G4: 6.3295\n",
      "Loss G5: 6.1096\n",
      "--------------------------------------------------\n",
      "Saved waveform_mal_2/epoch25_gen1_sample1.wav\n",
      "Saved waveform_mal_2/epoch25_gen1_sample2.wav\n",
      "Saved waveform_mal_2/epoch25_gen1_sample3.wav\n",
      "Saved waveform_mal_2/epoch25_gen1_sample4.wav\n",
      "Saved waveform_mal_2/epoch25_gen1_sample5.wav\n",
      "Saved waveform_mal_2/epoch25_gen2_sample1.wav\n",
      "Saved waveform_mal_2/epoch25_gen2_sample2.wav\n",
      "Saved waveform_mal_2/epoch25_gen2_sample3.wav\n",
      "Saved waveform_mal_2/epoch25_gen2_sample4.wav\n",
      "Saved waveform_mal_2/epoch25_gen2_sample5.wav\n",
      "Saved waveform_mal_2/epoch25_gen3_sample1.wav\n",
      "Saved waveform_mal_2/epoch25_gen3_sample2.wav\n",
      "Saved waveform_mal_2/epoch25_gen3_sample3.wav\n",
      "Saved waveform_mal_2/epoch25_gen3_sample4.wav\n",
      "Saved waveform_mal_2/epoch25_gen3_sample5.wav\n",
      "Saved waveform_mal_2/epoch25_gen4_sample1.wav\n",
      "Saved waveform_mal_2/epoch25_gen4_sample2.wav\n",
      "Saved waveform_mal_2/epoch25_gen4_sample3.wav\n",
      "Saved waveform_mal_2/epoch25_gen4_sample4.wav\n",
      "Saved waveform_mal_2/epoch25_gen4_sample5.wav\n",
      "Saved waveform_mal_2/epoch25_gen5_sample1.wav\n",
      "Saved waveform_mal_2/epoch25_gen5_sample2.wav\n",
      "Saved waveform_mal_2/epoch25_gen5_sample3.wav\n",
      "Saved waveform_mal_2/epoch25_gen5_sample4.wav\n",
      "Saved waveform_mal_2/epoch25_gen5_sample5.wav\n",
      "Checkpoint saved at epoch 24\n",
      "Epoch [26/200] Loss D: -5.9860\n",
      "Loss G1: 6.2484\n",
      "Loss G2: 6.6539\n",
      "Loss G3: 6.0579\n",
      "Loss G4: 6.1516\n",
      "Loss G5: 5.7014\n",
      "--------------------------------------------------\n",
      "Saved waveform_mal_2/epoch26_gen1_sample1.wav\n",
      "Saved waveform_mal_2/epoch26_gen1_sample2.wav\n",
      "Saved waveform_mal_2/epoch26_gen1_sample3.wav\n",
      "Saved waveform_mal_2/epoch26_gen1_sample4.wav\n",
      "Saved waveform_mal_2/epoch26_gen1_sample5.wav\n",
      "Saved waveform_mal_2/epoch26_gen2_sample1.wav\n",
      "Saved waveform_mal_2/epoch26_gen2_sample2.wav\n",
      "Saved waveform_mal_2/epoch26_gen2_sample3.wav\n",
      "Saved waveform_mal_2/epoch26_gen2_sample4.wav\n",
      "Saved waveform_mal_2/epoch26_gen2_sample5.wav\n",
      "Saved waveform_mal_2/epoch26_gen3_sample1.wav\n",
      "Saved waveform_mal_2/epoch26_gen3_sample2.wav\n",
      "Saved waveform_mal_2/epoch26_gen3_sample3.wav\n",
      "Saved waveform_mal_2/epoch26_gen3_sample4.wav\n",
      "Saved waveform_mal_2/epoch26_gen3_sample5.wav\n",
      "Saved waveform_mal_2/epoch26_gen4_sample1.wav\n",
      "Saved waveform_mal_2/epoch26_gen4_sample2.wav\n",
      "Saved waveform_mal_2/epoch26_gen4_sample3.wav\n",
      "Saved waveform_mal_2/epoch26_gen4_sample4.wav\n",
      "Saved waveform_mal_2/epoch26_gen4_sample5.wav\n",
      "Saved waveform_mal_2/epoch26_gen5_sample1.wav\n",
      "Saved waveform_mal_2/epoch26_gen5_sample2.wav\n",
      "Saved waveform_mal_2/epoch26_gen5_sample3.wav\n",
      "Saved waveform_mal_2/epoch26_gen5_sample4.wav\n",
      "Saved waveform_mal_2/epoch26_gen5_sample5.wav\n",
      "Checkpoint saved at epoch 25\n",
      "Epoch [27/200] Loss D: -5.7742\n",
      "Loss G1: 7.3982\n",
      "Loss G2: 7.6130\n",
      "Loss G3: 7.1258\n",
      "Loss G4: 7.2277\n",
      "Loss G5: 6.9204\n",
      "--------------------------------------------------\n",
      "Saved waveform_mal_2/epoch27_gen1_sample1.wav\n",
      "Saved waveform_mal_2/epoch27_gen1_sample2.wav\n",
      "Saved waveform_mal_2/epoch27_gen1_sample3.wav\n",
      "Saved waveform_mal_2/epoch27_gen1_sample4.wav\n",
      "Saved waveform_mal_2/epoch27_gen1_sample5.wav\n",
      "Saved waveform_mal_2/epoch27_gen2_sample1.wav\n",
      "Saved waveform_mal_2/epoch27_gen2_sample2.wav\n",
      "Saved waveform_mal_2/epoch27_gen2_sample3.wav\n",
      "Saved waveform_mal_2/epoch27_gen2_sample4.wav\n",
      "Saved waveform_mal_2/epoch27_gen2_sample5.wav\n",
      "Saved waveform_mal_2/epoch27_gen3_sample1.wav\n",
      "Saved waveform_mal_2/epoch27_gen3_sample2.wav\n",
      "Saved waveform_mal_2/epoch27_gen3_sample3.wav\n",
      "Saved waveform_mal_2/epoch27_gen3_sample4.wav\n",
      "Saved waveform_mal_2/epoch27_gen3_sample5.wav\n",
      "Saved waveform_mal_2/epoch27_gen4_sample1.wav\n",
      "Saved waveform_mal_2/epoch27_gen4_sample2.wav\n",
      "Saved waveform_mal_2/epoch27_gen4_sample3.wav\n",
      "Saved waveform_mal_2/epoch27_gen4_sample4.wav\n",
      "Saved waveform_mal_2/epoch27_gen4_sample5.wav\n",
      "Saved waveform_mal_2/epoch27_gen5_sample1.wav\n",
      "Saved waveform_mal_2/epoch27_gen5_sample2.wav\n",
      "Saved waveform_mal_2/epoch27_gen5_sample3.wav\n",
      "Saved waveform_mal_2/epoch27_gen5_sample4.wav\n",
      "Saved waveform_mal_2/epoch27_gen5_sample5.wav\n",
      "Checkpoint saved at epoch 26\n",
      "Epoch [28/200] Loss D: -5.6733\n",
      "Loss G1: 7.4383\n",
      "Loss G2: 7.7341\n",
      "Loss G3: 7.1690\n",
      "Loss G4: 7.1717\n",
      "Loss G5: 6.8406\n",
      "--------------------------------------------------\n",
      "Saved waveform_mal_2/epoch28_gen1_sample1.wav\n",
      "Saved waveform_mal_2/epoch28_gen1_sample2.wav\n",
      "Saved waveform_mal_2/epoch28_gen1_sample3.wav\n",
      "Saved waveform_mal_2/epoch28_gen1_sample4.wav\n",
      "Saved waveform_mal_2/epoch28_gen1_sample5.wav\n",
      "Saved waveform_mal_2/epoch28_gen2_sample1.wav\n",
      "Saved waveform_mal_2/epoch28_gen2_sample2.wav\n",
      "Saved waveform_mal_2/epoch28_gen2_sample3.wav\n",
      "Saved waveform_mal_2/epoch28_gen2_sample4.wav\n",
      "Saved waveform_mal_2/epoch28_gen2_sample5.wav\n",
      "Saved waveform_mal_2/epoch28_gen3_sample1.wav\n",
      "Saved waveform_mal_2/epoch28_gen3_sample2.wav\n",
      "Saved waveform_mal_2/epoch28_gen3_sample3.wav\n",
      "Saved waveform_mal_2/epoch28_gen3_sample4.wav\n",
      "Saved waveform_mal_2/epoch28_gen3_sample5.wav\n",
      "Saved waveform_mal_2/epoch28_gen4_sample1.wav\n",
      "Saved waveform_mal_2/epoch28_gen4_sample2.wav\n",
      "Saved waveform_mal_2/epoch28_gen4_sample3.wav\n",
      "Saved waveform_mal_2/epoch28_gen4_sample4.wav\n",
      "Saved waveform_mal_2/epoch28_gen4_sample5.wav\n",
      "Saved waveform_mal_2/epoch28_gen5_sample1.wav\n",
      "Saved waveform_mal_2/epoch28_gen5_sample2.wav\n",
      "Saved waveform_mal_2/epoch28_gen5_sample3.wav\n",
      "Saved waveform_mal_2/epoch28_gen5_sample4.wav\n",
      "Saved waveform_mal_2/epoch28_gen5_sample5.wav\n",
      "Checkpoint saved at epoch 27\n",
      "Epoch [29/200] Loss D: -5.6581\n",
      "Loss G1: 7.4820\n",
      "Loss G2: 7.4775\n",
      "Loss G3: 7.0831\n",
      "Loss G4: 7.0098\n",
      "Loss G5: 6.5835\n",
      "--------------------------------------------------\n",
      "Saved waveform_mal_2/epoch29_gen1_sample1.wav\n",
      "Saved waveform_mal_2/epoch29_gen1_sample2.wav\n",
      "Saved waveform_mal_2/epoch29_gen1_sample3.wav\n",
      "Saved waveform_mal_2/epoch29_gen1_sample4.wav\n",
      "Saved waveform_mal_2/epoch29_gen1_sample5.wav\n",
      "Saved waveform_mal_2/epoch29_gen2_sample1.wav\n",
      "Saved waveform_mal_2/epoch29_gen2_sample2.wav\n",
      "Saved waveform_mal_2/epoch29_gen2_sample3.wav\n",
      "Saved waveform_mal_2/epoch29_gen2_sample4.wav\n",
      "Saved waveform_mal_2/epoch29_gen2_sample5.wav\n",
      "Saved waveform_mal_2/epoch29_gen3_sample1.wav\n",
      "Saved waveform_mal_2/epoch29_gen3_sample2.wav\n",
      "Saved waveform_mal_2/epoch29_gen3_sample3.wav\n",
      "Saved waveform_mal_2/epoch29_gen3_sample4.wav\n",
      "Saved waveform_mal_2/epoch29_gen3_sample5.wav\n",
      "Saved waveform_mal_2/epoch29_gen4_sample1.wav\n",
      "Saved waveform_mal_2/epoch29_gen4_sample2.wav\n",
      "Saved waveform_mal_2/epoch29_gen4_sample3.wav\n",
      "Saved waveform_mal_2/epoch29_gen4_sample4.wav\n",
      "Saved waveform_mal_2/epoch29_gen4_sample5.wav\n",
      "Saved waveform_mal_2/epoch29_gen5_sample1.wav\n",
      "Saved waveform_mal_2/epoch29_gen5_sample2.wav\n",
      "Saved waveform_mal_2/epoch29_gen5_sample3.wav\n",
      "Saved waveform_mal_2/epoch29_gen5_sample4.wav\n",
      "Saved waveform_mal_2/epoch29_gen5_sample5.wav\n",
      "Checkpoint saved at epoch 28\n",
      "Epoch [30/200] Loss D: -5.5274\n",
      "Loss G1: 6.3073\n",
      "Loss G2: 6.4655\n",
      "Loss G3: 6.2647\n",
      "Loss G4: 6.2294\n",
      "Loss G5: 5.8093\n",
      "--------------------------------------------------\n",
      "Saved waveform_mal_2/epoch30_gen1_sample1.wav\n",
      "Saved waveform_mal_2/epoch30_gen1_sample2.wav\n",
      "Saved waveform_mal_2/epoch30_gen1_sample3.wav\n",
      "Saved waveform_mal_2/epoch30_gen1_sample4.wav\n",
      "Saved waveform_mal_2/epoch30_gen1_sample5.wav\n",
      "Saved waveform_mal_2/epoch30_gen2_sample1.wav\n",
      "Saved waveform_mal_2/epoch30_gen2_sample2.wav\n",
      "Saved waveform_mal_2/epoch30_gen2_sample3.wav\n",
      "Saved waveform_mal_2/epoch30_gen2_sample4.wav\n",
      "Saved waveform_mal_2/epoch30_gen2_sample5.wav\n",
      "Saved waveform_mal_2/epoch30_gen3_sample1.wav\n",
      "Saved waveform_mal_2/epoch30_gen3_sample2.wav\n",
      "Saved waveform_mal_2/epoch30_gen3_sample3.wav\n",
      "Saved waveform_mal_2/epoch30_gen3_sample4.wav\n",
      "Saved waveform_mal_2/epoch30_gen3_sample5.wav\n",
      "Saved waveform_mal_2/epoch30_gen4_sample1.wav\n",
      "Saved waveform_mal_2/epoch30_gen4_sample2.wav\n",
      "Saved waveform_mal_2/epoch30_gen4_sample3.wav\n",
      "Saved waveform_mal_2/epoch30_gen4_sample4.wav\n",
      "Saved waveform_mal_2/epoch30_gen4_sample5.wav\n",
      "Saved waveform_mal_2/epoch30_gen5_sample1.wav\n",
      "Saved waveform_mal_2/epoch30_gen5_sample2.wav\n",
      "Saved waveform_mal_2/epoch30_gen5_sample3.wav\n",
      "Saved waveform_mal_2/epoch30_gen5_sample4.wav\n",
      "Saved waveform_mal_2/epoch30_gen5_sample5.wav\n",
      "Checkpoint saved at epoch 29\n",
      "Epoch [31/200] Loss D: -5.5445\n",
      "Loss G1: 7.8938\n",
      "Loss G2: 7.8252\n",
      "Loss G3: 7.4215\n",
      "Loss G4: 7.3666\n",
      "Loss G5: 6.8742\n",
      "--------------------------------------------------\n",
      "Saved waveform_mal_2/epoch31_gen1_sample1.wav\n",
      "Saved waveform_mal_2/epoch31_gen1_sample2.wav\n",
      "Saved waveform_mal_2/epoch31_gen1_sample3.wav\n",
      "Saved waveform_mal_2/epoch31_gen1_sample4.wav\n",
      "Saved waveform_mal_2/epoch31_gen1_sample5.wav\n",
      "Saved waveform_mal_2/epoch31_gen2_sample1.wav\n",
      "Saved waveform_mal_2/epoch31_gen2_sample2.wav\n",
      "Saved waveform_mal_2/epoch31_gen2_sample3.wav\n",
      "Saved waveform_mal_2/epoch31_gen2_sample4.wav\n",
      "Saved waveform_mal_2/epoch31_gen2_sample5.wav\n",
      "Saved waveform_mal_2/epoch31_gen3_sample1.wav\n",
      "Saved waveform_mal_2/epoch31_gen3_sample2.wav\n",
      "Saved waveform_mal_2/epoch31_gen3_sample3.wav\n",
      "Saved waveform_mal_2/epoch31_gen3_sample4.wav\n",
      "Saved waveform_mal_2/epoch31_gen3_sample5.wav\n",
      "Saved waveform_mal_2/epoch31_gen4_sample1.wav\n",
      "Saved waveform_mal_2/epoch31_gen4_sample2.wav\n",
      "Saved waveform_mal_2/epoch31_gen4_sample3.wav\n",
      "Saved waveform_mal_2/epoch31_gen4_sample4.wav\n",
      "Saved waveform_mal_2/epoch31_gen4_sample5.wav\n",
      "Saved waveform_mal_2/epoch31_gen5_sample1.wav\n",
      "Saved waveform_mal_2/epoch31_gen5_sample2.wav\n",
      "Saved waveform_mal_2/epoch31_gen5_sample3.wav\n",
      "Saved waveform_mal_2/epoch31_gen5_sample4.wav\n",
      "Saved waveform_mal_2/epoch31_gen5_sample5.wav\n",
      "Checkpoint saved at epoch 30\n",
      "Epoch [32/200] Loss D: -5.3638\n",
      "Loss G1: 7.1889\n",
      "Loss G2: 7.1541\n",
      "Loss G3: 6.8124\n",
      "Loss G4: 6.7520\n",
      "Loss G5: 6.2872\n",
      "--------------------------------------------------\n",
      "Saved waveform_mal_2/epoch32_gen1_sample1.wav\n",
      "Saved waveform_mal_2/epoch32_gen1_sample2.wav\n",
      "Saved waveform_mal_2/epoch32_gen1_sample3.wav\n",
      "Saved waveform_mal_2/epoch32_gen1_sample4.wav\n",
      "Saved waveform_mal_2/epoch32_gen1_sample5.wav\n",
      "Saved waveform_mal_2/epoch32_gen2_sample1.wav\n",
      "Saved waveform_mal_2/epoch32_gen2_sample2.wav\n",
      "Saved waveform_mal_2/epoch32_gen2_sample3.wav\n",
      "Saved waveform_mal_2/epoch32_gen2_sample4.wav\n",
      "Saved waveform_mal_2/epoch32_gen2_sample5.wav\n",
      "Saved waveform_mal_2/epoch32_gen3_sample1.wav\n",
      "Saved waveform_mal_2/epoch32_gen3_sample2.wav\n",
      "Saved waveform_mal_2/epoch32_gen3_sample3.wav\n",
      "Saved waveform_mal_2/epoch32_gen3_sample4.wav\n",
      "Saved waveform_mal_2/epoch32_gen3_sample5.wav\n",
      "Saved waveform_mal_2/epoch32_gen4_sample1.wav\n",
      "Saved waveform_mal_2/epoch32_gen4_sample2.wav\n",
      "Saved waveform_mal_2/epoch32_gen4_sample3.wav\n",
      "Saved waveform_mal_2/epoch32_gen4_sample4.wav\n",
      "Saved waveform_mal_2/epoch32_gen4_sample5.wav\n",
      "Saved waveform_mal_2/epoch32_gen5_sample1.wav\n",
      "Saved waveform_mal_2/epoch32_gen5_sample2.wav\n",
      "Saved waveform_mal_2/epoch32_gen5_sample3.wav\n",
      "Saved waveform_mal_2/epoch32_gen5_sample4.wav\n",
      "Saved waveform_mal_2/epoch32_gen5_sample5.wav\n",
      "Checkpoint saved at epoch 31\n",
      "Epoch [33/200] Loss D: -5.2489\n",
      "Loss G1: 7.7162\n",
      "Loss G2: 7.5334\n",
      "Loss G3: 7.2949\n",
      "Loss G4: 7.0016\n",
      "Loss G5: 6.6577\n",
      "--------------------------------------------------\n",
      "Saved waveform_mal_2/epoch33_gen1_sample1.wav\n",
      "Saved waveform_mal_2/epoch33_gen1_sample2.wav\n",
      "Saved waveform_mal_2/epoch33_gen1_sample3.wav\n",
      "Saved waveform_mal_2/epoch33_gen1_sample4.wav\n",
      "Saved waveform_mal_2/epoch33_gen1_sample5.wav\n",
      "Saved waveform_mal_2/epoch33_gen2_sample1.wav\n",
      "Saved waveform_mal_2/epoch33_gen2_sample2.wav\n",
      "Saved waveform_mal_2/epoch33_gen2_sample3.wav\n",
      "Saved waveform_mal_2/epoch33_gen2_sample4.wav\n",
      "Saved waveform_mal_2/epoch33_gen2_sample5.wav\n",
      "Saved waveform_mal_2/epoch33_gen3_sample1.wav\n",
      "Saved waveform_mal_2/epoch33_gen3_sample2.wav\n",
      "Saved waveform_mal_2/epoch33_gen3_sample3.wav\n",
      "Saved waveform_mal_2/epoch33_gen3_sample4.wav\n",
      "Saved waveform_mal_2/epoch33_gen3_sample5.wav\n",
      "Saved waveform_mal_2/epoch33_gen4_sample1.wav\n",
      "Saved waveform_mal_2/epoch33_gen4_sample2.wav\n",
      "Saved waveform_mal_2/epoch33_gen4_sample3.wav\n",
      "Saved waveform_mal_2/epoch33_gen4_sample4.wav\n",
      "Saved waveform_mal_2/epoch33_gen4_sample5.wav\n",
      "Saved waveform_mal_2/epoch33_gen5_sample1.wav\n",
      "Saved waveform_mal_2/epoch33_gen5_sample2.wav\n",
      "Saved waveform_mal_2/epoch33_gen5_sample3.wav\n",
      "Saved waveform_mal_2/epoch33_gen5_sample4.wav\n",
      "Saved waveform_mal_2/epoch33_gen5_sample5.wav\n",
      "Checkpoint saved at epoch 32\n",
      "Epoch [34/200] Loss D: -5.1988\n",
      "Loss G1: 7.4513\n",
      "Loss G2: 7.5465\n",
      "Loss G3: 7.1197\n",
      "Loss G4: 7.1884\n",
      "Loss G5: 6.7532\n",
      "--------------------------------------------------\n",
      "Saved waveform_mal_2/epoch34_gen1_sample1.wav\n",
      "Saved waveform_mal_2/epoch34_gen1_sample2.wav\n",
      "Saved waveform_mal_2/epoch34_gen1_sample3.wav\n",
      "Saved waveform_mal_2/epoch34_gen1_sample4.wav\n",
      "Saved waveform_mal_2/epoch34_gen1_sample5.wav\n",
      "Saved waveform_mal_2/epoch34_gen2_sample1.wav\n",
      "Saved waveform_mal_2/epoch34_gen2_sample2.wav\n",
      "Saved waveform_mal_2/epoch34_gen2_sample3.wav\n",
      "Saved waveform_mal_2/epoch34_gen2_sample4.wav\n",
      "Saved waveform_mal_2/epoch34_gen2_sample5.wav\n",
      "Saved waveform_mal_2/epoch34_gen3_sample1.wav\n",
      "Saved waveform_mal_2/epoch34_gen3_sample2.wav\n",
      "Saved waveform_mal_2/epoch34_gen3_sample3.wav\n",
      "Saved waveform_mal_2/epoch34_gen3_sample4.wav\n",
      "Saved waveform_mal_2/epoch34_gen3_sample5.wav\n",
      "Saved waveform_mal_2/epoch34_gen4_sample1.wav\n",
      "Saved waveform_mal_2/epoch34_gen4_sample2.wav\n",
      "Saved waveform_mal_2/epoch34_gen4_sample3.wav\n",
      "Saved waveform_mal_2/epoch34_gen4_sample4.wav\n",
      "Saved waveform_mal_2/epoch34_gen4_sample5.wav\n",
      "Saved waveform_mal_2/epoch34_gen5_sample1.wav\n",
      "Saved waveform_mal_2/epoch34_gen5_sample2.wav\n",
      "Saved waveform_mal_2/epoch34_gen5_sample3.wav\n",
      "Saved waveform_mal_2/epoch34_gen5_sample4.wav\n",
      "Saved waveform_mal_2/epoch34_gen5_sample5.wav\n",
      "Checkpoint saved at epoch 33\n",
      "Epoch [35/200] Loss D: -5.1305\n",
      "Loss G1: 7.6833\n",
      "Loss G2: 7.7760\n",
      "Loss G3: 7.2737\n",
      "Loss G4: 7.2236\n",
      "Loss G5: 6.9814\n",
      "--------------------------------------------------\n",
      "Saved waveform_mal_2/epoch35_gen1_sample1.wav\n",
      "Saved waveform_mal_2/epoch35_gen1_sample2.wav\n",
      "Saved waveform_mal_2/epoch35_gen1_sample3.wav\n",
      "Saved waveform_mal_2/epoch35_gen1_sample4.wav\n",
      "Saved waveform_mal_2/epoch35_gen1_sample5.wav\n",
      "Saved waveform_mal_2/epoch35_gen2_sample1.wav\n",
      "Saved waveform_mal_2/epoch35_gen2_sample2.wav\n",
      "Saved waveform_mal_2/epoch35_gen2_sample3.wav\n",
      "Saved waveform_mal_2/epoch35_gen2_sample4.wav\n",
      "Saved waveform_mal_2/epoch35_gen2_sample5.wav\n",
      "Saved waveform_mal_2/epoch35_gen3_sample1.wav\n",
      "Saved waveform_mal_2/epoch35_gen3_sample2.wav\n",
      "Saved waveform_mal_2/epoch35_gen3_sample3.wav\n",
      "Saved waveform_mal_2/epoch35_gen3_sample4.wav\n",
      "Saved waveform_mal_2/epoch35_gen3_sample5.wav\n",
      "Saved waveform_mal_2/epoch35_gen4_sample1.wav\n",
      "Saved waveform_mal_2/epoch35_gen4_sample2.wav\n",
      "Saved waveform_mal_2/epoch35_gen4_sample3.wav\n",
      "Saved waveform_mal_2/epoch35_gen4_sample4.wav\n",
      "Saved waveform_mal_2/epoch35_gen4_sample5.wav\n",
      "Saved waveform_mal_2/epoch35_gen5_sample1.wav\n",
      "Saved waveform_mal_2/epoch35_gen5_sample2.wav\n",
      "Saved waveform_mal_2/epoch35_gen5_sample3.wav\n",
      "Saved waveform_mal_2/epoch35_gen5_sample4.wav\n",
      "Saved waveform_mal_2/epoch35_gen5_sample5.wav\n",
      "Checkpoint saved at epoch 34\n",
      "Training interrupted at epoch 35. Saving checkpoint...\n",
      "Checkpoint saved at epoch 35\n",
      "Exiting training early.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Generator(\n",
       "   (model): Sequential(\n",
       "     (0): Linear(in_features=100, out_features=128000, bias=True)\n",
       "     (1): LeakyReLU(negative_slope=0.2)\n",
       "     (2): Unflatten(dim=1, unflattened_size=(512, 250))\n",
       "     (3): ConvTranspose1d(512, 256, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "     (4): LeakyReLU(negative_slope=0.2)\n",
       "     (5): ConvTranspose1d(256, 128, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "     (6): LeakyReLU(negative_slope=0.2)\n",
       "     (7): ConvTranspose1d(128, 64, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "     (8): LeakyReLU(negative_slope=0.2)\n",
       "     (9): ConvTranspose1d(64, 1, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "     (10): Tanh()\n",
       "   )\n",
       " ),\n",
       " Generator(\n",
       "   (model): Sequential(\n",
       "     (0): Linear(in_features=100, out_features=128000, bias=True)\n",
       "     (1): LeakyReLU(negative_slope=0.2)\n",
       "     (2): Unflatten(dim=1, unflattened_size=(512, 250))\n",
       "     (3): ConvTranspose1d(512, 256, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "     (4): LeakyReLU(negative_slope=0.2)\n",
       "     (5): ConvTranspose1d(256, 128, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "     (6): LeakyReLU(negative_slope=0.2)\n",
       "     (7): ConvTranspose1d(128, 64, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "     (8): LeakyReLU(negative_slope=0.2)\n",
       "     (9): ConvTranspose1d(64, 1, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "     (10): Tanh()\n",
       "   )\n",
       " ),\n",
       " Generator(\n",
       "   (model): Sequential(\n",
       "     (0): Linear(in_features=100, out_features=128000, bias=True)\n",
       "     (1): LeakyReLU(negative_slope=0.2)\n",
       "     (2): Unflatten(dim=1, unflattened_size=(512, 250))\n",
       "     (3): ConvTranspose1d(512, 256, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "     (4): LeakyReLU(negative_slope=0.2)\n",
       "     (5): ConvTranspose1d(256, 128, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "     (6): LeakyReLU(negative_slope=0.2)\n",
       "     (7): ConvTranspose1d(128, 64, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "     (8): LeakyReLU(negative_slope=0.2)\n",
       "     (9): ConvTranspose1d(64, 1, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "     (10): Tanh()\n",
       "   )\n",
       " ),\n",
       " Generator(\n",
       "   (model): Sequential(\n",
       "     (0): Linear(in_features=100, out_features=128000, bias=True)\n",
       "     (1): LeakyReLU(negative_slope=0.2)\n",
       "     (2): Unflatten(dim=1, unflattened_size=(512, 250))\n",
       "     (3): ConvTranspose1d(512, 256, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "     (4): LeakyReLU(negative_slope=0.2)\n",
       "     (5): ConvTranspose1d(256, 128, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "     (6): LeakyReLU(negative_slope=0.2)\n",
       "     (7): ConvTranspose1d(128, 64, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "     (8): LeakyReLU(negative_slope=0.2)\n",
       "     (9): ConvTranspose1d(64, 1, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "     (10): Tanh()\n",
       "   )\n",
       " ),\n",
       " Generator(\n",
       "   (model): Sequential(\n",
       "     (0): Linear(in_features=100, out_features=128000, bias=True)\n",
       "     (1): LeakyReLU(negative_slope=0.2)\n",
       "     (2): Unflatten(dim=1, unflattened_size=(512, 250))\n",
       "     (3): ConvTranspose1d(512, 256, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "     (4): LeakyReLU(negative_slope=0.2)\n",
       "     (5): ConvTranspose1d(256, 128, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "     (6): LeakyReLU(negative_slope=0.2)\n",
       "     (7): ConvTranspose1d(128, 64, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "     (8): LeakyReLU(negative_slope=0.2)\n",
       "     (9): ConvTranspose1d(64, 1, kernel_size=(25,), stride=(4,), padding=(11,), output_padding=(1,))\n",
       "     (10): Tanh()\n",
       "   )\n",
       " )]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gan_with_pretrained_generators(\n",
    "    pretrained_generator,\n",
    "    num_epochs=200,\n",
    "    z_dim=100,\n",
    "    lr_gen=0.0002,\n",
    "    lr_disc=0.0002,\n",
    "    batch_size=24,\n",
    "    train_dataset=train_dataset,\n",
    "    num_generators=5,\n",
    "    seed=42,\n",
    "    audio_length=64000,\n",
    "    output_dir='waveform_mal_2',\n",
    "    checkpoint_dir='my_checkpoints',\n",
    "    resume=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
