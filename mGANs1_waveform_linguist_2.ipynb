{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  orthogonal loss \n",
    "This model work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from glob import glob\n",
    "from phonemizer import phonemize\n",
    "from torch.nn.utils import spectral_norm\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a FLAC audio file and resample it to the target sample rate\n",
    "def load_flac(file_path, target_sr=16000):\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
    "    max_val = np.max(np.abs(audio))\n",
    "    if max_val > 0:\n",
    "        audio = audio / max_val  # Normalize to [-1, 1]\n",
    "    return audio\n",
    "\n",
    "# Pad or trim audio to the target length\n",
    "def pad_or_trim(audio, target_length=64000):\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:target_length]\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the transcription for a given audio file from the corresponding .trans.txt file\n",
    "def get_transcription(file_path):\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "    base_name = os.path.basename(file_path)\n",
    "    file_id = os.path.splitext(base_name)[0]\n",
    "\n",
    "    # Locate the transcription file\n",
    "    transcription_file = None\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith(\".trans.txt\"):\n",
    "            transcription_file = os.path.join(dir_path, file)\n",
    "            break\n",
    "\n",
    "    if not transcription_file:\n",
    "        raise FileNotFoundError(f\"No transcription file found in {dir_path}\")\n",
    "\n",
    "    # Read the transcription file and find the transcription for the current audio file\n",
    "    with open(transcription_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\" \", 1)\n",
    "            if parts[0] == file_id:\n",
    "                transcription = parts[1]\n",
    "                return transcription\n",
    "\n",
    "    raise ValueError(f\"No transcription found for file {file_id}\")\n",
    "\n",
    "# Extract phonetic features from transcription\n",
    "def get_phonetic_features(transcription, max_length=100):\n",
    "    phonemes = phonemize(transcription, backend=\"espeak\", language=\"en-us\")\n",
    "    phoneme_to_id = {char: idx for idx, char in enumerate(sorted(set(phonemes)))}\n",
    "    phonetic_features = [phoneme_to_id[p] for p in phonemes]\n",
    "\n",
    "    # Convert to tensor and pad/truncate\n",
    "    phonetic_features = torch.tensor(phonetic_features, dtype=torch.float32)\n",
    "    if len(phonetic_features) < max_length:\n",
    "        phonetic_features = nn.functional.pad(phonetic_features, (0, max_length - len(phonetic_features)))\n",
    "    else:\n",
    "        phonetic_features = phonetic_features[:max_length]\n",
    "    return phonetic_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset and create a TensorDataset\n",
    "def preprocess_dataset(root_dir, target_sr=16000, target_length=64000, feature_length=100):\n",
    "    flac_files = glob(os.path.join(root_dir, '**', '*.flac'), recursive=True)\n",
    "    print(f\"Found {len(flac_files)} .flac files in {root_dir}.\")\n",
    "    if len(flac_files) == 0:\n",
    "        print(\"No .flac files found. Please check the root_dir path.\")\n",
    "    audio_dataset = []\n",
    "    feature_dataset = []\n",
    "    for file in flac_files:\n",
    "        try:\n",
    "            audio = load_flac(file, target_sr)\n",
    "            audio = pad_or_trim(audio, target_length)\n",
    "            transcription = get_transcription(file)\n",
    "            phonetic_features = get_phonetic_features(transcription, max_length=feature_length)\n",
    "            audio_dataset.append(audio)\n",
    "            feature_dataset.append(phonetic_features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "    audio_dataset = torch.tensor(audio_dataset, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
    "    feature_dataset = torch.stack(feature_dataset)  # Stack tensors\n",
    "    return TensorDataset(audio_dataset, feature_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a waveform to an audio file\n",
    "def save_waveform_to_audio(waveform, sample_rate, filename):\n",
    "    if isinstance(waveform, torch.Tensor):\n",
    "        waveform = waveform.detach().cpu().numpy()\n",
    "    waveform = np.squeeze(waveform)\n",
    "    max_val = np.max(np.abs(waveform))\n",
    "    if max_val > 0:\n",
    "        waveform = waveform / max_val\n",
    "    sf.write(filename, waveform, sample_rate)\n",
    "\n",
    "# Verify waveform-to-audio conversion using preprocessed dataset\n",
    "def verify_waveform_to_audio(root_dir, sample_rate=16000, target_length=64000, output_dir=\"verified_audio\"):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    dataset = preprocess_dataset(root_dir, sample_rate, target_length)\n",
    "    num_samples_to_verify = min(5, len(dataset))\n",
    "    for idx in range(num_samples_to_verify):\n",
    "        waveform = dataset[idx][0]  # Access audio data\n",
    "        filename = os.path.join(output_dir, f\"example_waveform_{idx+1}.wav\")\n",
    "        save_waveform_to_audio(waveform, sample_rate, filename)\n",
    "        print(f\"Waveform saved to {filename}\")\n",
    "        # Plot the waveform\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(waveform.numpy().squeeze())\n",
    "        plt.title(f\"Waveform {idx+1}\")\n",
    "        plt.xlabel(\"Sample Index\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate noise for the generator\n",
    "def generate_noise(batch_size, z_dim, device):\n",
    "    return torch.randn(batch_size, z_dim).to(device)\n",
    "\n",
    "# Orthogonal loss function\n",
    "def orthogonal_loss(feature1, feature2):\n",
    "    inner_product = torch.sum(feature1 * feature2, dim=1)\n",
    "    norm1 = torch.norm(feature1, dim=1)\n",
    "    norm2 = torch.norm(feature2, dim=1)\n",
    "    cosine_similarity = inner_product / (norm1 * norm2 + 1e-8)\n",
    "    return torch.mean(cosine_similarity**2)  # Minimize the cosine similarity to make vectors orthogonal\n",
    "\n",
    "# Compute gradient penalty for WGAN-GP\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, conditions, device):\n",
    "    batch_size = real_samples.size(0)\n",
    "    epsilon = torch.rand(batch_size, 1, 1, device=device)  # Shape: [batch_size, 1, 1]\n",
    "    epsilon = epsilon.expand_as(real_samples)  # Expand to match `real_samples` shape\n",
    "\n",
    "    # Interpolate between real and fake samples\n",
    "    interpolates = (epsilon * real_samples + (1 - epsilon) * fake_samples).requires_grad_(True)\n",
    "\n",
    "    # Pass interpolates, fake_samples, and conditions to the discriminator\n",
    "    real_outputs, fake_outputs = discriminator(interpolates, fake_samples, conditions)\n",
    "\n",
    "    # Compute gradient penalty for each output\n",
    "    gradient_penalties = []\n",
    "    for real_output in real_outputs:\n",
    "        grad_outputs = torch.ones_like(real_output, device=device)\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=real_output,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=grad_outputs,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "\n",
    "        # Reshape gradients and compute L2 norm\n",
    "        gradients = gradients.view(batch_size, -1)  # Flatten\n",
    "        gradient_norm = gradients.norm(2, dim=1)  # L2 norm\n",
    "        gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "        gradient_penalties.append(gradient_penalty)\n",
    "\n",
    "    # Average all gradient penalties\n",
    "    return sum(gradient_penalties) / len(gradient_penalties)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save_generated_waveforms(generators, z_dim, features, num_waveforms, device, epoch, sample_rate=16000, output_dir='generated_audio'):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for idx, gen in enumerate(generators):\n",
    "        gen.eval()\n",
    "        with torch.no_grad():\n",
    "            noise = generate_noise(num_waveforms, z_dim, device)\n",
    "            fake_waveforms = gen(features[:num_waveforms].to(device), noise).cpu()\n",
    "            \n",
    "            # Adjust num_waveforms to the actual size of fake_waveforms\n",
    "            num_available_waveforms = fake_waveforms.size(0)\n",
    "            if num_available_waveforms < num_waveforms:\n",
    "                print(f\"Warning: Requested {num_waveforms} waveforms, but generator produced {num_available_waveforms}\")\n",
    "                num_waveforms = num_available_waveforms\n",
    "\n",
    "            for i in range(num_waveforms):\n",
    "                waveform = fake_waveforms[i]\n",
    "                filename = f'epoch{epoch+1}_gen{idx+1}_sample{i+1}.wav'\n",
    "                filepath = os.path.join(output_dir, filename)\n",
    "                save_waveform_to_audio(waveform, sample_rate, filepath)\n",
    "                print(f\"Saved {filepath}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels=1, z_channels=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.z_channels = z_channels\n",
    "\n",
    "        self.preprocess = nn.Conv1d(1, 768, kernel_size=3, padding=1)\n",
    "\n",
    "        self.gblocks = nn.ModuleList([\n",
    "            GBlock(768, 768, z_channels, 5),  # Upsample by 4\n",
    "            GBlock(768, 768, z_channels, 4),  # Upsample by 4\n",
    "            GBlock(768, 384, z_channels, 4),  # Upsample by 4\n",
    "            GBlock(384, 384, z_channels, 4),  # Upsample by 2\n",
    "            GBlock(384, 192, z_channels, 2),  # Upsample by 2\n",
    "        ])\n",
    "        self.postprocess = nn.Sequential(\n",
    "            nn.Conv1d(192, 1, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, inputs, z):\n",
    "        # print(f\"Input shape: {inputs.shape}\")  # Debug input shape\n",
    "        inputs = self.preprocess(inputs)\n",
    "        outputs = inputs\n",
    "        for i, layer in enumerate(self.gblocks):\n",
    "            outputs = layer(outputs, z)\n",
    "            # print(f\"After GBlock {i}: {outputs.shape}\")  # Debug output shape\n",
    "        outputs = self.postprocess(outputs)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "class GBlock(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, z_channels, upsample_factor):\n",
    "        super(GBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.z_channels = z_channels\n",
    "        self.upsample_factor = upsample_factor\n",
    "\n",
    "        # GroupNorm for memory efficiency\n",
    "        self.condition_norm1 = nn.GroupNorm(32, in_channels)\n",
    "        self.first_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            UpsampleNet(in_channels, in_channels, upsample_factor),\n",
    "            nn.Conv1d(in_channels, hidden_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.condition_norm2 = nn.GroupNorm(32, hidden_channels)\n",
    "        self.second_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, dilation=1, padding=1)\n",
    "        )\n",
    "        self.residual1 = nn.Sequential(\n",
    "            UpsampleNet(in_channels, in_channels, upsample_factor),\n",
    "            nn.Conv1d(in_channels, hidden_channels, kernel_size=1)\n",
    "        )\n",
    "        self.condition_norm3 = nn.GroupNorm(32, hidden_channels)\n",
    "        self.third_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        )\n",
    "        self.condition_norm4 = nn.GroupNorm(32, hidden_channels)\n",
    "        self.fourth_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, condition, z):\n",
    "        def run_forward(inputs):\n",
    "            outputs = self.condition_norm1(inputs)\n",
    "            outputs = self.first_stack(outputs)\n",
    "            outputs = self.condition_norm2(outputs)\n",
    "            outputs = self.second_stack(outputs)\n",
    "            residual_outputs = self.residual1(inputs) + outputs\n",
    "            outputs = self.condition_norm3(residual_outputs)\n",
    "            outputs = self.third_stack(outputs)\n",
    "            outputs = self.condition_norm4(outputs)\n",
    "            outputs = self.fourth_stack(outputs)\n",
    "            outputs = outputs + residual_outputs\n",
    "            return outputs\n",
    "\n",
    "        # Gradient checkpointing to save memory\n",
    "        outputs = checkpoint.checkpoint(run_forward, condition)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, upsample_factor):\n",
    "        super(UpsampleNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.upsample_factor = upsample_factor\n",
    "\n",
    "        layer = nn.ConvTranspose1d(input_size, output_size, upsample_factor * 2,\n",
    "                                   upsample_factor, padding=upsample_factor // 2)\n",
    "        nn.init.orthogonal_(layer.weight)\n",
    "        self.layer = spectral_norm(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.layer(inputs)\n",
    "        outputs = outputs[:, :, : inputs.size(-1) * self.upsample_factor]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalBatchNorm1d(nn.Module):\n",
    "    \"\"\"Conditional Batch Normalization\"\"\"\n",
    "    def __init__(self, num_features, z_channels=128):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.z_channels = z_channels\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features, affine=False)\n",
    "        self.layer = spectral_norm(nn.Linear(z_channels, num_features * 2))\n",
    "        self.layer.weight.data.normal_(1, 0.02)\n",
    "        self.layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, inputs, noise):\n",
    "        outputs = self.batch_norm(inputs)\n",
    "        gamma, beta = self.layer(noise).chunk(2, 1)\n",
    "        gamma = gamma.view(-1, self.num_features, 1)\n",
    "        beta = beta.view(-1, self.num_features, 1)\n",
    "        outputs = gamma * outputs + beta\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator architecture using Conv1d layers\n",
    "class Multiple_Random_Window_Discriminators(nn.Module):\n",
    "    def __init__(self, lc_channels, window_size=(2, 4, 8, 16, 30), upsample_factor=120):\n",
    "        super(Multiple_Random_Window_Discriminators, self).__init__()\n",
    "        self.lc_channels = lc_channels\n",
    "        self.window_size = window_size\n",
    "        self.upsample_factor = upsample_factor\n",
    "\n",
    "        self.udiscriminators = nn.ModuleList([\n",
    "            UnConditionalDBlocks(in_channels=1, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=2, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=4, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=8, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=15, factors=(2, 2), out_channels=(128, 256)),\n",
    "        ])\n",
    "\n",
    "        self.discriminators = nn.ModuleList([\n",
    "            ConditionalDBlocks(in_channels=1, lc_channels=lc_channels,\n",
    "                               factors=(5, 3, 2, 2, 2), out_channels=(128, 128, 256, 256)),\n",
    "            ConditionalDBlocks(in_channels=2, lc_channels=lc_channels,\n",
    "                               factors=(5, 3, 2, 2), out_channels=(128, 256, 256)),\n",
    "            ConditionalDBlocks(in_channels=4, lc_channels=lc_channels,\n",
    "                               factors=(5, 3, 2), out_channels=(128, 256)),\n",
    "            ConditionalDBlocks(in_channels=8, lc_channels=lc_channels,\n",
    "                               factors=(5, 3), out_channels=(256,)),\n",
    "            ConditionalDBlocks(in_channels=15, lc_channels=lc_channels,\n",
    "                               factors=(2, 2, 2), out_channels=(128, 256)),\n",
    "        ])\n",
    "\n",
    "    def forward(self, real_samples, fake_samples, conditions):\n",
    "        real_outputs, fake_outputs = [], []\n",
    "\n",
    "        # Unconditional discriminator\n",
    "        for (size, layer) in zip(self.window_size, self.udiscriminators):\n",
    "            size = size * self.upsample_factor\n",
    "            if real_samples.size(-1) < size:\n",
    "                raise ValueError(f\"Window size {size} is too large for input with length {real_samples.size(-1)}\")\n",
    "\n",
    "            index = np.random.randint(0, real_samples.size(-1) - size + 1)\n",
    "            #print(f\"Unconditional index: {index}, size: {size}, slice: {index}:{index + size}\")\n",
    "\n",
    "            real_slice = real_samples[:, :, index: index + size]\n",
    "            fake_slice = fake_samples[:, :, index: index + size]\n",
    "\n",
    "            if real_slice.size(-1) == 0 or fake_slice.size(-1) == 0:\n",
    "                raise ValueError(f\"Generated slice has zero length: real_slice.shape={real_slice.shape}, fake_slice.shape={fake_slice.shape}\")\n",
    "\n",
    "            real_output = layer(real_slice)\n",
    "            fake_output = layer(fake_slice)\n",
    "\n",
    "            real_outputs.append(real_output)\n",
    "            fake_outputs.append(fake_output)\n",
    "\n",
    "        # Conditional discriminator\n",
    "        for (size, layer) in zip(self.window_size, self.discriminators):\n",
    "            lc_index = np.random.randint(0, conditions.size(-1) - size + 1)\n",
    "            sample_index = lc_index * self.upsample_factor\n",
    "\n",
    "            if real_samples.size(-1) < (lc_index + size) * self.upsample_factor:\n",
    "                raise ValueError(f\"Window size exceeds input size for conditional discriminator.\")\n",
    "\n",
    "            real_x = real_samples[:, :, sample_index: (lc_index + size) * self.upsample_factor]\n",
    "            fake_x = fake_samples[:, :, sample_index: (lc_index + size) * self.upsample_factor]\n",
    "            lc = conditions[:, :, lc_index: lc_index + size]\n",
    "\n",
    "            if real_x.size(-1) == 0 or fake_x.size(-1) == 0:\n",
    "                raise ValueError(f\"Generated slice has zero length: real_x.shape={real_x.shape}, fake_x.shape={fake_x.shape}\")\n",
    "\n",
    "            real_output = layer(real_x, lc)\n",
    "            fake_output = layer(fake_x, lc)\n",
    "\n",
    "            real_outputs.append(real_output)\n",
    "            fake_outputs.append(fake_output)\n",
    "\n",
    "        return real_outputs, fake_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample_factor):\n",
    "        super(DBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.AvgPool1d(downsample_factor, stride=downsample_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        )\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1),\n",
    "            nn.AvgPool1d(downsample_factor, stride=downsample_factor)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.layers(inputs) + self.residual(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondDBlock(nn.Module):\n",
    "    def __init__(self, in_channels, lc_channels, upsample_factor):\n",
    "        super(CondDBlock, self).__init__()\n",
    "        self.lc_conv1d = nn.Conv1d(lc_channels, in_channels, kernel_size=1)\n",
    "        self.start = nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.end = nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.residual = nn.Conv1d(in_channels, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, inputs, conditions):\n",
    "        # Ensure conditions are processed to match inputs\n",
    "        conditions = self.lc_conv1d(conditions)\n",
    "        # print(f\"Conditions shape after lc_conv1d: {conditions.shape}\")  # Debug\n",
    "        outputs = self.start(inputs) + conditions\n",
    "        outputs = self.end(outputs)\n",
    "        residual_outputs = self.residual(inputs)\n",
    "        return outputs + residual_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalDBlocks(nn.Module):\n",
    "    def __init__(self, in_channels, lc_channels, factors=(2, 2, 2), out_channels=(128, 256)):\n",
    "        super(ConditionalDBlocks, self).__init__()\n",
    "        assert len(factors) == len(out_channels) + 1\n",
    "        self.in_channels = in_channels\n",
    "        self.lc_channels = lc_channels\n",
    "        self.factors = factors\n",
    "        self.out_channels = out_channels\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(DBlock(in_channels, 64, 1))\n",
    "        in_channels = 64\n",
    "        for i, channel in enumerate(out_channels):\n",
    "            self.layers.append(DBlock(in_channels, channel, factors[i]))\n",
    "            in_channels = channel\n",
    "        self.cond_layer = CondDBlock(in_channels, lc_channels, factors[-1])\n",
    "        \n",
    "        # New adjustment layer to match post_process input\n",
    "        self.adjust_channels = nn.Conv1d(in_channels, 512, kernel_size=1)\n",
    "\n",
    "        self.post_process = nn.ModuleList([\n",
    "            DBlock(512, 512, 1),\n",
    "            DBlock(512, 512, 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, inputs, conditions):\n",
    "        batch_size = inputs.size(0)\n",
    "        # print(f\"Initial inputs shape: {inputs.shape}, conditions shape: {conditions.shape}\")  # Debug\n",
    "\n",
    "        # Reshape inputs\n",
    "        outputs = inputs.view(batch_size, self.in_channels, -1)\n",
    "        # print(f\"After view: {outputs.shape}\")  # Debug\n",
    "\n",
    "        # Process through layers\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            outputs = layer(outputs)\n",
    "            # print(f\"After layer {i}: {outputs.shape}\")  # Debug\n",
    "\n",
    "        # Adjust conditions to match temporal dimension\n",
    "        conditions = F.adaptive_avg_pool1d(conditions, output_size=1)  # Reduce temporal dimension to 1\n",
    "        # print(f\"Conditions shape after pooling: {conditions.shape}\")  # Debug\n",
    "\n",
    "        conditions = conditions.expand(-1, self.lc_channels, outputs.size(-1))  # Expand to match outputs\n",
    "        # print(f\"Conditions shape after expand: {conditions.shape}\")  # Debug\n",
    "\n",
    "        # Apply cond_layer\n",
    "        outputs = self.cond_layer(outputs, conditions)\n",
    "        #print(f\"After cond_layer: {outputs.shape}\")  # Debug\n",
    "\n",
    "        # Adjust channels\n",
    "        outputs = self.adjust_channels(outputs)\n",
    "        #print(f\"After adjust_channels: {outputs.shape}\")  # Debug\n",
    "\n",
    "        # Post-process\n",
    "        for i, layer in enumerate(self.post_process):\n",
    "            outputs = layer(outputs)\n",
    "            #print(f\"After post_process layer {i}: {outputs.shape}\")  # Debug\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnConditionalDBlocks(nn.Module):\n",
    "    def __init__(self, in_channels, factors=(5, 3), out_channels=(128, 256)):\n",
    "        super(UnConditionalDBlocks, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.factors = factors\n",
    "        self.out_channels = out_channels\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(DBlock(in_channels, 64, 1))\n",
    "        in_channels = 64\n",
    "        for (i, factor) in enumerate(factors):\n",
    "            self.layers.append(DBlock(in_channels, out_channels[i], factor))\n",
    "            in_channels = out_channels[i]\n",
    "        self.layers.append(DBlock(in_channels, in_channels, 1))\n",
    "        self.layers.append(DBlock(in_channels, in_channels, 1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size()[0]\n",
    "        outputs = inputs.view(batch_size, self.in_channels, -1)\n",
    "        for layer in self.layers:\n",
    "            outputs = layer(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, audio_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.audio_length = audio_length\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=5, stride=2, padding=2),  # Downsample\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=2),  # Downsample\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2),  # Downsample\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool1d(1)  # Reduce to (batch_size, 128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)  # Shape: (batch_size, 128)\n",
    "\n",
    "\n",
    "def orthogonal_loss(features1, features2):\n",
    "    \"\"\"Compute orthogonal loss between two feature sets.\"\"\"\n",
    "    return torch.mean(torch.sum(features1 * features2, dim=1)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_with_pretrained_generators(\n",
    "    pretrained_generator, num_epochs, z_dim, lr_gen, lr_disc, batch_size, train_dataset,\n",
    "    num_generators, seed, audio_length, output_dir, lambda_gp=10, lambda_ortho=0.1, num_critic=5,\n",
    "    checkpoint_path=\"checkpoint.pth\", resume=False\n",
    "):\n",
    "    set_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize multiple generators with pretrained weights\n",
    "    generators = []\n",
    "    for _ in range(num_generators):\n",
    "        gen = Generator(z_dim, audio_length).to(device)\n",
    "        gen.load_state_dict(pretrained_generator.state_dict())\n",
    "        generators.append(gen)\n",
    "\n",
    "    # Initialize Discriminator and Encoder\n",
    "    discriminator = Multiple_Random_Window_Discriminators(lc_channels=1).to(device)\n",
    "    encoder = Encoder(audio_length).to(device)\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_gens = [optim.Adam(gen.parameters(), lr=lr_gen, betas=(0.5, 0.9)) for gen in generators]\n",
    "    optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "    optimizer_encoder = optim.Adam(encoder.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "    # Mixed precision scalers\n",
    "    scaler_gens = [GradScaler() for _ in range(num_generators)]\n",
    "    scaler_disc = GradScaler()\n",
    "    scaler_encoder = GradScaler()\n",
    "\n",
    "    # Load checkpoint if resuming\n",
    "    start_epoch = 0\n",
    "    if resume and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        for idx, gen in enumerate(generators):\n",
    "            gen.load_state_dict(checkpoint['generator_state_dicts'][idx])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "        for idx, opt_gen in enumerate(optimizer_gens):\n",
    "            opt_gen.load_state_dict(checkpoint['optimizer_gen_state_dicts'][idx])\n",
    "        optimizer_disc.load_state_dict(checkpoint['optimizer_disc_state_dict'])\n",
    "        optimizer_encoder.load_state_dict(checkpoint['optimizer_encoder_state_dict'])\n",
    "        print(f\"Resumed training from epoch {start_epoch}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        for batch_idx, (real_audio, features) in enumerate(train_loader):\n",
    "            real_audio = real_audio.to(device)\n",
    "            features = features.unsqueeze(1).to(device)\n",
    "            batch_size = real_audio.size(0)\n",
    "\n",
    "            # Train Discriminator\n",
    "            for _ in range(num_critic):\n",
    "                optimizer_disc.zero_grad()\n",
    "                noises = [generate_noise(batch_size, z_dim, device) for _ in range(num_generators)]\n",
    "\n",
    "                with autocast(device_type='cuda'):\n",
    "                    fakes = [gen(features, noises[idx]).detach() for idx, gen in enumerate(generators)]\n",
    "                    real_outputs, fake_outputs = discriminator(real_audio, torch.stack(fakes), features)\n",
    "                    loss_disc = sum(torch.mean(fake) - torch.mean(real) for real, fake in zip(real_outputs, fake_outputs))\n",
    "                    gradient_penalty = compute_gradient_penalty(discriminator, real_audio, fakes[0], features, device)\n",
    "                    loss_disc += lambda_gp * gradient_penalty\n",
    "\n",
    "                scaler_disc.scale(loss_disc).backward()\n",
    "                scaler_disc.step(optimizer_disc)\n",
    "                scaler_disc.update()\n",
    "\n",
    "            # Train Generators and Encoder\n",
    "            for idx, gen in enumerate(generators):\n",
    "                optimizer_gens[idx].zero_grad()\n",
    "                optimizer_encoder.zero_grad()\n",
    "\n",
    "                noise = generate_noise(batch_size, z_dim, device)\n",
    "                with autocast(device_type='cuda'):\n",
    "                    fake = gen(features, noise)\n",
    "                    fake_outputs = discriminator(fake, fake, features)[1]\n",
    "                    loss_gen = -torch.mean(fake_outputs)\n",
    "\n",
    "                    # Orthogonal loss\n",
    "                    gen_feature = encoder(fake)\n",
    "                    ortho_loss = 0\n",
    "                    for other_idx, other_gen in enumerate(generators):\n",
    "                        if idx != other_idx:\n",
    "                            other_noise = generate_noise(batch_size, z_dim, device)\n",
    "                            other_fake = other_gen(features, other_noise)\n",
    "                            other_feature = encoder(other_fake)\n",
    "                            ortho_loss += orthogonal_loss(gen_feature, other_feature)\n",
    "                    ortho_loss /= (num_generators - 1)\n",
    "                    total_loss_gen = loss_gen + lambda_ortho * ortho_loss\n",
    "\n",
    "                scaler_gens[idx].scale(total_loss_gen).backward()\n",
    "                scaler_gens[idx].step(optimizer_gens[idx])\n",
    "                scaler_gens[idx].update()\n",
    "                scaler_encoder.step(optimizer_encoder)\n",
    "                scaler_encoder.update()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {loss_disc.item():.4f}, Loss G: {loss_gen.item():.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'generator_state_dicts': [gen.state_dict() for gen in generators],\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'encoder_state_dict': encoder.state_dict(),\n",
    "            'optimizer_gen_state_dicts': [opt.state_dict() for opt in optimizer_gens],\n",
    "            'optimizer_disc_state_dict': optimizer_disc.state_dict(),\n",
    "            'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensors_to_match(tensor_list):\n",
    "    # Find the maximum size in each dimension\n",
    "    max_shape = list(max(tensor.size(dim) for tensor in tensor_list) for dim in range(tensor_list[0].dim()))\n",
    "\n",
    "    padded_tensors = []\n",
    "    for tensor in tensor_list:\n",
    "        # Calculate the padding needed for each dimension\n",
    "        pad = []\n",
    "        for dim, max_dim in enumerate(max_shape[::-1]):\n",
    "            pad.extend([0, max_dim - tensor.size(dim)])\n",
    "        pad = pad[::-1]  # Reverse padding list\n",
    "        # Apply padding\n",
    "        padded_tensor = torch.nn.functional.pad(tensor, pad)\n",
    "        padded_tensors.append(padded_tensor)\n",
    "    \n",
    "    return torch.stack(padded_tensors, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "def pretrain_single_generator(\n",
    "    num_epochs, z_dim, lr_gen, lr_disc, batch_size, seed,\n",
    "    audio_length, output_dir, train_dataset, checkpoint_path=\"checkpoint.pth\",\n",
    "    resume=False\n",
    "):\n",
    "    set_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define models and move to device\n",
    "    generator = Generator(in_channels=train_dataset[0][1].shape[0], z_channels=z_dim).to(device)\n",
    "    discriminator = Multiple_Random_Window_Discriminators(lc_channels=train_dataset[0][1].shape[0]).to(device)\n",
    "\n",
    "    optimizer_gen = optim.Adam(generator.parameters(), lr=lr_gen, betas=(0.5, 0.9))\n",
    "    optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "    # Mixed precision scaler\n",
    "    scaler_gen = GradScaler()\n",
    "    scaler_disc = GradScaler()\n",
    "\n",
    "    start_epoch = 0\n",
    "    if resume and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        optimizer_gen.load_state_dict(checkpoint['optimizer_gen_state_dict'])\n",
    "        optimizer_disc.load_state_dict(checkpoint['optimizer_disc_state_dict'])\n",
    "        scaler_gen.load_state_dict(checkpoint['scaler_gen'])\n",
    "        scaler_disc.load_state_dict(checkpoint['scaler_disc'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        for batch_idx, (real_audio, features) in enumerate(train_loader):\n",
    "            real_audio = real_audio.to(device)\n",
    "            features = features.unsqueeze(1).to(device)\n",
    "            batch_size = real_audio.size(0)\n",
    "\n",
    "            # Train Discriminator\n",
    "            for _ in range(5):\n",
    "                optimizer_disc.zero_grad()\n",
    "                noise = generate_noise(batch_size, z_dim, device)\n",
    "\n",
    "                with autocast(device_type='cuda'):\n",
    "                    # Detach fake_audio to avoid grad on generator when training disc\n",
    "                    fake_audio = generator(features, noise).detach()\n",
    "                    real_outputs, fake_outputs = discriminator(real_audio, fake_audio, features)\n",
    "                    loss_disc = sum(torch.mean(fake) - torch.mean(real) for real, fake in zip(real_outputs, fake_outputs))\n",
    "                    gradient_penalty = compute_gradient_penalty(discriminator, real_audio, fake_audio, features, device)\n",
    "                    loss_disc += 10 * gradient_penalty\n",
    "\n",
    "                scaler_disc.scale(loss_disc).backward()\n",
    "                scaler_disc.step(optimizer_disc)\n",
    "                scaler_disc.update()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_gen.zero_grad()\n",
    "            noise = generate_noise(batch_size, z_dim, device)\n",
    "\n",
    "            with autocast(device_type='cuda'):\n",
    "                fake_audio = generator(features, noise)\n",
    "                fake_outputs = discriminator(fake_audio, fake_audio, features)[1]\n",
    "                # Adjust if needed, e.g. if fake_outputs is a list of tensors\n",
    "                # Ensure pad_tensors_to_match is defined and imports are correct\n",
    "                fake_outputs = pad_tensors_to_match(fake_outputs)\n",
    "                loss_gen = -torch.mean(fake_outputs)\n",
    "\n",
    "            scaler_gen.scale(loss_gen).backward()\n",
    "            scaler_gen.step(optimizer_gen)\n",
    "            scaler_gen.update()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {loss_disc.item():.4f}, Loss G: {loss_gen.item():.4f}\")\n",
    "\n",
    "        # Save model output\n",
    "        visualize_and_save_generated_waveforms(\n",
    "            [generator], z_dim, features, num_waveforms=5, device=device, epoch=epoch, sample_rate=16000, output_dir=output_dir\n",
    "        )\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch+1,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_gen_state_dict': optimizer_gen.state_dict(),\n",
    "            'optimizer_disc_state_dict': optimizer_disc.state_dict(),\n",
    "            'scaler_gen': scaler_gen.state_dict(),\n",
    "            'scaler_disc': scaler_disc.state_dict(),\n",
    "            'z_dim': z_dim,\n",
    "            'lr_gen': lr_gen,\n",
    "            'lr_disc': lr_disc,\n",
    "            'batch_size': batch_size,\n",
    "            'seed': seed,\n",
    "            'audio_length': audio_length,\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "        torch.save(generator.state_dict(), os.path.join(output_dir, f\"pretrained_generator_epoch{epoch+1}.pth\"))\n",
    "\n",
    "    print(\"Pretraining complete.\")\n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pretrain_single_generator(num_epochs, z_dim, lr_gen, lr_disc, batch_size, seed, audio_length, output_dir, train_dataset):\n",
    "#     set_seed(seed)\n",
    "#     # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     # # Define the single generator and discriminator\n",
    "#     # generator = Generator(in_channels=1, z_channels=z_dim).to(device)\n",
    "\n",
    "#     # discriminator = Multiple_Random_Window_Discriminators(lc_channels=1)\n",
    "\n",
    "# # In CondDBlock (and related blocks), ensure lc_conv1d is defined as:\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "#     # Move models to device\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     generator = Generator(in_channels=train_dataset[0][1].shape[0], z_channels=z_dim).to(device)\n",
    "#     discriminator = Multiple_Random_Window_Discriminators(lc_channels=train_dataset[0][1].shape[0]).to(device)\n",
    "#     optimizer_gen = optim.Adam(generator.parameters(), lr=lr_gen, betas=(0.5, 0.9))\n",
    "#     optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "# # Training loop\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         torch.cuda.empty_cache()\n",
    "#         for batch_idx, (real_audio, features) in enumerate(train_loader):\n",
    "#             real_audio = real_audio.to(device)\n",
    "#             features = features.unsqueeze(1).to(device)\n",
    "#             print(f\"real_audio.shape: {real_audio.shape}\")\n",
    "#             print(f\"features.shape: {features.shape}\")\n",
    "            \n",
    "\n",
    "\n",
    "#             batch_size = real_audio.size(0)\n",
    "\n",
    "#             # Train Discriminator\n",
    "#             for _ in range(5):\n",
    "#                 optimizer_disc.zero_grad()\n",
    "#                 noise = generate_noise(batch_size, z_dim, device)\n",
    "#                 fake_audio = generator(features, noise).detach()\n",
    "#                 real_outputs, fake_outputs = discriminator(real_audio, fake_audio, features)\n",
    "#                 loss_disc = sum([torch.mean(fake) - torch.mean(real) for real, fake in zip(real_outputs, fake_outputs)])\n",
    "#                 gradient_penalty = compute_gradient_penalty(discriminator, real_audio, fake_audio, features, device)\n",
    "#                 loss_disc += 10 * gradient_penalty\n",
    "#                 loss_disc.backward()\n",
    "#                 optimizer_disc.step()\n",
    "\n",
    "#             # Train Generator\n",
    "#             optimizer_gen.zero_grad()\n",
    "#             noise = generate_noise(batch_size, z_dim, device)\n",
    "#             fake_audio = generator(features, noise)\n",
    "#             print(f\"fake_audio.shape: {fake_audio.shape}\")\n",
    "#             fake_outputs = discriminator(fake_audio, fake_audio, features)[1]\n",
    "#             fake_outputs = pad_tensors_to_match(fake_outputs)  # Ensure uniform tensor size\n",
    "#             loss_gen = -torch.mean(fake_outputs)\n",
    "#             loss_gen.backward()\n",
    "#             optimizer_gen.step()\n",
    "\n",
    "#         print(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {loss_disc.item():.4f}, Loss G: {loss_gen.item():.4f}\")\n",
    "\n",
    "#         visualize_and_save_generated_waveforms(\n",
    "#             [generator], z_dim, features, num_waveforms=5, device=device, epoch=epoch, sample_rate=16000, output_dir=output_dir\n",
    "#         )\n",
    "#         torch.save(generator.state_dict(), os.path.join(output_dir, f\"pretrained_generator_epoch{epoch+1}.pth\"))\n",
    "\n",
    "#     print(\"Pretraining complete.\")\n",
    "#     return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "audio_length = 64000\n",
    "z_dim = 128\n",
    "lr_gen = 0.0002\n",
    "lr_disc = 0.0002\n",
    "batch_size = 4\n",
    "num_epochs = 50\n",
    "root_dir = \"./data\"\n",
    "sample_rate = 16000\n",
    "num_generators = 5\n",
    "output_dir = 'generated_audio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2703 .flac files in ./data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33822/206904885.py:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  audio_dataset = torch.tensor(audio_dataset, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n"
     ]
    }
   ],
   "source": [
    "train_dataset = preprocess_dataset(root_dir, target_sr=sample_rate, target_length=audio_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU memory cache.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/tameszaza/My Passport/ml/.venv/lib/python3.12/site-packages/torch/__init__.py:1021: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(obj, torch.Tensor)\n",
      "/tmp/ipykernel_33822/4245394891.py:15: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final GPU cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_all_memory():\n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()  # Clear the cache\n",
    "        torch.cuda.synchronize()  # Ensure all CUDA operations are finished\n",
    "        print(\"Cleared GPU memory cache.\")\n",
    "\n",
    "    # Delete all tensors\n",
    "    gc.collect()  # Run garbage collection to free up Python memory\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
    "                del obj\n",
    "        except Exception as e:\n",
    "            pass  # Skip over objects that can't be deleted\n",
    "\n",
    "    # Re-run garbage collection and clear the cache again\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Final GPU cleanup complete.\")\n",
    "\n",
    "# Call the function to clear all memory\n",
    "clear_all_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/tameszaza/My Passport/ml/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Loss D: 0.0220, Loss G: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/tameszaza/My Passport/ml/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Requested 5 waveforms, but generator produced 3\n",
      "Saved waveform_pre/epoch1_gen1_sample1.wav\n",
      "Saved waveform_pre/epoch1_gen1_sample2.wav\n",
      "Saved waveform_pre/epoch1_gen1_sample3.wav\n",
      "Checkpoint saved at gan_single_check.pth\n",
      "Epoch [2/20] Loss D: 0.0197, Loss G: -0.0023\n",
      "Warning: Requested 5 waveforms, but generator produced 3\n",
      "Saved waveform_pre/epoch2_gen1_sample1.wav\n",
      "Saved waveform_pre/epoch2_gen1_sample2.wav\n",
      "Saved waveform_pre/epoch2_gen1_sample3.wav\n",
      "Checkpoint saved at gan_single_check.pth\n",
      "Epoch [3/20] Loss D: 0.0056, Loss G: -0.0001\n",
      "Warning: Requested 5 waveforms, but generator produced 3\n",
      "Saved waveform_pre/epoch3_gen1_sample1.wav\n",
      "Saved waveform_pre/epoch3_gen1_sample2.wav\n",
      "Saved waveform_pre/epoch3_gen1_sample3.wav\n",
      "Checkpoint saved at gan_single_check.pth\n",
      "Epoch [4/20] Loss D: 0.0110, Loss G: -0.0026\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 5] Input/output error: 'waveform_pre'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pretrained_generator \u001b[38;5;241m=\u001b[39m \u001b[43mpretrain_single_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mz_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_gen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_disc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_disc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43maudio_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwaveform_pre\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgan_single_check.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 84\u001b[0m, in \u001b[0;36mpretrain_single_generator\u001b[0;34m(num_epochs, z_dim, lr_gen, lr_disc, batch_size, seed, audio_length, output_dir, train_dataset, checkpoint_path, resume)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Loss D: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_disc\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss G: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_gen\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Save model output\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m \u001b[43mvisualize_and_save_generated_waveforms\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_waveforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Save checkpoint\u001b[39;00m\n\u001b[1;32m     89\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerator_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: generator\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_length\u001b[39m\u001b[38;5;124m'\u001b[39m: audio_length,\n\u001b[1;32m    103\u001b[0m }\n",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m, in \u001b[0;36mvisualize_and_save_generated_waveforms\u001b[0;34m(generators, z_dim, features, num_waveforms, device, epoch, sample_rate, output_dir)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_and_save_generated_waveforms\u001b[39m(generators, z_dim, features, num_waveforms, device, epoch, sample_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_audio\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_dir):\n\u001b[0;32m----> 3\u001b[0m         \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, gen \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(generators):\n\u001b[1;32m      5\u001b[0m         gen\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error: 'waveform_pre'"
     ]
    }
   ],
   "source": [
    "pretrained_generator = pretrain_single_generator(\n",
    "        num_epochs=20,\n",
    "        z_dim=z_dim,\n",
    "        lr_gen=lr_gen,\n",
    "        lr_disc=lr_disc,\n",
    "        batch_size=batch_size,\n",
    "        seed=42,\n",
    "        audio_length=audio_length,\n",
    "        output_dir='waveform_pre',\n",
    "        train_dataset=train_dataset,\n",
    "        checkpoint_path=\"gan_single_check.pth\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan_with_pretrained_generators(\n",
    "        pretrained_generator,\n",
    "        num_epochs=num_epochs,\n",
    "        z_dim=z_dim,\n",
    "        lr_gen=lr_gen,\n",
    "        lr_disc=lr_disc,\n",
    "        batch_size=batch_size,\n",
    "        train_dataset=train_dataset,\n",
    "        num_generators=num_generators,\n",
    "        seed=42,\n",
    "        audio_length=audio_length,\n",
    "        output_dir=output_dir,\n",
    "        checkpoint_dir='my_checkpoints',\n",
    "        resume=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
