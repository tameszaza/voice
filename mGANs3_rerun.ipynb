{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 6789\n",
    "torch.manual_seed(manualSeed)\n",
    "np.random.seed(manualSeed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "# Generator shared layers\n",
    "class GeneratorSharedLayers(nn.Module):\n",
    "    def __init__(self, ngf, nc):\n",
    "        super(GeneratorSharedLayers, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # First upsampling\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # Second upsampling\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # Third upsampling\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # Output layer\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Generator with unique input layer and shared layers\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc, shared_layers):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngf = ngf  # Store ngf as an instance variable\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(nz, ngf * 8 * 4 * 4),\n",
    "            nn.BatchNorm1d(ngf * 8 * 4 * 4),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.shared_layers = shared_layers\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.input_layer(input)\n",
    "        x = x.view(-1, self.ngf * 8, 4, 4)  # Use self.ngf here\n",
    "        x = self.shared_layers(x)\n",
    "        return x\n",
    "\n",
    "# Discriminator and Classifier shared layers\n",
    "class DiscriminatorSharedLayers(nn.Module):\n",
    "    def __init__(self, ndf, nc):\n",
    "        super(DiscriminatorSharedLayers, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input layer\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Hidden layer\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Hidden layer\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Hidden layer\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Discriminator with shared layers and unique output layers\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ndf, nc, shared_layers, num_gens):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ndf = ndf  # Store ndf as an instance variable\n",
    "        self.shared_layers = shared_layers\n",
    "        self.output_bin = nn.Sequential(\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.output_mul = nn.Sequential(\n",
    "            nn.Conv2d(ndf * 8, num_gens, 4, 1, 0, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.shared_layers(input)\n",
    "        output_bin = self.output_bin(x).view(-1, 1).squeeze(1)\n",
    "        output_mul = self.output_mul(x).squeeze()\n",
    "        return output_bin, output_mul\n",
    "\n",
    "# MGAN class encapsulating the training loop\n",
    "class MGAN:\n",
    "    def __init__(self,\n",
    "                 num_z=100,\n",
    "                 beta=0.5,\n",
    "                 num_gens=4,\n",
    "                 batch_size=128,\n",
    "                 z_prior=\"gaussian\",\n",
    "                 learning_rate=0.0002,\n",
    "                 num_epochs=50,\n",
    "                 img_size=(64, 64, 1),\n",
    "                 num_gen_feature_maps=64,\n",
    "                 num_dis_feature_maps=64,\n",
    "                 sample_dir=\"samples\",\n",
    "                 device='cpu'):\n",
    "        self.beta = beta\n",
    "        self.num_z = num_z\n",
    "        self.num_gens = num_gens\n",
    "        self.batch_size = batch_size\n",
    "        self.z_prior = z_prior\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.img_size = img_size\n",
    "        self.ngf = num_gen_feature_maps\n",
    "        self.ndf = num_dis_feature_maps\n",
    "        self.sample_dir = sample_dir\n",
    "        self.device = device\n",
    "\n",
    "        self.history = {'d_loss': [], 'g_loss': []}\n",
    "\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Shared layers for Generators\n",
    "        self.shared_gen_layers = GeneratorSharedLayers(self.ngf, self.img_size[2]).to(self.device)\n",
    "        # List of Generators\n",
    "        self.generators = nn.ModuleList([\n",
    "            Generator(self.num_z, self.ngf, self.img_size[2], self.shared_gen_layers).to(self.device)\n",
    "            for _ in range(self.num_gens)\n",
    "        ])\n",
    "\n",
    "        # Shared layers for Discriminator\n",
    "        self.shared_dis_layers = DiscriminatorSharedLayers(self.ndf, self.img_size[2]).to(self.device)\n",
    "        # Discriminator\n",
    "        self.discriminator = Discriminator(self.ndf, self.img_size[2], self.shared_dis_layers, self.num_gens).to(self.device)\n",
    "\n",
    "        # Optimizers\n",
    "        # Combine parameters for shared layers and unique layers\n",
    "        self.optimizerD = optim.Adam(\n",
    "            list(self.discriminator.parameters()) + list(self.shared_dis_layers.parameters()),\n",
    "            lr=self.learning_rate, betas=(0.5, 0.999)\n",
    "        )\n",
    "        all_params = list(self.discriminator.parameters()) + list(self.shared_dis_layers.parameters())\n",
    "        print(f\"Unique Parameters: {len(set(id(p) for p in all_params))}\")\n",
    "        print(f\"Total Parameters: {len(all_params)}\")\n",
    "\n",
    "\n",
    "        # Generators have unique input layers but shared layers\n",
    "        gen_params = []\n",
    "        for gen in self.generators:\n",
    "            gen_params += list(gen.input_layer.parameters())\n",
    "        # Add shared generator layers\n",
    "        gen_params += list(self.shared_gen_layers.parameters())\n",
    "        self.optimizerG = optim.Adam(gen_params, lr=self.learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "        # Loss functions\n",
    "        self.criterion_bin = nn.BCELoss()\n",
    "        self.criterion_mul = nn.CrossEntropyLoss()\n",
    "\n",
    "    def fit(self, trainloader):\n",
    "        fixed_noise = self._sample_z(self.num_gens * 16).to(self.device)\n",
    "\n",
    "        real_label = 1.0  # Ensure this is a float\n",
    "        fake_label = 0.0  # Ensure this is a float\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                ############################\n",
    "                # (1) Update D network\n",
    "                ###########################\n",
    "                self.discriminator.zero_grad()\n",
    "                real_images = data[0].to(self.device)\n",
    "                b_size = real_images.size(0)\n",
    "                label = torch.full((b_size,), real_label, device=self.device, dtype=torch.float)\n",
    "\n",
    "                # Forward pass real batch through D\n",
    "                output_bin_real, _ = self.discriminator(real_images)\n",
    "                d_bin_real_loss = self.criterion_bin(output_bin_real, label)\n",
    "\n",
    "                # Generate fake images\n",
    "                fake_images = []\n",
    "                gen_labels = []\n",
    "                for idx, gen in enumerate(self.generators):\n",
    "                    z = self._sample_z(b_size // self.num_gens).to(self.device)\n",
    "                    gen_imgs = gen(z)\n",
    "                    fake_images.append(gen_imgs)\n",
    "                    gen_labels.append(torch.full((gen_imgs.size(0),), idx, dtype=torch.long, device=self.device))\n",
    "\n",
    "                fake_images = torch.cat(fake_images, 0)\n",
    "                gen_labels = torch.cat(gen_labels, 0)\n",
    "\n",
    "                label_fake = torch.full((fake_images.size(0),), fake_label, device=self.device, dtype=torch.float)\n",
    "\n",
    "                # Forward pass fake batch through D\n",
    "                output_bin_fake, output_mul_fake = self.discriminator(fake_images.detach())\n",
    "                d_bin_fake_loss = self.criterion_bin(output_bin_fake, label_fake)\n",
    "                d_mul_loss = self.criterion_mul(output_mul_fake, gen_labels)\n",
    "\n",
    "                # Sum all discriminator losses\n",
    "                d_loss = d_bin_real_loss + d_bin_fake_loss + d_mul_loss * self.beta\n",
    "                d_loss.backward()\n",
    "                self.optimizerD.step()\n",
    "\n",
    "                ############################\n",
    "                # (2) Update G network\n",
    "                ###########################\n",
    "                for gen in self.generators:\n",
    "                    gen.zero_grad()\n",
    "                self.shared_gen_layers.zero_grad()\n",
    "\n",
    "                # We want the generator to fool the discriminator\n",
    "                label = torch.full((fake_images.size(0),), real_label, device=self.device, dtype=torch.float)\n",
    "\n",
    "                output_bin_fake, output_mul_fake = self.discriminator(fake_images)\n",
    "                g_bin_loss = self.criterion_bin(output_bin_fake, label)\n",
    "                g_mul_loss = self.criterion_mul(output_mul_fake, gen_labels) * self.beta\n",
    "\n",
    "                g_loss = g_bin_loss + g_mul_loss\n",
    "                g_loss.backward()\n",
    "                self.optimizerG.step()\n",
    "\n",
    "                # Save losses for plotting\n",
    "                self.history['d_loss'].append(d_loss.item())\n",
    "                self.history['g_loss'].append(g_loss.item())\n",
    "\n",
    "            # Output training stats\n",
    "            print('[%d/%d] d_loss: %.4f | g_loss: %.4f'\n",
    "                  % (epoch+1, self.num_epochs,\n",
    "                     d_loss.item(),\n",
    "                     g_loss.item()))\n",
    "\n",
    "            # Save samples every few epochs\n",
    "            if (epoch+1) % 5 == 0:\n",
    "                self._save_samples(epoch+1, fixed_noise)\n",
    "\n",
    "        # After training, plot the learning curves\n",
    "        self._plot_history()\n",
    "\n",
    "    def _sample_z(self, size):\n",
    "        if self.z_prior == \"uniform\":\n",
    "            return torch.rand(size, self.num_z) * 2 - 1  # Uniform between [-1, 1]\n",
    "        else:\n",
    "            return torch.randn(size, self.num_z)\n",
    "\n",
    "    def _save_samples(self, epoch, fixed_noise):\n",
    "        # Create directory if it doesn't exist\n",
    "        if not os.path.exists(self.sample_dir):\n",
    "            os.makedirs(self.sample_dir)\n",
    "\n",
    "        # Generate images\n",
    "        with torch.no_grad():\n",
    "            fake_images_list = []\n",
    "            for idx, gen in enumerate(self.generators):\n",
    "                noise = fixed_noise[idx * 16: (idx + 1) * 16]\n",
    "                gen.eval()\n",
    "                fake_images = gen(noise.to(self.device))\n",
    "                gen.train()\n",
    "                fake_images_list.append(fake_images)\n",
    "\n",
    "            # Concatenate images from all generators\n",
    "            fake_images = torch.cat(fake_images_list, 0)\n",
    "\n",
    "            # Normalize images to range [0, 1]\n",
    "            fake_images = (fake_images + 1) / 2.0\n",
    "\n",
    "            # Save the images\n",
    "            sample_path = os.path.join(self.sample_dir, 'epoch_{:04d}.png'.format(epoch))\n",
    "            vutils.save_image(fake_images, sample_path, nrow=16, padding=2, normalize=True)\n",
    "\n",
    "    def _plot_history(self):\n",
    "        # Plot the learning curves\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.title(\"Loss During Training\")\n",
    "        plt.plot(self.history['d_loss'], label=\"D Loss\")\n",
    "        plt.plot(self.history['g_loss'], label=\"G Loss\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Unique Parameters: 14\n",
      "Total Parameters: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/tameszaza/My Passport/ml/.venv/lib/python3.12/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50] d_loss: 1.6870 | g_loss: 3.1622\n",
      "[2/50] d_loss: 0.2800 | g_loss: 3.1783\n",
      "[3/50] d_loss: 0.0714 | g_loss: 3.6869\n",
      "[4/50] d_loss: 0.6402 | g_loss: 1.5769\n",
      "[5/50] d_loss: 0.0400 | g_loss: 4.8623\n",
      "[6/50] d_loss: 0.1326 | g_loss: 3.5003\n",
      "[7/50] d_loss: 0.0326 | g_loss: 4.7683\n",
      "[8/50] d_loss: 0.0306 | g_loss: 5.0503\n",
      "[9/50] d_loss: 1.3559 | g_loss: 1.3223\n",
      "[10/50] d_loss: 0.4234 | g_loss: 1.1355\n",
      "[11/50] d_loss: 0.0443 | g_loss: 4.7578\n",
      "[12/50] d_loss: 0.2446 | g_loss: 3.1180\n",
      "[13/50] d_loss: 0.1646 | g_loss: 3.9531\n",
      "[14/50] d_loss: 0.0683 | g_loss: 4.1928\n",
      "[15/50] d_loss: 0.5358 | g_loss: 2.9124\n",
      "[16/50] d_loss: 0.6334 | g_loss: 1.7265\n",
      "[17/50] d_loss: 0.1083 | g_loss: 3.6146\n",
      "[18/50] d_loss: 0.0163 | g_loss: 6.3064\n",
      "[19/50] d_loss: 1.4168 | g_loss: 0.1740\n",
      "[20/50] d_loss: 0.0400 | g_loss: 4.2872\n",
      "[21/50] d_loss: 0.1637 | g_loss: 3.3855\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Main function to train MGAN\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    num_z = 100  # Size of z latent vector (i.e. size of generator input)\n",
    "    beta = 0.5   # Diversity parameter beta\n",
    "    num_gens = 10  # Number of generators\n",
    "    batch_size = 128  # Batch size\n",
    "    z_prior = \"gaussian\"  # Prior distribution of the noise ('uniform' or 'gaussian')\n",
    "    learning_rate = 0.0002  # Learning rate for optimizers\n",
    "    num_epochs = 50  # Number of training epochs\n",
    "    image_size = 64  # Spatial size of training images\n",
    "    num_channels = 1  # Number of channels in the training images. For MNIST, it's 1\n",
    "    num_gen_feature_maps = 64\n",
    "    num_dis_feature_maps = 64\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # Create the dataset\n",
    "    dataroot = './data'\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize the images to [-1, 1]\n",
    "    ])\n",
    "\n",
    "    # Load the dataset\n",
    "    trainset = torchvision.datasets.MNIST(root=dataroot, train=True,\n",
    "                                          download=True, transform=transform)\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size,\n",
    "                             shuffle=True, num_workers=2)\n",
    "\n",
    "    # Create the MGAN model\n",
    "    mgan_model = MGAN(\n",
    "        num_z=num_z,\n",
    "        beta=beta,\n",
    "        num_gens=num_gens,\n",
    "        batch_size=batch_size,\n",
    "        z_prior=z_prior,\n",
    "        learning_rate=learning_rate,\n",
    "        num_epochs=num_epochs,\n",
    "        img_size=(image_size, image_size, num_channels),\n",
    "        num_gen_feature_maps=num_gen_feature_maps,\n",
    "        num_dis_feature_maps=num_dis_feature_maps,\n",
    "        sample_dir=\"samples4(seed 6789)\",\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    mgan_model.fit(trainloader)\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
