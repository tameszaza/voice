{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from model import MGAN  # Import the MGAN class\n",
    "from dataset_load import librispeech_to_mel  # Your dataset preprocessing function\n",
    "\n",
    "def librispeech_to_mel(root_dir, target_sr=16000, n_mels=64, n_fft=1024, hop_length=512, target_length=128):\n",
    "    \"\"\"\n",
    "    Convert raw LibriSpeech audio files to Mel spectrograms and return a TensorDataset.\n",
    "    \"\"\"\n",
    "    flac_files = glob.glob(os.path.join(root_dir, '**', '*.flac'), recursive=True)\n",
    "    mel_spectrograms = []  # Initialize list to store Mel spectrograms\n",
    "\n",
    "    if len(flac_files) == 0:\n",
    "        raise ValueError(f\"No FLAC files found in directory: {root_dir}\")\n",
    "\n",
    "    for file in flac_files:\n",
    "        try:\n",
    "            # Load the audio file\n",
    "            audio, sr = librosa.load(file, sr=target_sr)\n",
    "            # Normalize audio to range [-1, 1]\n",
    "            audio = audio / np.max(np.abs(audio))\n",
    "            \n",
    "            # Convert to Mel spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "            \n",
    "            # Convert to dB scale\n",
    "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            \n",
    "            # Ensure the Mel spectrogram has a fixed length\n",
    "            if mel_spec_db.shape[1] < target_length:\n",
    "                # Pad if shorter\n",
    "                pad_width = target_length - mel_spec_db.shape[1]\n",
    "                mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)), mode='constant')\n",
    "            else:\n",
    "                # Trim if longer\n",
    "                mel_spec_db = mel_spec_db[:, :target_length]\n",
    "\n",
    "            # Add channel dimension and append to list\n",
    "            mel_tensor = torch.tensor(mel_spec_db, dtype=torch.float32).unsqueeze(0)  # Add channel dim\n",
    "            mel_spectrograms.append(mel_tensor)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    if len(mel_spectrograms) == 0:\n",
    "        raise ValueError(\"No valid audio files were processed into Mel spectrograms.\")\n",
    "\n",
    "    # Stack into a single tensor\n",
    "    mel_spectrograms = torch.stack(mel_spectrograms)  # Shape: [num_samples, 1, n_mels, target_length]\n",
    "    return TensorDataset(mel_spectrograms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_z = 100\n",
    "beta = 0.5\n",
    "num_gens = 10\n",
    "batch_size = 16\n",
    "z_prior = \"gaussian\"\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 50\n",
    "\n",
    "# Spectrogram dimensions\n",
    "mel_bins = 64\n",
    "num_frames = 128\n",
    "num_channels = 1\n",
    "img_size = (mel_bins, num_frames, num_channels)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset preprocessing\n",
    "data_dir = os.path.join(\"data\", \"LibriSpeech\", \"LibriSpeech\", \"dev-clean\")\n",
    "print(\"Preprocessing dataset into Mel spectrograms...\")\n",
    "try:\n",
    "    mel_dataset = librispeech_to_mel(data_dir, target_sr=16000, n_mels=mel_bins, target_length=num_frames)\n",
    "    print(f\"Dataset size: {len(mel_dataset)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during dataset preprocessing: {e}\")\n",
    "    return\n",
    "\n",
    "# Check if dataset is empty\n",
    "if len(mel_dataset) == 0:\n",
    "    raise ValueError(\"The dataset is empty. Please check your data directory and preprocessing function.\")\n",
    "\n",
    "# Save visualizations of the first 5 Mel spectrograms\n",
    "visualization_dir = \"mel_spectrograms\"\n",
    "print(\"Saving Mel spectrogram visualizations...\")\n",
    "save_mel_spectrograms(mel_dataset, visualization_dir, num_to_visualize=5)\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(mel_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize MGAN\n",
    "sample_dir = \"samples\"\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "mgan_model = MGAN(\n",
    "    num_z=num_z,\n",
    "    beta=beta,\n",
    "    num_gens=num_gens,\n",
    "    batch_size=batch_size,\n",
    "    z_prior=z_prior,\n",
    "    learning_rate=learning_rate,\n",
    "    num_epochs=num_epochs,\n",
    "    img_size=img_size,\n",
    "    num_gen_feature_maps=64,\n",
    "    num_dis_feature_maps=64,\n",
    "    sample_dir=sample_dir,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Load pre-trained model if available\n",
    "model_path = \"mgan_model.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        mgan_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "try:\n",
    "    mgan_model.fit(dataloader)\n",
    "    print(\"Training completed.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    return\n",
    "\n",
    "# Save the trained model\n",
    "try:\n",
    "    torch.save(mgan_model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
