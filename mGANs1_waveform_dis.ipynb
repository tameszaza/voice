{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  orthogonal loss \n",
    "This model work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "import logging\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename=\"training_log_500.txt\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_flac(file_path, target_sr=16000):\n",
    "    \"\"\"Load a FLAC audio file and resample it to the target sample rate.\"\"\"\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
    "    max_val = np.max(np.abs(audio))\n",
    "    if max_val > 0:\n",
    "        audio = audio / max_val  # Normalize to [-1, 1]\n",
    "    return audio\n",
    "\n",
    "def pad_or_trim(audio, target_length=64000):\n",
    "    \"\"\"Pad or trim audio to the target length.\"\"\"\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:target_length]\n",
    "    return audio\n",
    "\n",
    "def preprocess_dataset(root_dir, target_sr=16000, target_length=64000):\n",
    "    flac_files = glob(os.path.join(root_dir, '**', '*.flac'), recursive=True)\n",
    "    logging.info(f\"Found {len(flac_files)} .flac files in {root_dir}.\")\n",
    "    if len(flac_files) == 0:\n",
    "        logging.info(\"No .flac files found. Please check the root_dir path.\")\n",
    "    dataset = []\n",
    "    fragmented_count = 0  # Count of fragmented audio files\n",
    "    sink = []  # Buffer to hold audio fragments less than target_length\n",
    "\n",
    "    for file in flac_files:\n",
    "        try:\n",
    "            audio = load_flac(file, target_sr)\n",
    "            audio_length = len(audio)\n",
    "            if audio_length == target_length:\n",
    "                # Audio is exactly target_length, add directly to dataset\n",
    "                dataset.append(audio)\n",
    "            elif audio_length < target_length:\n",
    "                # Audio is shorter than target_length, add to sink\n",
    "                fragmented_count += 1\n",
    "                sink.append(audio)\n",
    "            else:\n",
    "                # Audio is longer than target_length\n",
    "                # Split audio into chunks of target_length\n",
    "                num_full_chunks = audio_length // target_length\n",
    "                for i in range(num_full_chunks):\n",
    "                    chunk = audio[i * target_length : (i + 1) * target_length]\n",
    "                    dataset.append(chunk)\n",
    "                # Add any remaining part to the sink\n",
    "                remainder = audio[num_full_chunks * target_length :]\n",
    "                if len(remainder) > 0:\n",
    "                    sink.append(remainder)\n",
    "        except Exception as e:\n",
    "            logging.info(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    # Now, process the sink to create additional samples\n",
    "    logging.info(f\"Number of fragmented audio fragments in sink: {len(sink)}\")\n",
    "    # Keep track of how many additional samples we create\n",
    "    num_additional_samples = 0\n",
    "\n",
    "    # Combine fragments in the sink to form new audio samples of target_length\n",
    "    current_audio = np.array([], dtype=np.float32)\n",
    "    for fragment in sink:\n",
    "        current_audio = np.concatenate((current_audio, fragment))\n",
    "        while len(current_audio) >= target_length:\n",
    "            # Extract a chunk of target_length\n",
    "            chunk = current_audio[:target_length]\n",
    "            dataset.append(chunk)\n",
    "            num_additional_samples += 1\n",
    "            # Remove the chunk from current_audio\n",
    "            current_audio = current_audio[target_length:]\n",
    "\n",
    "    # Optionally handle the last remaining fragment\n",
    "    # Decide whether to discard or pad the last fragment\n",
    "    if len(current_audio) > 0:\n",
    "        # You can choose to discard it or pad it to target_length\n",
    "        # Here we pad it and add it to the dataset\n",
    "        padded_audio = pad_or_trim(current_audio, target_length)\n",
    "        dataset.append(padded_audio)\n",
    "        num_additional_samples += 1\n",
    "\n",
    "    logging.info(f\"Number of additional samples created from sink fragments: {num_additional_samples}\")\n",
    "    logging.info(f\"Total number of audio samples in dataset: {len(dataset)}\")\n",
    "\n",
    "    dataset = torch.tensor(dataset, dtype=torch.float32)\n",
    "    return TensorDataset(dataset)\n",
    "\n",
    "def save_waveform_to_audio(waveform, sample_rate, filename):\n",
    "    \"\"\"\n",
    "    Save a waveform to an audio file.\n",
    "    \"\"\"\n",
    "    if isinstance(waveform, torch.Tensor):\n",
    "        waveform = waveform.detach().cpu().numpy()\n",
    "    \n",
    "    waveform = np.squeeze(waveform)\n",
    "    max_val = np.max(np.abs(waveform))\n",
    "    if max_val > 0:\n",
    "        waveform = waveform / max_val\n",
    "    \n",
    "    sf.write(filename, waveform, sample_rate)\n",
    "\n",
    "def verify_waveform_to_audio(root_dir, sample_rate=16000, target_length=64000, output_dir=\"verified_audio\"):\n",
    "    \"\"\"\n",
    "    Verify waveform-to-audio conversion using preprocessed dataset.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    dataset = preprocess_dataset(root_dir, sample_rate, target_length)\n",
    "    \n",
    "    num_samples_to_verify = min(5, len(dataset))\n",
    "    \n",
    "    for idx in range(num_samples_to_verify):\n",
    "        waveform = dataset.tensors[0][idx]\n",
    "        \n",
    "        filename = os.path.join(output_dir, f\"example_waveform_{idx+1}.wav\")\n",
    "        save_waveform_to_audio(waveform, sample_rate, filename)\n",
    "        logging.info(f\"Waveform saved to {filename}\")\n",
    "        \n",
    "        # Plot the waveform\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(waveform.numpy())\n",
    "        plt.title(f\"Waveform {idx+1}\")\n",
    "        plt.xlabel(\"Sample Index\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_length = 64000  # Set this to your actual audio waveform length\n",
    "root_dir = \"./data\"  # Replace with your actual data directory\n",
    "# verify_waveform_to_audio(root_dir, sample_rate=16000, target_length=64000, output_dir=\"verified_audio\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = preprocess_dataset(root_dir=\"./data\", target_sr=16000, target_length=audio_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.benchmark = False  # Turn off the cuDNN auto-tuner to avoid nondeterministic behavior\n",
    "\n",
    "\n",
    "# img_size = 28\n",
    "# img_channels = 1\n",
    "\n",
    "def get_dim_for_each_layer(z_dim, total_l, l, output_dim):\n",
    "    \"\"\"\n",
    "    Calculate the dimension for the l-th layer in the generator.\n",
    "\n",
    "    Parameters:\n",
    "    - z_dim: int, the input dimension (e.g., latent vector size).\n",
    "    - total_l: int, the total number of layers in the generator.\n",
    "    - l: int, the current layer index (1-indexed).\n",
    "    - output_dim: int, the final output dimension (e.g., audio length).\n",
    "\n",
    "    Returns:\n",
    "    - int: the calculated dimension for the l-th layer.\n",
    "    \"\"\"\n",
    "    if l < 1 or l > total_l:\n",
    "        raise ValueError(\"Layer index 'l' must be in the range [1, total_l].\")\n",
    "    if l == total_l:\n",
    "        return output_dim\n",
    "    if l == 1:\n",
    "        return z_dim\n",
    "    # Calculate the dimension change per layer\n",
    "    step = (output_dim - z_dim) / (total_l - 1)\n",
    "    \n",
    "    # Compute the dimension for the l-th layer\n",
    "    dim = z_dim + (l - 1) * step\n",
    "    return math.ceil(dim)  # Use math.ceil to round up to an integer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, audio_length):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.initial_length = audio_length // 256  # 64000 / 256 = 250\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(z_dim, 512 * self.initial_length),  # Output: (batch_size, 512 * 250)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Unflatten(1, (512, self.initial_length)),  # Shape: (batch_size, 512, 250)\n",
    "            nn.ConvTranspose1d(512, 256, kernel_size=25, stride=4, padding=11, output_padding=1),  # Output length: 1000\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=25, stride=4, padding=11, output_padding=1),  # Output length: 4000\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=25, stride=4, padding=11, output_padding=1),   # Output length: 16000\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose1d(64, 1, kernel_size=25, stride=4, padding=11, output_padding=1),     # Output length: 64000\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, audio_length):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=25, stride=4, padding=11),  # Output: (batch_size, 64, L1)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(64, 128, kernel_size=25, stride=4, padding=11),  # Output: (batch_size, 128, L2)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(128, 256, kernel_size=25, stride=4, padding=11),  # Output: (batch_size, 256, L3)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(256, 512, kernel_size=25, stride=4, padding=11),  # Output: (batch_size, 512, L4)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool1d(1),  # Output: (batch_size, 512, 1)\n",
    "            nn.Flatten(),             # Output: (batch_size, 512)\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Remove the unnecessary unsqueeze\n",
    "        # x = x.unsqueeze(1)  # This line is removed\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, audio_length):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool1d(1),  # Output: (batch_size, 128, 1)\n",
    "            nn.Flatten(),             # Output: (batch_size, 128)\n",
    "            nn.Linear(128, 64)        # Output: (batch_size, 64)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.unsqueeze(1)  # Remove if input x already has channel dimension\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "# z_dim = 100\n",
    "# audio_length = 64000\n",
    "\n",
    "# generator = Generator(z_dim, audio_length)\n",
    "# discriminator = Discriminator(audio_length)\n",
    "\n",
    "# # Generate fake audio\n",
    "# noise = torch.randn(batch_size, z_dim)\n",
    "# fake_audio = generator(noise)\n",
    "\n",
    "# # Pass fake audio through the discriminator\n",
    "# disc_output = discriminator(fake_audio)\n",
    "\n",
    "# logging.info(f\"Fake audio shape: {fake_audio.shape}\")         # Expected: (batch_size, 1, 64000)\n",
    "# logging.info(f\"Discriminator output shape: {disc_output.shape}\")  # Expected: (batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noise(batch_size, z_dim, device):\n",
    "    rand = torch.rand(batch_size, z_dim).to(device)  # Values in [0, 1]\n",
    "    scaled_rand = rand * 20 - 10  # Scale to [-10, 10]\n",
    "    return scaled_rand\n",
    "\n",
    "\n",
    "# Orthogonal loss function\n",
    "def orthogonal_loss(feature1, feature2):\n",
    "    inner_product = torch.sum(feature1 * feature2, dim=1)\n",
    "    norm1 = torch.norm(feature1, dim=1)\n",
    "    norm2 = torch.norm(feature2, dim=1)\n",
    "    cosine_similarity = inner_product / (norm1 * norm2 + 1e-8)\n",
    "    return torch.mean(cosine_similarity**2)  # Minimize the cosine similarity to make vectors orthogonal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(critic, real_samples, fake_samples, device):\n",
    "    batch_size = real_samples.size(0)\n",
    "\n",
    "    # Sample epsilon uniformly in [0,1]\n",
    "    epsilon = torch.rand(batch_size, 1, 1, device=device)\n",
    "    epsilon = epsilon.expand_as(real_samples)\n",
    "\n",
    "    # Interpolate between real and fake samples\n",
    "    interpolates = (epsilon * real_samples + (1 - epsilon) * fake_samples).requires_grad_(True)\n",
    "    interpolates_output = critic(interpolates)\n",
    "\n",
    "    # Compute gradients\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=interpolates_output,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(interpolates_output),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "\n",
    "    # Reshape gradients\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_and_save_generated_waveforms(generators, z_dim, num_waveforms, device,epoch, sample_rate=16000, output_dir='generated_audio'):\n",
    "    import os\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    noise = generate_noise(num_waveforms, z_dim, device)\n",
    "    for idx, gen in enumerate(generators):\n",
    "        fake_waveforms = gen(noise).detach().cpu().numpy()\n",
    "        for i in range(num_waveforms):\n",
    "            waveform = fake_waveforms[i]\n",
    "            # Save each waveform to an audio file\n",
    "            filename = f'epoch{epoch+1}_gen{idx+1}_sample{i+1}.wav'\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            save_waveform_to_audio(waveform, sample_rate, filepath)\n",
    "            logging.info(f\"Saved {filepath}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def pretrain_single_generator(num_epochs, z_dim, lr_gen, lr_disc, batch_size, seed, audio_length, output_dir, train_dataset):\n",
    "    # Set the seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    # Check for device (GPU or CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define the single generator and discriminator\n",
    "    generator = Generator(z_dim, audio_length).to(device)\n",
    "    discriminator = Discriminator(audio_length).to(device)\n",
    "\n",
    "    optimizer_gen = optim.Adam(generator.parameters(), lr=lr_gen, betas=(0.5, 0.9))\n",
    "    optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "    # Load and preprocess the audio dataset\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    lambda_gp = 10  # Gradient penalty coefficient\n",
    "    num_critic = 5  # Number of discriminator updates per generator update\n",
    "\n",
    "    # To track the losses\n",
    "    loss_disc_history = []\n",
    "    loss_gen_history = []\n",
    "\n",
    "    # Resume training if checkpoints exist\n",
    "    checkpoint_path = os.path.join(output_dir, \"checkpoint.pth\")\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        logging.info(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        optimizer_gen.load_state_dict(checkpoint['optimizer_gen_state_dict'])\n",
    "        optimizer_disc.load_state_dict(checkpoint['optimizer_disc_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        loss_disc_history = checkpoint['loss_disc_history']\n",
    "        loss_gen_history = checkpoint['loss_gen_history']\n",
    "        logging.info(f\"Resumed from epoch {start_epoch}\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        torch.cuda.empty_cache()  # Clear unused memory\n",
    "        loss_disc_epoch = 0\n",
    "        loss_gen_epoch = 0\n",
    "\n",
    "        for batch_idx, (real,) in enumerate(train_loader):\n",
    "            real = real.to(device)\n",
    "            batch_size = real.size(0)\n",
    "            if real.dim() == 2:\n",
    "                real = real.unsqueeze(1)  # Add channel dimension (batch_size, 1, audio_length)\n",
    "\n",
    "            #logging.info(f\"real shape: {real.shape}\")\n",
    "\n",
    "            # Train Discriminator multiple times\n",
    "            for _ in range(num_critic):\n",
    "                optimizer_disc.zero_grad()\n",
    "\n",
    "                # Generate fake data\n",
    "                noise = generate_noise(batch_size, z_dim, device)\n",
    "                fake = generator(noise).detach()\n",
    "\n",
    "                # Compute discriminator outputs\n",
    "                disc_real = discriminator(real)\n",
    "                disc_fake = discriminator(fake)\n",
    "\n",
    "                # Compute Wasserstein loss\n",
    "                loss_disc_real = -torch.mean(disc_real)\n",
    "                loss_disc_fake = torch.mean(disc_fake)\n",
    "                loss_disc = loss_disc_real + loss_disc_fake\n",
    "\n",
    "                # Compute gradient penalty\n",
    "                gradient_penalty = compute_gradient_penalty(discriminator, real.data, fake.data, device)\n",
    "                loss_disc += lambda_gp * gradient_penalty\n",
    "\n",
    "                loss_disc.backward()\n",
    "                optimizer_disc.step()\n",
    "\n",
    "            loss_disc_epoch += loss_disc.item()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_gen.zero_grad()\n",
    "            noise = generate_noise(batch_size, z_dim, device)\n",
    "            fake = generator(noise)\n",
    "            disc_fake = discriminator(fake)\n",
    "            loss_gen = -torch.mean(disc_fake)\n",
    "            loss_gen.backward()\n",
    "            optimizer_gen.step()\n",
    "\n",
    "            loss_gen_epoch += loss_gen.item()\n",
    "\n",
    "        avg_loss_disc = loss_disc_epoch / len(train_loader)\n",
    "        avg_loss_gen = loss_gen_epoch / len(train_loader)\n",
    "\n",
    "        # Record the losses\n",
    "        loss_disc_history.append(avg_loss_disc)\n",
    "        loss_gen_history.append(avg_loss_gen)\n",
    "\n",
    "        logging.info(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {avg_loss_disc:.4f}, Loss G: {avg_loss_gen:.4f}\")\n",
    "\n",
    "        # Visualize generated waveforms\n",
    "        visualize_and_save_generated_waveforms(\n",
    "            [generator], z_dim, num_waveforms=5, device=device, epoch=epoch, sample_rate=16000, output_dir=output_dir\n",
    "        )\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_gen_state_dict': optimizer_gen.state_dict(),\n",
    "            'optimizer_disc_state_dict': optimizer_disc.state_dict(),\n",
    "            'loss_disc_history': loss_disc_history,\n",
    "            'loss_gen_history': loss_gen_history,\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    # Save the generator model\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    torch.save(generator.state_dict(), os.path.join(output_dir, \"pretrained_generator.pth\"))\n",
    "    logging.info(f\"Pretrained generator model saved to {os.path.join(output_dir, 'pretrained_generator.pth')}\")\n",
    "\n",
    "    # Plot the learning curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(loss_disc_history, label=\"Discriminator Loss\")\n",
    "    plt.plot(loss_gen_history, label=\"Generator Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Learning Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, \"learning_curves.png\"))\n",
    "    plt.show()\n",
    "\n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import os\n",
    "from torch.nn.utils import spectral_norm\n",
    "import torch.autograd as autograd\n",
    "# Wasserstein loss\n",
    "def wasserstein_loss(y_pred, y_true):\n",
    "    return torch.mean(y_pred * y_true)\n",
    "\n",
    "def compute_gradient_penalty(discriminator, real_data, fake_data, device):\n",
    "    batch_size = real_data.size(0)\n",
    "    # Corrected epsilon shape to match real_data dimensions\n",
    "    epsilon = torch.rand(batch_size, 1, 1, device=device)  # Shape: [batch_size, 1, 1]\n",
    "    epsilon = epsilon.expand_as(real_data)  # Now expands to [batch_size, 1, audio_length]\n",
    "    \n",
    "    # Interpolate between real and fake data\n",
    "    interpolates = epsilon * real_data + (1 - epsilon) * fake_data\n",
    "    interpolates = interpolates.requires_grad_(True)\n",
    "    \n",
    "    # Compute discriminator output\n",
    "    disc_interpolates = discriminator(interpolates)\n",
    "    \n",
    "    # Compute gradients\n",
    "    grad_outputs = torch.ones_like(disc_interpolates)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=disc_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=grad_outputs,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    # Reshape gradients\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_with_pretrained_generators(\n",
    "    pretrained_generator, num_epochs, z_dim, lr_gen, lr_disc, batch_size, train_dataset,\n",
    "    num_generators, seed, audio_length, output_dir, lambda_gp=10, lambda_ortho=0.1, num_critic=5,\n",
    "    checkpoint_dir='checkpoints', resume=True,\n",
    "    use_external_tts=True  # ADDED: flag to use external TTS samples\n",
    "):\n",
    "    import os\n",
    "\n",
    "    # Set the seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize multiple generators with the pretrained generator\n",
    "    generators = []\n",
    "    for _ in range(num_generators):\n",
    "        gen = Generator(z_dim, audio_length).to(device)\n",
    "        gen.load_state_dict(pretrained_generator.state_dict())\n",
    "        generators.append(gen)\n",
    "\n",
    "    # Initialize Discriminator\n",
    "    discriminator = Discriminator(audio_length).to(device)\n",
    "\n",
    "    optimizer_gens = [optim.Adam(gen.parameters(), lr=lr_gen, betas=(0.5, 0.9)) for gen in generators]\n",
    "    optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "    # Initialize Encoder only if lambda_ortho > 0\n",
    "    if lambda_ortho > 0:\n",
    "        encoder = Encoder(audio_length).to(device)\n",
    "        optimizer_encoder = optim.Adam(encoder.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "    else:\n",
    "        encoder = None\n",
    "        optimizer_encoder = None\n",
    "\n",
    "    # Load and preprocess the audio dataset\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Check for existing checkpoint\n",
    "    start_epoch = 0\n",
    "    if resume and os.path.exists(os.path.join(checkpoint_dir, 'checkpoint.pth')):\n",
    "        logging.info(\"Resuming from checkpoint...\")\n",
    "        checkpoint = torch.load(os.path.join(checkpoint_dir, 'checkpoint.pth'), map_location=device)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "        # Load models\n",
    "        for idx, gen in enumerate(generators):\n",
    "            gen.load_state_dict(checkpoint['generator_state_dicts'][idx])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        if lambda_ortho > 0:\n",
    "            encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "\n",
    "        # Load optimizers\n",
    "        for idx, optimizer_gen in enumerate(optimizer_gens):\n",
    "            optimizer_gen.load_state_dict(checkpoint['optimizer_gens_state_dicts'][idx])\n",
    "        optimizer_disc.load_state_dict(checkpoint['optimizer_disc_state_dict'])\n",
    "        if lambda_ortho > 0:\n",
    "            optimizer_encoder.load_state_dict(checkpoint['optimizer_encoder_state_dict'])\n",
    "\n",
    "        # Load loss histories\n",
    "        loss_disc_history = checkpoint['loss_disc_history']\n",
    "        loss_gens_history = checkpoint['loss_gens_history']\n",
    "\n",
    "    else:\n",
    "        logging.info(\"Starting training from scratch.\")\n",
    "        start_epoch = 0\n",
    "        # Initialize loss histories\n",
    "        loss_disc_history = []\n",
    "        loss_gens_history = [[] for _ in range(num_generators)]\n",
    "\n",
    "    # Optionally, load external TTS model(s) here if needed (not shown).\n",
    "    # For demonstration, we just use `generate_external_tts_samples()` directly.\n",
    "\n",
    "    try:\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            torch.cuda.empty_cache()\n",
    "            loss_disc_epoch = 0\n",
    "            loss_gens_epoch = [0] * num_generators\n",
    "\n",
    "            for batch_idx, (real,) in enumerate(train_loader):\n",
    "                real = real.to(device)\n",
    "                if real.dim() == 2:\n",
    "                    real = real.unsqueeze(1)  # Add channel dimension (batch_size, 1, audio_length)\n",
    "                batch_size = real.size(0)\n",
    "                real_label = -torch.ones(batch_size, 1, device=device)\n",
    "                fake_label = torch.ones(batch_size, 1, device=device)\n",
    "\n",
    "                real = real + 0.001 * torch.randn_like(real)\n",
    "\n",
    "                # Train Discriminator multiple times\n",
    "                for _ in range(num_critic):\n",
    "                    optimizer_disc.zero_grad()\n",
    "\n",
    "                    disc_real = discriminator(real)\n",
    "\n",
    "                    noises = [generate_noise(batch_size, z_dim, device) for _ in range(num_generators)]\n",
    "                    fakes = [gen(noises[idx]).detach() for idx, gen in enumerate(generators)]\n",
    "                    for idx in range(num_generators):\n",
    "                        fakes[idx] = fakes[idx] + 0.001 * torch.randn_like(fakes[idx])\n",
    "\n",
    "                    # ADDED CODE: Generate external TTS samples (not used for orthogonal loss, not updated)\n",
    "                    external_fakes = []\n",
    "                    if use_external_tts:\n",
    "                        external_fakes = generate_external_tts_samples(batch_size, audio_length, device)\n",
    "                        external_fakes = external_fakes + 0.001 * torch.randn_like(external_fakes)\n",
    "\n",
    "                    # Combine all fakes for discriminator: our own generators + external TTS\n",
    "                    all_fakes = fakes[:]\n",
    "                    if use_external_tts:\n",
    "                        all_fakes.append(external_fakes)\n",
    "\n",
    "                    disc_fakes = [discriminator(fake_sample) for fake_sample in all_fakes]\n",
    "\n",
    "                    # Average the fake losses over all sources (both internal and external)\n",
    "                    loss_disc_fake = sum(wasserstein_loss(disc_fake, fake_label) for disc_fake in disc_fakes) / len(disc_fakes)\n",
    "                    loss_disc_real = wasserstein_loss(disc_real, real_label)\n",
    "                    loss_disc = loss_disc_real + loss_disc_fake\n",
    "\n",
    "                    # Compute gradient penalty averaged over all fake sources\n",
    "                    gradient_penalty = sum(\n",
    "                        compute_gradient_penalty(discriminator, real, fake_sample, device) \n",
    "                        for fake_sample in all_fakes\n",
    "                    ) / len(all_fakes)\n",
    "                    loss_disc += lambda_gp * gradient_penalty\n",
    "\n",
    "                    loss_disc.backward()\n",
    "                    optimizer_disc.step()\n",
    "\n",
    "                loss_disc_epoch += loss_disc.item()\n",
    "\n",
    "                # Train Generators (only your own generators, not external TTS)\n",
    "                for idx, gen in enumerate(generators):\n",
    "                    optimizer_gens[idx].zero_grad()\n",
    "                    if lambda_ortho > 0:\n",
    "                        optimizer_encoder.zero_grad()\n",
    "\n",
    "                    noise = generate_noise(batch_size, z_dim, device)\n",
    "                    fake = gen(noise)\n",
    "                    disc_fake = discriminator(fake)\n",
    "\n",
    "                    loss_gen = wasserstein_loss(disc_fake, real_label)\n",
    "\n",
    "                    if lambda_ortho > 0:\n",
    "                        # Compute orthogonal loss only among your own generators\n",
    "                        gen_feature = encoder(fake)\n",
    "                        ortho_loss_total = 0\n",
    "                        for other_idx, other_gen in enumerate(generators):\n",
    "                            if idx != other_idx:\n",
    "                                other_noise = generate_noise(batch_size, z_dim, device)\n",
    "                                other_fake = other_gen(other_noise)\n",
    "                                other_feature = encoder(other_fake)\n",
    "                                ortho_loss = orthogonal_loss(gen_feature, other_feature)\n",
    "                                ortho_loss_total += ortho_loss\n",
    "\n",
    "                        # Average orthogonal loss over other internal generators\n",
    "                        ortho_loss_total /= (num_generators - 1)\n",
    "                        total_loss_gen = loss_gen + lambda_ortho * ortho_loss_total\n",
    "                    else:\n",
    "                        total_loss_gen = loss_gen\n",
    "\n",
    "                    total_loss_gen.backward()\n",
    "                    optimizer_gens[idx].step()\n",
    "\n",
    "                    if lambda_ortho > 0:\n",
    "                        optimizer_encoder.step()\n",
    "\n",
    "                    loss_gens_epoch[idx] += total_loss_gen.item()\n",
    "\n",
    "            avg_loss_disc = loss_disc_epoch / len(train_loader)\n",
    "            avg_loss_gens = [loss / len(train_loader) for loss in loss_gens_epoch]\n",
    "\n",
    "            logging.info(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {avg_loss_disc:.4f}\")\n",
    "            for idx in range(num_generators):\n",
    "                logging.info(f\"Loss G{idx+1}: {avg_loss_gens[idx]:.4f}\")\n",
    "            logging.info('-' * 50)\n",
    "\n",
    "            # Record the losses\n",
    "            loss_disc_history.append(avg_loss_disc)\n",
    "            for idx in range(num_generators):\n",
    "                loss_gens_history[idx].append(avg_loss_gens[idx])\n",
    "\n",
    "            # Visualize generated waveforms from your own generators only\n",
    "            visualize_and_save_generated_waveforms(\n",
    "                generators, z_dim, num_waveforms=5, device=device, epoch=epoch, sample_rate=16000, output_dir=output_dir\n",
    "            )\n",
    "\n",
    "            # Save checkpoint after each epoch\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'generator_state_dicts': [gen.state_dict() for gen in generators],\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_gens_state_dicts': [opt.state_dict() for opt in optimizer_gens],\n",
    "                'optimizer_disc_state_dict': optimizer_disc.state_dict(),\n",
    "                'loss_disc_history': loss_disc_history,\n",
    "                'loss_gens_history': loss_gens_history\n",
    "            }\n",
    "            if lambda_ortho > 0:\n",
    "                checkpoint['encoder_state_dict'] = encoder.state_dict()\n",
    "                checkpoint['optimizer_encoder_state_dict'] = optimizer_encoder.state_dict()\n",
    "\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            torch.save(checkpoint, os.path.join(checkpoint_dir, 'checkpoint.pth'))\n",
    "            logging.info(f\"Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logging.info(f\"Training interrupted at epoch {epoch}. Saving checkpoint...\")\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dicts': [gen.state_dict() for gen in generators],\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_gens_state_dicts': [opt.state_dict() for opt in optimizer_gens],\n",
    "            'optimizer_disc_state_dict': optimizer_disc.state_dict(),\n",
    "            'loss_disc_history': loss_disc_history,\n",
    "            'loss_gens_history': loss_gens_history\n",
    "        }\n",
    "        if lambda_ortho > 0:\n",
    "            checkpoint['encoder_state_dict'] = encoder.state_dict()\n",
    "            checkpoint['optimizer_encoder_state_dict'] = optimizer_encoder.state_dict()\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, 'checkpoint.pth'))\n",
    "        logging.info(f\"Checkpoint saved at epoch {epoch}\")\n",
    "        logging.info(\"Exiting training early.\")\n",
    "        return generators\n",
    "\n",
    "    return generators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_multiple_generators(pretrained_generator, num_generators, z_dim):\n",
    "    # Initialize multiple generators from the pretrained generator's weights\n",
    "    generators = []\n",
    "    for _ in range(num_generators):\n",
    "        new_generator = Generator(z_dim).to(pretrained_generator.gen[0].weight.device)  # Ensure same device\n",
    "        new_generator.load_state_dict(pretrained_generator.state_dict())  # Copy weights\n",
    "        generators.append(new_generator)\n",
    "    return generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_generator = pretrain_single_generator(num_epochs=20, \n",
    "                                                 z_dim=100, lr_disc=0.0002,lr_gen=0.0002, batch_size=64, seed=42, audio_length=64000, \n",
    "                                                 output_dir='waveform_pre_500_scalednoise', train_dataset=train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_generator(z_dim, weight_path, device, audio_length):\n",
    "    \"\"\"\n",
    "    Load the pretrained generator model weights.\n",
    "\n",
    "    Args:\n",
    "        z_dim (int): Latent space dimension.\n",
    "        weight_path (str): Path to the saved model weights.\n",
    "        device (torch.device): Device to load the model onto.\n",
    "\n",
    "    Returns:\n",
    "        Generator: Loaded generator model.\n",
    "    \"\"\"\n",
    "    generator = Generator(z_dim, audio_length=audio_length).to(device)\n",
    "    generator.load_state_dict(torch.load(weight_path, map_location=device))\n",
    "    generator.eval()  # Set the model to evaluation mode\n",
    "    logging.info(f\"Pretrained generator model loaded from {weight_path}\")\n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_generator = load_pretrained_generator(\n",
    "#     z_dim=100,\n",
    "#     weight_path='waveform_pre/pretrained_generator.pth',  # Path to saved weights\n",
    "#     device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), audio_length = audio_length\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training with multiple generators initialized from the pretrained one\n",
    "# train_gan_with_pretrained_generators(pretrained_generator, num_epochs=200, z_dim=100, lr_disc=0.0002, batch_size=24, num_generators=3, seed=42, lr_gen=0.0002, output_dir='waveform_mul', audio_length=64000, train_dataset = train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Clear unused CUDA memory\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accual train code (resumable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan_with_pretrained_generators(\n",
    "    pretrained_generator,\n",
    "    num_epochs=200,\n",
    "    z_dim=100,\n",
    "    lr_gen=0.0002,\n",
    "    lr_disc=0.0002,\n",
    "    batch_size=24,\n",
    "    train_dataset=train_dataset,\n",
    "    num_generators=5,\n",
    "    seed=42,\n",
    "    audio_length=64000,\n",
    "    output_dir='waveform_mal_2',\n",
    "    checkpoint_dir='my_checkpoints',\n",
    "    resume=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
