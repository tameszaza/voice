{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  orthogonal loss \n",
    "This model work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "import logging\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename=\"training_log_500.txt\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def load_flac(file_path, target_sr=16000):\n",
    "    \"\"\"Load a FLAC audio file and resample it to the target sample rate.\"\"\"\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
    "    max_val = np.max(np.abs(audio))\n",
    "    if max_val > 0:\n",
    "        audio = audio / max_val  # Normalize to [-1, 1]\n",
    "    return audio\n",
    "\n",
    "\n",
    "def pad_or_trim(audio, target_length=64000):\n",
    "    \"\"\"Pad or trim audio to the target length.\"\"\"\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:target_length]\n",
    "    return audio\n",
    "\n",
    "\n",
    "def preprocess_and_save_dataset(root_dir, output_dir, target_sr=16000, target_length=64000):\n",
    "    flac_files = glob(os.path.join(root_dir, '**', '*.flac'), recursive=True)\n",
    "    if len(flac_files) == 0:\n",
    "        raise ValueError(\"No .flac files found. Please check the root_dir path.\")\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    dataset = []\n",
    "    fragmented_count = 0\n",
    "    sink = []\n",
    "\n",
    "    counter = 0\n",
    "    for file in flac_files:\n",
    "        try:\n",
    "            audio = load_flac(file, target_sr)\n",
    "            audio_length = len(audio)\n",
    "\n",
    "            if audio_length == target_length:\n",
    "                sf.write(os.path.join(output_dir, f\"audio_{counter}.wav\"), audio, target_sr)\n",
    "                counter += 1\n",
    "            elif audio_length < target_length:\n",
    "                fragmented_count += 1\n",
    "                sink.append(audio)\n",
    "            else:\n",
    "                num_full_chunks = audio_length // target_length\n",
    "                for i in range(num_full_chunks):\n",
    "                    chunk = audio[i * target_length: (i + 1) * target_length]\n",
    "                    sf.write(os.path.join(output_dir, f\"audio_{counter}.wav\"), chunk, target_sr)\n",
    "                    counter += 1\n",
    "                \n",
    "                remainder = audio[num_full_chunks * target_length:]\n",
    "                if len(remainder) > 0:\n",
    "                    sink.append(remainder)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    current_audio = np.array([], dtype=np.float32)\n",
    "    num_additional_samples = 0\n",
    "\n",
    "    for fragment in sink:\n",
    "        current_audio = np.concatenate((current_audio, fragment))\n",
    "        while len(current_audio) >= target_length:\n",
    "            chunk = current_audio[:target_length]\n",
    "            sf.write(os.path.join(output_dir, f\"audio_{counter}.wav\"), chunk, target_sr)\n",
    "            counter += 1\n",
    "            num_additional_samples += 1\n",
    "            current_audio = current_audio[target_length:]\n",
    "\n",
    "    if len(current_audio) > 0:\n",
    "        padded_audio = pad_or_trim(current_audio, target_length)\n",
    "        sf.write(os.path.join(output_dir, f\"audio_{counter}.wav\"), padded_audio, target_sr)\n",
    "        counter += 1\n",
    "        num_additional_samples += 1\n",
    "\n",
    "    print(f\"Number of additional samples created from sink fragments: {num_additional_samples}\")\n",
    "    print(f\"Preprocessed data saved to {output_dir}. Total samples: {counter}\")\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.audio_files = sorted(glob(os.path.join(data_dir, '*.wav')))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.audio_files[idx]\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "        return torch.tensor(audio, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_waveform_to_audio(waveform, sample_rate, filename):\n",
    "    \"\"\"\n",
    "    Save a waveform to an audio file.\n",
    "    \"\"\"\n",
    "    if isinstance(waveform, torch.Tensor):\n",
    "        waveform = waveform.detach().cpu().numpy()\n",
    "    \n",
    "    waveform = np.squeeze(waveform)\n",
    "    max_val = np.max(np.abs(waveform))\n",
    "    if max_val > 0:\n",
    "        waveform = waveform / max_val\n",
    "    \n",
    "    sf.write(filename, waveform, sample_rate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of additional samples created from sink fragments: 1422\n",
      "Preprocessed data saved to ./preprocessed_data. Total samples: 4850\n"
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "root_dir = \"./data\"\n",
    "processed_data = './preprocessed_data'\n",
    "audio_length = 64000  # Set this to your desired length\n",
    "\n",
    "target_sr = 16000\n",
    "preprocess_and_save_dataset(root_dir, processed_data, target_sr=target_sr, target_length=audio_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AudioDataset(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.benchmark = False  # Turn off the cuDNN auto-tuner to avoid nondeterministic behavior\n",
    "\n",
    "\n",
    "# img_size = 28\n",
    "# img_channels = 1\n",
    "\n",
    "def get_dim_for_each_layer(z_dim, total_l, l, output_dim):\n",
    "    \"\"\"\n",
    "    Calculate the dimension for the l-th layer in the generator.\n",
    "\n",
    "    Parameters:\n",
    "    - z_dim: int, the input dimension (e.g., latent vector size).\n",
    "    - total_l: int, the total number of layers in the generator.\n",
    "    - l: int, the current layer index (1-indexed).\n",
    "    - output_dim: int, the final output dimension (e.g., audio length).\n",
    "\n",
    "    Returns:\n",
    "    - int: the calculated dimension for the l-th layer.\n",
    "    \"\"\"\n",
    "    if l < 1 or l > total_l:\n",
    "        raise ValueError(\"Layer index 'l' must be in the range [1, total_l].\")\n",
    "    if l == total_l:\n",
    "        return output_dim\n",
    "    if l == 1:\n",
    "        return z_dim\n",
    "    # Calculate the dimension change per layer\n",
    "    step = (output_dim - z_dim) / (total_l - 1)\n",
    "    \n",
    "    # Compute the dimension for the l-th layer\n",
    "    dim = z_dim + (l - 1) * step\n",
    "    return math.ceil(dim)  # Use math.ceil to round up to an integer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, audio_length):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.initial_length = audio_length // 256  # 64000 / 256 = 250\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(z_dim, 512 * self.initial_length),  # Output: (batch_size, 512 * 250)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Unflatten(1, (512, self.initial_length)),  # Shape: (batch_size, 512, 250)\n",
    "            nn.ConvTranspose1d(512, 256, kernel_size=25, stride=4, padding=11, output_padding=1),  # Output length: 1000\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=25, stride=4, padding=11, output_padding=1),  # Output length: 4000\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=25, stride=4, padding=11, output_padding=1),   # Output length: 16000\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose1d(64, 1, kernel_size=25, stride=4, padding=11, output_padding=1),     # Output length: 64000\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, audio_length):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=25, stride=4, padding=11),  # Output: (batch_size, 64, L1)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(64, 128, kernel_size=25, stride=4, padding=11),  # Output: (batch_size, 128, L2)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(128, 256, kernel_size=25, stride=4, padding=11),  # Output: (batch_size, 256, L3)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(256, 512, kernel_size=25, stride=4, padding=11),  # Output: (batch_size, 512, L4)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool1d(1),  # Output: (batch_size, 512, 1)\n",
    "            nn.Flatten(),             # Output: (batch_size, 512)\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Remove the unnecessary unsqueeze\n",
    "        # x = x.unsqueeze(1)  # This line is removed\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, audio_length):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool1d(1),  # Output: (batch_size, 128, 1)\n",
    "            nn.Flatten(),             # Output: (batch_size, 128)\n",
    "            nn.Linear(128, 64)        # Output: (batch_size, 64)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.unsqueeze(1)  # Remove if input x already has channel dimension\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noise(batch_size, z_dim, device):\n",
    "    return torch.randn(batch_size, z_dim).to(device)\n",
    "\n",
    "# Orthogonal loss function\n",
    "def orthogonal_loss(feature1, feature2):\n",
    "    inner_product = torch.sum(feature1 * feature2, dim=1)\n",
    "    norm1 = torch.norm(feature1, dim=1)\n",
    "    norm2 = torch.norm(feature2, dim=1)\n",
    "    cosine_similarity = inner_product / (norm1 * norm2 + 1e-8)\n",
    "    return torch.mean(cosine_similarity**2)  # Minimize the cosine similarity to make vectors orthogonal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(critic, real_samples, fake_samples, device):\n",
    "    batch_size = real_samples.size(0)\n",
    "\n",
    "    # Sample epsilon uniformly in [0,1]\n",
    "    epsilon = torch.rand(batch_size, 1, 1, device=device)\n",
    "    epsilon = epsilon.expand_as(real_samples)\n",
    "\n",
    "    # Interpolate between real and fake samples\n",
    "    interpolates = (epsilon * real_samples + (1 - epsilon) * fake_samples).requires_grad_(True)\n",
    "    interpolates_output = critic(interpolates)\n",
    "\n",
    "    # Compute gradients\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=interpolates_output,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(interpolates_output),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "\n",
    "    # Reshape gradients\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_and_save_generated_waveforms(generators, z_dim, num_waveforms, device,epoch, sample_rate=16000, output_dir='generated_audio'):\n",
    "    import os\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    noise = generate_noise(num_waveforms, z_dim, device)\n",
    "    for idx, gen in enumerate(generators):\n",
    "        fake_waveforms = gen(noise).detach().cpu().numpy()\n",
    "        for i in range(num_waveforms):\n",
    "            waveform = fake_waveforms[i]\n",
    "            # Save each waveform to an audio file\n",
    "            filename = f'epoch{epoch+1}_gen{idx+1}_sample{i+1}.wav'\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            save_waveform_to_audio(waveform, sample_rate, filepath)\n",
    "            logging.info(f\"Saved {filepath}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def pretrain_single_generator(num_epochs, z_dim, lr_gen, lr_disc, batch_size, seed, audio_length, output_dir, train_dataset):\n",
    "    # Set the seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    # Check for device (GPU or CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define the single generator and discriminator\n",
    "    generator = Generator(z_dim, audio_length).to(device)\n",
    "    discriminator = Discriminator(audio_length).to(device)\n",
    "\n",
    "    optimizer_gen = optim.Adam(generator.parameters(), lr=lr_gen, betas=(0.5, 0.9))\n",
    "    optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "    lambda_gp = 10  # Gradient penalty coefficient\n",
    "    num_critic = 5  # Number of discriminator updates per generator update\n",
    "\n",
    "    # To track the losses\n",
    "    loss_disc_history = []\n",
    "    loss_gen_history = []\n",
    "\n",
    "    # Resume training if checkpoints exist\n",
    "    checkpoint_path = os.path.join(output_dir, \"checkpoint.pth\")\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        logging.info(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        optimizer_gen.load_state_dict(checkpoint['optimizer_gen_state_dict'])\n",
    "        optimizer_disc.load_state_dict(checkpoint['optimizer_disc_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        loss_disc_history = checkpoint['loss_disc_history']\n",
    "        loss_gen_history = checkpoint['loss_gen_history']\n",
    "        logging.info(f\"Resumed from epoch {start_epoch}\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        torch.cuda.empty_cache()  # Clear unused memory\n",
    "        loss_disc_epoch = 0\n",
    "        loss_gen_epoch = 0\n",
    "\n",
    "        for batch_idx, real in enumerate(train_loader):\n",
    "            real = real.to(device)\n",
    "            batch_size = real.size(0)\n",
    "            if real.dim() == 2:\n",
    "                real = real.unsqueeze(1)  # Add channel dimension (batch_size, 1, audio_length)\n",
    "\n",
    "            #logging.info(f\"real shape: {real.shape}\")\n",
    "\n",
    "            # Train Discriminator multiple times\n",
    "            for _ in range(num_critic):\n",
    "                optimizer_disc.zero_grad()\n",
    "\n",
    "                # Generate fake data\n",
    "                noise = generate_noise(batch_size, z_dim, device)\n",
    "                fake = generator(noise).detach()\n",
    "\n",
    "                # Compute discriminator outputs\n",
    "                disc_real = discriminator(real)\n",
    "                disc_fake = discriminator(fake)\n",
    "\n",
    "                # Compute Wasserstein loss\n",
    "                loss_disc_real = -torch.mean(disc_real)\n",
    "                loss_disc_fake = torch.mean(disc_fake)\n",
    "                loss_disc = loss_disc_real + loss_disc_fake\n",
    "\n",
    "                # Compute gradient penalty\n",
    "                gradient_penalty = compute_gradient_penalty(discriminator, real.data, fake.data, device)\n",
    "                loss_disc += lambda_gp * gradient_penalty\n",
    "\n",
    "                loss_disc.backward()\n",
    "                optimizer_disc.step()\n",
    "\n",
    "            loss_disc_epoch += loss_disc.item()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_gen.zero_grad()\n",
    "            noise = generate_noise(batch_size, z_dim, device)\n",
    "            fake = generator(noise)\n",
    "            disc_fake = discriminator(fake)\n",
    "            loss_gen = -torch.mean(disc_fake)\n",
    "            loss_gen.backward()\n",
    "            optimizer_gen.step()\n",
    "\n",
    "            loss_gen_epoch += loss_gen.item()\n",
    "\n",
    "        avg_loss_disc = loss_disc_epoch / len(train_loader)\n",
    "        avg_loss_gen = loss_gen_epoch / len(train_loader)\n",
    "\n",
    "        # Record the losses\n",
    "        loss_disc_history.append(avg_loss_disc)\n",
    "        loss_gen_history.append(avg_loss_gen)\n",
    "\n",
    "        logging.info(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {avg_loss_disc:.4f}, Loss G: {avg_loss_gen:.4f}\")\n",
    "\n",
    "        # Visualize generated waveforms\n",
    "        visualize_and_save_generated_waveforms(\n",
    "            [generator], z_dim, num_waveforms=5, device=device, epoch=epoch, sample_rate=16000, output_dir=output_dir\n",
    "        )\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_gen_state_dict': optimizer_gen.state_dict(),\n",
    "            'optimizer_disc_state_dict': optimizer_disc.state_dict(),\n",
    "            'loss_disc_history': loss_disc_history,\n",
    "            'loss_gen_history': loss_gen_history,\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    # Save the generator model\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    torch.save(generator.state_dict(), os.path.join(output_dir, \"pretrained_generator.pth\"))\n",
    "    logging.info(f\"Pretrained generator model saved to {os.path.join(output_dir, 'pretrained_generator.pth')}\")\n",
    "\n",
    "    # Plot the learning curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(loss_disc_history, label=\"Discriminator Loss\")\n",
    "    plt.plot(loss_gen_history, label=\"Generator Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Learning Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, \"learning_curves.png\"))\n",
    "    plt.show()\n",
    "\n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import os\n",
    "from torch.nn.utils import spectral_norm\n",
    "import torch.autograd as autograd\n",
    "# Wasserstein loss\n",
    "def wasserstein_loss(y_pred, y_true):\n",
    "    return torch.mean(y_pred * y_true)\n",
    "\n",
    "def compute_gradient_penalty(discriminator, real_data, fake_data, device):\n",
    "    batch_size = real_data.size(0)\n",
    "    # Corrected epsilon shape to match real_data dimensions\n",
    "    epsilon = torch.rand(batch_size, 1, 1, device=device)  # Shape: [batch_size, 1, 1]\n",
    "    epsilon = epsilon.expand_as(real_data)  # Now expands to [batch_size, 1, audio_length]\n",
    "    \n",
    "    # Interpolate between real and fake data\n",
    "    interpolates = epsilon * real_data + (1 - epsilon) * fake_data\n",
    "    interpolates = interpolates.requires_grad_(True)\n",
    "    \n",
    "    # Compute discriminator output\n",
    "    disc_interpolates = discriminator(interpolates)\n",
    "    \n",
    "    # Compute gradients\n",
    "    grad_outputs = torch.ones_like(disc_interpolates)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=disc_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=grad_outputs,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    # Reshape gradients\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "def train_gan_with_pretrained_generators(\n",
    "    pretrained_generator, num_epochs, z_dim, lr_gen, lr_disc, batch_size, train_dataset,\n",
    "    num_generators, seed, audio_length, output_dir, lambda_gp=10, lambda_ortho=0.1, num_critic=5,\n",
    "    checkpoint_dir='checkpoints', resume=True\n",
    "):\n",
    "    import os\n",
    "\n",
    "    # Set the seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize multiple generators with the pretrained generator\n",
    "    generators = []\n",
    "    for _ in range(num_generators):\n",
    "        gen = Generator(z_dim, audio_length).to(device)\n",
    "        gen.load_state_dict(pretrained_generator.state_dict())\n",
    "        generators.append(gen)\n",
    "\n",
    "    # Initialize Discriminator\n",
    "    discriminator = Discriminator(audio_length).to(device)\n",
    "\n",
    "    optimizer_gens = [optim.Adam(gen.parameters(), lr=lr_gen, betas=(0.5, 0.9)) for gen in generators]\n",
    "    optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "    # Initialize Encoder only if lambda_ortho > 0\n",
    "    if lambda_ortho > 0:\n",
    "        encoder = Encoder(audio_length).to(device)\n",
    "        optimizer_encoder = optim.Adam(encoder.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "    else:\n",
    "        encoder = None\n",
    "        optimizer_encoder = None\n",
    "\n",
    "    # Load and preprocess\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Check for existing checkpoint\n",
    "    start_epoch = 0\n",
    "    if resume and os.path.exists(os.path.join(checkpoint_dir, 'checkpoint.pth')):\n",
    "        logging.info(\"Resuming from checkpoint...\")\n",
    "        checkpoint = torch.load(os.path.join(checkpoint_dir, 'checkpoint.pth'), map_location=device)\n",
    "        start_epoch = checkpoint['epoch'] + 1  # Next epoch to start from\n",
    "\n",
    "        # Load models\n",
    "        for idx, gen in enumerate(generators):\n",
    "            gen.load_state_dict(checkpoint['generator_state_dicts'][idx])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        if lambda_ortho > 0:\n",
    "            encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "\n",
    "        # Load optimizers\n",
    "        for idx, optimizer_gen in enumerate(optimizer_gens):\n",
    "            optimizer_gen.load_state_dict(checkpoint['optimizer_gens_state_dicts'][idx])\n",
    "        optimizer_disc.load_state_dict(checkpoint['optimizer_disc_state_dict'])\n",
    "        if lambda_ortho > 0:\n",
    "            optimizer_encoder.load_state_dict(checkpoint['optimizer_encoder_state_dict'])\n",
    "\n",
    "        # Load loss histories\n",
    "        loss_disc_history = checkpoint['loss_disc_history']\n",
    "        loss_gens_history = checkpoint['loss_gens_history']\n",
    "\n",
    "    else:\n",
    "        logging.info(\"Starting training from scratch.\")\n",
    "        start_epoch = 0\n",
    "        # Initialize loss histories\n",
    "        loss_disc_history = []\n",
    "        loss_gens_history = [[] for _ in range(num_generators)]\n",
    "\n",
    "    # Training loop\n",
    "    try:\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            torch.cuda.empty_cache()\n",
    "            loss_disc_epoch = 0\n",
    "            loss_gens_epoch = [0] * num_generators\n",
    "\n",
    "            for batch_idx, (real,) in enumerate(train_loader):\n",
    "                real = real.to(device)\n",
    "                if real.dim() == 2:\n",
    "                    real = real.unsqueeze(1)  # Add channel dimension (batch_size, 1, audio_length)\n",
    "                batch_size = real.size(0)\n",
    "                real_label = -torch.ones(batch_size, 1, device=device)\n",
    "                fake_label = torch.ones(batch_size, 1, device=device)\n",
    "\n",
    "                real = real + 0.001 * torch.randn_like(real)\n",
    "\n",
    "                # Train Discriminator multiple times\n",
    "                for _ in range(num_critic):\n",
    "                    optimizer_disc.zero_grad()\n",
    "\n",
    "                    disc_real = discriminator(real)\n",
    "\n",
    "                    noises = [generate_noise(batch_size, z_dim, device) for _ in range(num_generators)]\n",
    "                    fakes = [gen(noises[idx]).detach() for idx, gen in enumerate(generators)]\n",
    "\n",
    "                    for idx in range(num_generators):\n",
    "                        fakes[idx] = fakes[idx] + 0.001 * torch.randn_like(fakes[idx])\n",
    "\n",
    "                    disc_fakes = [discriminator(fake) for fake in fakes]\n",
    "\n",
    "                    # Average the fake losses\n",
    "                    loss_disc_fake = sum(wasserstein_loss(disc_fake, fake_label) for disc_fake in disc_fakes) / num_generators\n",
    "                    loss_disc_real = wasserstein_loss(disc_real, real_label)\n",
    "                    loss_disc = loss_disc_real + loss_disc_fake\n",
    "\n",
    "                    # Average gradient penalty\n",
    "                    gradient_penalty = sum(\n",
    "                        compute_gradient_penalty(discriminator, real, fake, device) for fake in fakes\n",
    "                    ) / num_generators\n",
    "                    loss_disc += lambda_gp * gradient_penalty\n",
    "\n",
    "                    loss_disc.backward()\n",
    "                    optimizer_disc.step()\n",
    "\n",
    "                loss_disc_epoch += loss_disc.item()\n",
    "\n",
    "                # Train Generators\n",
    "                for idx, gen in enumerate(generators):\n",
    "                    optimizer_gens[idx].zero_grad()\n",
    "                    if lambda_ortho > 0:\n",
    "                        optimizer_encoder.zero_grad()\n",
    "\n",
    "                    noise = generate_noise(batch_size, z_dim, device)\n",
    "                    fake = gen(noise)\n",
    "                    disc_fake = discriminator(fake)\n",
    "\n",
    "                    loss_gen = wasserstein_loss(disc_fake, real_label)\n",
    "\n",
    "                    if lambda_ortho > 0:\n",
    "                        # Compute orthogonal loss\n",
    "                        gen_feature = encoder(fake)\n",
    "                        ortho_loss_total = 0\n",
    "                        for other_idx, other_gen in enumerate(generators):\n",
    "                            if idx != other_idx:\n",
    "                                other_noise = generate_noise(batch_size, z_dim, device)\n",
    "                                other_fake = other_gen(other_noise)\n",
    "                                other_feature = encoder(other_fake)\n",
    "                                ortho_loss = orthogonal_loss(gen_feature, other_feature)\n",
    "                                ortho_loss_total += ortho_loss\n",
    "\n",
    "                        ortho_loss_total /= (num_generators - 1)\n",
    "                        total_loss_gen = loss_gen + lambda_ortho * ortho_loss_total\n",
    "                    else:\n",
    "                        total_loss_gen = loss_gen\n",
    "\n",
    "                    total_loss_gen.backward()\n",
    "                    optimizer_gens[idx].step()\n",
    "\n",
    "                    if lambda_ortho > 0:\n",
    "                        optimizer_encoder.step()\n",
    "\n",
    "                    loss_gens_epoch[idx] += total_loss_gen.item()\n",
    "\n",
    "            avg_loss_disc = loss_disc_epoch / len(train_loader)\n",
    "            avg_loss_gens = [loss / len(train_loader) for loss in loss_gens_epoch]\n",
    "\n",
    "            logging.info(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {avg_loss_disc:.4f}\")\n",
    "            for idx in range(num_generators):\n",
    "                logging.info(f\"Loss G{idx+1}: {avg_loss_gens[idx]:.4f}\")\n",
    "            logging.info('-' * 50)\n",
    "\n",
    "            # Record the losses\n",
    "            loss_disc_history.append(avg_loss_disc)\n",
    "            for idx in range(num_generators):\n",
    "                loss_gens_history[idx].append(avg_loss_gens[idx])\n",
    "\n",
    "            # Visualize generated waveforms\n",
    "            visualize_and_save_generated_waveforms(\n",
    "                generators, z_dim, num_waveforms=5, device=device, epoch=epoch, sample_rate=16000, output_dir=output_dir\n",
    "            )\n",
    "\n",
    "            # Save checkpoint after each epoch\n",
    "            checkpoint = {\n",
    "                'epo  ch': epoch,\n",
    "                'generator_state_dicts': [gen.state_dict() for gen in generators],\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_gens_state_dicts': [opt.state_dict() for opt in optimizer_gens],\n",
    "                'optimizer_disc_state_dict': optimizer_disc.state_dict(),\n",
    "                'loss_disc_history': loss_disc_history,\n",
    "                'loss_gens_history': loss_gens_history\n",
    "            }\n",
    "            if lambda_ortho > 0:\n",
    "                checkpoint['encoder_state_dict'] = encoder.state_dict()\n",
    "                checkpoint['optimizer_encoder_state_dict'] = optimizer_encoder.state_dict()\n",
    "\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            torch.save(checkpoint, os.path.join(checkpoint_dir, 'checkpoint.pth'))\n",
    "            logging.info(f\"Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logging.info(f\"Training interrupted at epoch {epoch}. Saving checkpoint...\")\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dicts': [gen.state_dict() for gen in generators],\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_gens_state_dicts': [opt.state_dict() for opt in optimizer_gens],\n",
    "            'optimizer_disc_state_dict': optimizer_disc.state_dict(),\n",
    "            'loss_disc_history': loss_disc_history,\n",
    "            'loss_gens_history': loss_gens_history\n",
    "        }\n",
    "        if lambda_ortho > 0:\n",
    "            checkpoint['encoder_state_dict'] = encoder.state_dict()\n",
    "            checkpoint['optimizer_encoder_state_dict'] = optimizer_encoder.state_dict()\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, 'checkpoint.pth'))\n",
    "        logging.info(f\"Checkpoint saved at epoch {epoch}\")\n",
    "        logging.info(\"Exiting training early.\")\n",
    "        return generators\n",
    "\n",
    "    return generators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_multiple_generators(pretrained_generator, num_generators, z_dim):\n",
    "    # Initialize multiple generators from the pretrained generator's weights\n",
    "    generators = []\n",
    "    for _ in range(num_generators):\n",
    "        new_generator = Generator(z_dim).to(pretrained_generator.gen[0].weight.device)  # Ensure same device\n",
    "        new_generator.load_state_dict(pretrained_generator.state_dict())  # Copy weights\n",
    "        generators.append(new_generator)\n",
    "    return generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/tameszaza/My Passport/ml/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pretrained_generator \u001b[38;5;241m=\u001b[39m \u001b[43mpretrain_single_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mz_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_disc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0002\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlr_gen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0002\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwaveform_pre_500\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 67\u001b[0m, in \u001b[0;36mpretrain_single_generator\u001b[0;34m(num_epochs, z_dim, lr_gen, lr_disc, batch_size, seed, audio_length, output_dir, train_dataset)\u001b[0m\n\u001b[1;32m     64\u001b[0m optimizer_disc\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Generate fake data\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m noise \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m fake \u001b[38;5;241m=\u001b[39m generator(noise)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Compute discriminator outputs\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m, in \u001b[0;36mgenerate_noise\u001b[0;34m(batch_size, z_dim, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_noise\u001b[39m(batch_size, z_dim, device):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pretrained_generator = pretrain_single_generator(num_epochs=20, \n",
    "                                                 z_dim=100, lr_disc=0.0002,lr_gen=0.0002, batch_size=64, seed=42, audio_length=64000, \n",
    "                                                 output_dir='waveform_pre_500', train_dataset=train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_generator(z_dim, weight_path, device, audio_length):\n",
    "    \"\"\"\n",
    "    Load the pretrained generator model weights.\n",
    "\n",
    "    Args:\n",
    "        z_dim (int): Latent space dimension.\n",
    "        weight_path (str): Path to the saved model weights.\n",
    "        device (torch.device): Device to load the model onto.\n",
    "\n",
    "    Returns:\n",
    "        Generator: Loaded generator model.\n",
    "    \"\"\"\n",
    "    generator = Generator(z_dim, audio_length=audio_length).to(device)\n",
    "    generator.load_state_dict(torch.load(weight_path, map_location=device))\n",
    "    generator.eval()  # Set the model to evaluation mode\n",
    "    logging.info(f\"Pretrained generator model loaded from {weight_path}\")\n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Clear unused CUDA memory\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accual train code (resumable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan_with_pretrained_generators(\n",
    "    pretrained_generator,\n",
    "    num_epochs=200,\n",
    "    z_dim=100,\n",
    "    lr_gen=0.0002,\n",
    "    lr_disc=0.0002,\n",
    "    batch_size=24,\n",
    "    train_dataset=dataset,\n",
    "    num_generators=5,\n",
    "    seed=42,\n",
    "    audio_length=64000,\n",
    "    output_dir='waveform_mal_2',\n",
    "    checkpoint_dir='my_checkpoints',\n",
    "    resume=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
