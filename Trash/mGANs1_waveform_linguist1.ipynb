{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  orthogonal loss \n",
    "This model work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from glob import glob\n",
    "from phonemizer import phonemize\n",
    "from torch.nn.utils import spectral_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a FLAC audio file and resample it to the target sample rate\n",
    "def load_flac(file_path, target_sr=16000):\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
    "    max_val = np.max(np.abs(audio))\n",
    "    if max_val > 0:\n",
    "        audio = audio / max_val  # Normalize to [-1, 1]\n",
    "    return audio\n",
    "\n",
    "# Pad or trim audio to the target length\n",
    "def pad_or_trim(audio, target_length=64000):\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:target_length]\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the transcription for a given audio file from the corresponding .trans.txt file\n",
    "def get_transcription(file_path):\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "    base_name = os.path.basename(file_path)\n",
    "    file_id = os.path.splitext(base_name)[0]\n",
    "\n",
    "    # Locate the transcription file\n",
    "    transcription_file = None\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith(\".trans.txt\"):\n",
    "            transcription_file = os.path.join(dir_path, file)\n",
    "            break\n",
    "\n",
    "    if not transcription_file:\n",
    "        raise FileNotFoundError(f\"No transcription file found in {dir_path}\")\n",
    "\n",
    "    # Read the transcription file and find the transcription for the current audio file\n",
    "    with open(transcription_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\" \", 1)\n",
    "            if parts[0] == file_id:\n",
    "                transcription = parts[1]\n",
    "                return transcription\n",
    "\n",
    "    raise ValueError(f\"No transcription found for file {file_id}\")\n",
    "\n",
    "# Extract phonetic features from transcription\n",
    "def get_phonetic_features(transcription, max_length=100):\n",
    "    phonemes = phonemize(transcription, backend=\"espeak\", language=\"en-us\")\n",
    "    phoneme_to_id = {char: idx for idx, char in enumerate(sorted(set(phonemes)))}\n",
    "    phonetic_features = [phoneme_to_id[p] for p in phonemes]\n",
    "\n",
    "    # Convert to tensor and pad/truncate\n",
    "    phonetic_features = torch.tensor(phonetic_features, dtype=torch.float32)\n",
    "    if len(phonetic_features) < max_length:\n",
    "        phonetic_features = nn.functional.pad(phonetic_features, (0, max_length - len(phonetic_features)))\n",
    "    else:\n",
    "        phonetic_features = phonetic_features[:max_length]\n",
    "    return phonetic_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset and create a TensorDataset\n",
    "def preprocess_dataset(root_dir, target_sr=16000, target_length=64000, feature_length=100):\n",
    "    flac_files = glob(os.path.join(root_dir, '**', '*.flac'), recursive=True)\n",
    "    print(f\"Found {len(flac_files)} .flac files in {root_dir}.\")\n",
    "    if len(flac_files) == 0:\n",
    "        print(\"No .flac files found. Please check the root_dir path.\")\n",
    "    audio_dataset = []\n",
    "    feature_dataset = []\n",
    "    for file in flac_files:\n",
    "        try:\n",
    "            audio = load_flac(file, target_sr)\n",
    "            audio = pad_or_trim(audio, target_length)\n",
    "            transcription = get_transcription(file)\n",
    "            phonetic_features = get_phonetic_features(transcription, max_length=feature_length)\n",
    "            audio_dataset.append(audio)\n",
    "            feature_dataset.append(phonetic_features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "    audio_dataset = torch.tensor(audio_dataset, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
    "    feature_dataset = torch.stack(feature_dataset)  # Stack tensors\n",
    "    return TensorDataset(audio_dataset, feature_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a waveform to an audio file\n",
    "def save_waveform_to_audio(waveform, sample_rate, filename):\n",
    "    if isinstance(waveform, torch.Tensor):\n",
    "        waveform = waveform.detach().cpu().numpy()\n",
    "    waveform = np.squeeze(waveform)\n",
    "    max_val = np.max(np.abs(waveform))\n",
    "    if max_val > 0:\n",
    "        waveform = waveform / max_val\n",
    "    sf.write(filename, waveform, sample_rate)\n",
    "\n",
    "# Verify waveform-to-audio conversion using preprocessed dataset\n",
    "def verify_waveform_to_audio(root_dir, sample_rate=16000, target_length=64000, output_dir=\"verified_audio\"):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    dataset = preprocess_dataset(root_dir, sample_rate, target_length)\n",
    "    num_samples_to_verify = min(5, len(dataset))\n",
    "    for idx in range(num_samples_to_verify):\n",
    "        waveform = dataset[idx][0]  # Access audio data\n",
    "        filename = os.path.join(output_dir, f\"example_waveform_{idx+1}.wav\")\n",
    "        save_waveform_to_audio(waveform, sample_rate, filename)\n",
    "        print(f\"Waveform saved to {filename}\")\n",
    "        # Plot the waveform\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(waveform.numpy().squeeze())\n",
    "        plt.title(f\"Waveform {idx+1}\")\n",
    "        plt.xlabel(\"Sample Index\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate noise for the generator\n",
    "def generate_noise(batch_size, z_dim, device):\n",
    "    return torch.randn(batch_size, z_dim).to(device)\n",
    "\n",
    "# Orthogonal loss function\n",
    "def orthogonal_loss(feature1, feature2):\n",
    "    inner_product = torch.sum(feature1 * feature2, dim=1)\n",
    "    norm1 = torch.norm(feature1, dim=1)\n",
    "    norm2 = torch.norm(feature2, dim=1)\n",
    "    cosine_similarity = inner_product / (norm1 * norm2 + 1e-8)\n",
    "    return torch.mean(cosine_similarity**2)  # Minimize the cosine similarity to make vectors orthogonal\n",
    "\n",
    "# Compute gradient penalty for WGAN-GP\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, device):\n",
    "    batch_size = real_samples.size(0)\n",
    "    epsilon = torch.rand(batch_size, 1, 1, device=device)\n",
    "    epsilon = epsilon.expand_as(real_samples)\n",
    "    interpolates = (epsilon * real_samples + (1 - epsilon) * fake_samples).requires_grad_(True)\n",
    "    interpolates_output = discriminator(interpolates)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=interpolates_output,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(interpolates_output),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize and save generated waveforms\n",
    "def visualize_and_save_generated_waveforms(generators, z_dim, features, num_waveforms, device, epoch, sample_rate=16000, output_dir='generated_audio'):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for idx, gen in enumerate(generators):\n",
    "        gen.eval()\n",
    "        with torch.no_grad():\n",
    "            noise = generate_noise(num_waveforms, z_dim, device)\n",
    "            fake_waveforms = gen(features[:num_waveforms].to(device), noise).cpu()\n",
    "            for i in range(num_waveforms):\n",
    "                waveform = fake_waveforms[i]\n",
    "                # Save each waveform to an audio file\n",
    "                filename = f'epoch{epoch+1}_gen{idx+1}_sample{i+1}.wav'\n",
    "                filepath = os.path.join(output_dir, filename)\n",
    "                save_waveform_to_audio(waveform, sample_rate, filepath)\n",
    "                print(f\"Saved {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator architecture using Conv1d layers\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=100, z_channels=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.z_channels = z_channels\n",
    "\n",
    "        self.preprocess = nn.Conv1d(in_channels, 768, kernel_size=3, padding=1)\n",
    "        self.gblocks = nn.ModuleList([\n",
    "            GBlock(768, 768, z_channels, 1),\n",
    "            GBlock(768, 768, z_channels, 1),\n",
    "            GBlock(768, 384, z_channels, 2),\n",
    "            GBlock(384, 384, z_channels, 2),\n",
    "            GBlock(384, 384, z_channels, 2),\n",
    "            GBlock(384, 192, z_channels, 3),\n",
    "            GBlock(192, 96, z_channels, 5)\n",
    "        ])\n",
    "        self.postprocess = nn.Sequential(\n",
    "            nn.Conv1d(96, 1, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, z):\n",
    "        inputs = self.preprocess(inputs)\n",
    "        outputs = inputs\n",
    "        for layer in self.gblocks:\n",
    "            outputs = layer(outputs, z)\n",
    "        outputs = self.postprocess(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBlock(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, z_channels, upsample_factor):\n",
    "        super(GBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.z_channels = z_channels\n",
    "        self.upsample_factor = upsample_factor\n",
    "\n",
    "        self.condition_batchnorm1 = ConditionalBatchNorm1d(in_channels, z_channels)\n",
    "        self.first_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            UpsampleNet(in_channels, in_channels, upsample_factor),\n",
    "            nn.Conv1d(in_channels, hidden_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.condition_batchnorm2 = ConditionalBatchNorm1d(hidden_channels, z_channels)\n",
    "        self.second_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        )\n",
    "        self.residual1 = nn.Sequential(\n",
    "            UpsampleNet(in_channels, in_channels, upsample_factor),\n",
    "            nn.Conv1d(in_channels, hidden_channels, kernel_size=1)\n",
    "        )\n",
    "        self.condition_batchnorm3 = ConditionalBatchNorm1d(hidden_channels, z_channels)\n",
    "        self.third_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, dilation=4, padding=4)\n",
    "        )\n",
    "        self.condition_batchnorm4 = ConditionalBatchNorm1d(hidden_channels, z_channels)\n",
    "        self.fourth_stack = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, dilation=8, padding=8)\n",
    "        )\n",
    "\n",
    "    def forward(self, condition, z):\n",
    "        inputs = condition\n",
    "        outputs = self.condition_batchnorm1(inputs, z)\n",
    "        outputs = self.first_stack(outputs)\n",
    "        outputs = self.condition_batchnorm2(outputs, z)\n",
    "        outputs = self.second_stack(outputs)\n",
    "        residual_outputs = self.residual1(inputs) + outputs\n",
    "        outputs = self.condition_batchnorm3(residual_outputs, z)\n",
    "        outputs = self.third_stack(outputs)\n",
    "        outputs = self.condition_batchnorm4(outputs, z)\n",
    "        outputs = self.fourth_stack(outputs)\n",
    "        outputs = outputs + residual_outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, upsample_factor):\n",
    "        super(UpsampleNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.upsample_factor = upsample_factor\n",
    "\n",
    "        layer = nn.ConvTranspose1d(input_size, output_size, upsample_factor * 2,\n",
    "                                   upsample_factor, padding=upsample_factor // 2)\n",
    "        nn.init.orthogonal_(layer.weight)\n",
    "        self.layer = spectral_norm(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.layer(inputs)\n",
    "        outputs = outputs[:, :, : inputs.size(-1) * self.upsample_factor]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalBatchNorm1d(nn.Module):\n",
    "    \"\"\"Conditional Batch Normalization\"\"\"\n",
    "    def __init__(self, num_features, z_channels=128):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.z_channels = z_channels\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features, affine=False)\n",
    "        self.layer = spectral_norm(nn.Linear(z_channels, num_features * 2))\n",
    "        self.layer.weight.data.normal_(1, 0.02)\n",
    "        self.layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, inputs, noise):\n",
    "        outputs = self.batch_norm(inputs)\n",
    "        gamma, beta = self.layer(noise).chunk(2, 1)\n",
    "        gamma = gamma.view(-1, self.num_features, 1)\n",
    "        beta = beta.view(-1, self.num_features, 1)\n",
    "        outputs = gamma * outputs + beta\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator architecture using Conv1d layers\n",
    "class Multiple_Random_Window_Discriminators(nn.Module):\n",
    "    def __init__(self, lc_channels, window_size=(2, 4, 8, 16, 30), upsample_factor=120):\n",
    "        super(Multiple_Random_Window_Discriminators, self).__init__()\n",
    "        self.lc_channels = lc_channels\n",
    "        self.window_size = window_size\n",
    "        self.upsample_factor = upsample_factor\n",
    "\n",
    "        self.udiscriminators = nn.ModuleList([\n",
    "            UnConditionalDBlocks(in_channels=1, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=2, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=4, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=8, factors=(5, 3), out_channels=(128, 256)),\n",
    "            UnConditionalDBlocks(in_channels=15, factors=(2, 2), out_channels=(128, 256)),\n",
    "        ])\n",
    "\n",
    "        self.discriminators = nn.ModuleList([\n",
    "            ConditionalDBlocks(in_channels=1, lc_channels=lc_channels,\n",
    "                               factors=(5, 3, 2, 2, 2), out_channels=(128, 128, 256, 256)),\n",
    "            ConditionalDBlocks(in_channels=2, lc_channels=lc_channels,\n",
    "                               factors=(5, 3, 2, 2), out_channels=(128, 256, 256)),\n",
    "            ConditionalDBlocks(in_channels=4, lc_channels=lc_channels,\n",
    "                               factors=(5, 3, 2), out_channels=(128, 256)),\n",
    "            ConditionalDBlocks(in_channels=8, lc_channels=lc_channels,\n",
    "                               factors=(5, 3), out_channels=(256,)),\n",
    "            ConditionalDBlocks(in_channels=15, lc_channels=lc_channels,\n",
    "                               factors=(2, 2, 2), out_channels=(128, 256)),\n",
    "        ])\n",
    "\n",
    "    def forward(self, real_samples, fake_samples, conditions):\n",
    "        real_outputs, fake_outputs = [], []\n",
    "        # Unconditional discriminator\n",
    "        for (size, layer) in zip(self.window_size, self.udiscriminators):\n",
    "            size = size * self.upsample_factor\n",
    "            index = np.random.randint(0, real_samples.size(-1) - size + 1)\n",
    "            real_output = layer(real_samples[:, :, index: index + size])\n",
    "            real_outputs.append(real_output)\n",
    "            fake_output = layer(fake_samples[:, :, index: index + size])\n",
    "            fake_outputs.append(fake_output)\n",
    "        # Conditional discriminator\n",
    "        for (size, layer) in zip(self.window_size, self.discriminators):\n",
    "            lc_index = np.random.randint(0, conditions.size(-1) - size + 1)\n",
    "            sample_index = lc_index * self.upsample_factor\n",
    "            real_x = real_samples[:, :, sample_index: (lc_index + size) * self.upsample_factor]\n",
    "            fake_x = fake_samples[:, :, sample_index: (lc_index + size) * self.upsample_factor]\n",
    "            lc = conditions[:, :, lc_index: lc_index + size]\n",
    "            real_output = layer(real_x, lc)\n",
    "            real_outputs.append(real_output)\n",
    "            fake_output = layer(fake_x, lc)\n",
    "            fake_outputs.append(fake_output)\n",
    "        return real_outputs, fake_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample_factor):\n",
    "        super(DBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.AvgPool1d(downsample_factor, stride=downsample_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        )\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1),\n",
    "            nn.AvgPool1d(downsample_factor, stride=downsample_factor)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.layers(inputs) + self.residual(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondDBlock(nn.Module):\n",
    "    def __init__(self, in_channels, lc_channels, downsample_factor):\n",
    "        super(CondDBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.lc_channels = lc_channels\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.start = nn.Sequential(\n",
    "            nn.AvgPool1d(downsample_factor, stride=downsample_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels, in_channels * 2, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.lc_conv1d = nn.Conv1d(lc_channels, in_channels * 2, kernel_size=1)\n",
    "        self.end = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels * 2, in_channels * 2, kernel_size=3, dilation=2, padding=2)\n",
    "        )\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, in_channels * 2, kernel_size=1),\n",
    "            nn.AvgPool1d(downsample_factor, stride=downsample_factor)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, conditions):\n",
    "        outputs = self.start(inputs) + self.lc_conv1d(conditions)\n",
    "        outputs = self.end(outputs)\n",
    "        residual_outputs = self.residual(inputs)\n",
    "        outputs = outputs + residual_outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalDBlocks(nn.Module):\n",
    "    def __init__(self, in_channels, lc_channels, factors=(2, 2, 2), out_channels=(128, 256)):\n",
    "        super(ConditionalDBlocks, self).__init__()\n",
    "        assert len(factors) == len(out_channels) + 1\n",
    "        self.in_channels = in_channels\n",
    "        self.lc_channels = lc_channels\n",
    "        self.factors = factors\n",
    "        self.out_channels = out_channels\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(DBlock(in_channels, 64, 1))\n",
    "        in_channels = 64\n",
    "        for (i, channel) in enumerate(out_channels):\n",
    "            self.layers.append(DBlock(in_channels, channel, factors[i]))\n",
    "            in_channels = channel\n",
    "        self.cond_layer = CondDBlock(in_channels, lc_channels, factors[-1])\n",
    "        self.post_process = nn.ModuleList([\n",
    "            DBlock(in_channels * 2, in_channels * 2, 1),\n",
    "            DBlock(in_channels * 2, in_channels * 2, 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, inputs, conditions):\n",
    "        batch_size = inputs.size()[0]\n",
    "        outputs = inputs.view(batch_size, self.in_channels, -1)\n",
    "        for layer in self.layers:\n",
    "            outputs = layer(outputs)\n",
    "        outputs = self.cond_layer(outputs, conditions)\n",
    "        for layer in self.post_process:\n",
    "            outputs = layer(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnConditionalDBlocks(nn.Module):\n",
    "    def __init__(self, in_channels, factors=(5, 3), out_channels=(128, 256)):\n",
    "        super(UnConditionalDBlocks, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.factors = factors\n",
    "        self.out_channels = out_channels\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(DBlock(in_channels, 64, 1))\n",
    "        in_channels = 64\n",
    "        for (i, factor) in enumerate(factors):\n",
    "            self.layers.append(DBlock(in_channels, out_channels[i], factor))\n",
    "            in_channels = out_channels[i]\n",
    "        self.layers.append(DBlock(in_channels, in_channels, 1))\n",
    "        self.layers.append(DBlock(in_channels, in_channels, 1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size()[0]\n",
    "        outputs = inputs.view(batch_size, self.in_channels, -1)\n",
    "        for layer in self.layers:\n",
    "            outputs = layer(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan_with_pretrained_generators(\n",
    "    pretrained_generator, num_epochs, z_dim, lr_gen, lr_disc, batch_size, train_dataset,\n",
    "    num_generators, seed, audio_length, output_dir, save_dir, lambda_gp=10, lambda_ortho=0.1, num_critic=5,\n",
    "    checkpoint_dir='checkpoints', resume=True\n",
    "):\n",
    "    set_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize multiple generators with the pretrained generator\n",
    "    generators = []\n",
    "    for _ in range(num_generators):\n",
    "        gen = Generator(in_channels=train_dataset[0][1].shape[0], z_channels=z_dim).to(device)\n",
    "        gen.load_state_dict(pretrained_generator.state_dict())\n",
    "        generators.append(gen)\n",
    "\n",
    "    # Initialize Discriminator\n",
    "    discriminator = Multiple_Random_Window_Discriminators(lc_channels=train_dataset[0][1].shape[0]).to(device)\n",
    "\n",
    "    optimizer_gens = [optim.Adam(gen.parameters(), lr=lr_gen, betas=(0.5, 0.9)) for gen in generators]\n",
    "    optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "    # Load and preprocess the audio dataset\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Training loop\n",
    "    try:\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            torch.cuda.empty_cache()\n",
    "            for batch_idx, (real_audio, features) in enumerate(train_loader):\n",
    "                real_audio = real_audio.to(device)\n",
    "                features = features.unsqueeze(1).to(device)  # Add channel dimension if necessary\n",
    "                batch_size = real_audio.size(0)\n",
    "\n",
    "                # Train Discriminator\n",
    "                for _ in range(num_critic):\n",
    "                    optimizer_disc.zero_grad()\n",
    "                    disc_real_outputs, disc_fake_outputs = [], []\n",
    "                    noises = [generate_noise(batch_size, z_dim, device) for _ in range(num_generators)]\n",
    "                    fakes = [gen(features, noises[idx]) for idx, gen in enumerate(generators)]\n",
    "                    real_outputs, fake_outputs = discriminator(real_audio, torch.stack(fakes), features)\n",
    "                    loss_disc = sum([torch.mean(fake) - torch.mean(real) for real, fake in zip(real_outputs, fake_outputs)])\n",
    "                    gradient_penalty = compute_gradient_penalty(discriminator, real_audio, fakes[0], device)\n",
    "                    loss_disc += lambda_gp * gradient_penalty\n",
    "                    loss_disc.backward()\n",
    "                    optimizer_disc.step()\n",
    "\n",
    "                # Train Generators\n",
    "                for idx, gen in enumerate(generators):\n",
    "                    optimizer_gens[idx].zero_grad()\n",
    "                    noise = generate_noise(batch_size, z_dim, device)\n",
    "                    fake = gen(features, noise)\n",
    "                    fake_outputs = discriminator(fake, fake, features)[1]\n",
    "                    loss_gen = -torch.mean(torch.stack(fake_outputs))\n",
    "                    loss_gen.backward()\n",
    "                    optimizer_gens[idx].step()\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {loss_disc.item():.4f}, Loss G: {loss_gen.item():.4f}\")\n",
    "\n",
    "            # Save generated samples and models\n",
    "            visualize_and_save_generated_waveforms(\n",
    "                generators, z_dim, features, num_waveforms=5, device=device, epoch=epoch, sample_rate=16000, output_dir=output_dir\n",
    "            )\n",
    "            for idx, gen in enumerate(generators):\n",
    "                torch.save(gen.state_dict(), os.path.join(output_dir, f\"generator_{idx}_epoch{epoch+1}.pth\"))\n",
    "            torch.save(discriminator.state_dict(), os.path.join(output_dir, f\"discriminator_epoch{epoch+1}.pth\"))\n",
    "    except KeyboardInterrupt:\n",
    "        # On keyboard interrupt, save the model weights\n",
    "        gen_path = os.path.join(save_dir, \"generator_interrupted.pth\")\n",
    "        disc_path = os.path.join(save_dir, \"discriminator_interrupted.pth\")\n",
    "\n",
    "        torch.save(generator.state_dict(), gen_path)\n",
    "        torch.save(discriminator.state_dict(), disc_path)\n",
    "        print(f\"\\nTraining interrupted. Model weights saved to:\\n{gen_path}\\n{disc_path}\")\n",
    "\n",
    "        # Re-raise the exception so you know training was interrupted\n",
    "        raise\n",
    "    print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def train_gan_with_pretrained_generators(\n",
    "    pretrained_generator,\n",
    "    num_epochs,\n",
    "    z_dim,\n",
    "    lr_gen,\n",
    "    lr_disc,\n",
    "    batch_size,\n",
    "    train_dataset,\n",
    "    num_generators,\n",
    "    seed,\n",
    "    audio_length,\n",
    "    output_dir,\n",
    "    lambda_gp=10,\n",
    "    lambda_ortho=0.1,\n",
    "    num_critic=5,\n",
    "    checkpoint_dir='checkpoints',\n",
    "    resume=True\n",
    "):\n",
    "    # Set seeds and device\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize multiple generators with the pretrained generator\n",
    "    generators = []\n",
    "    for _ in range(num_generators):\n",
    "        gen = Generator(...)  # same as your code\n",
    "        gen.load_state_dict(pretrained_generator.state_dict())\n",
    "        gen.to(device)\n",
    "        generators.append(gen)\n",
    "\n",
    "    # Initialize Discriminator\n",
    "    discriminator = Multiple_Random_Window_Discriminators(lc_channels=...).to(device) # same as your code\n",
    "\n",
    "    optimizer_gens = [optim.Adam(gen.parameters(), lr=lr_gen, betas=(0.5, 0.9)) for gen in generators]\n",
    "    optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Load checkpoint if resume is True and checkpoint exists\n",
    "    start_epoch = 0\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')\n",
    "    if resume and os.path.exists(checkpoint_path):\n",
    "        print(\"Resuming from checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "        # Load models\n",
    "        for idx, gen in enumerate(generators):\n",
    "            gen.load_state_dict(checkpoint['generator_state_dicts'][idx])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "\n",
    "        # Load optimizers\n",
    "        for idx, optimizer_gen in enumerate(optimizer_gens):\n",
    "            optimizer_gen.load_state_dict(checkpoint['optimizer_gens_state_dicts'][idx])\n",
    "        optimizer_disc.load_state_dict(checkpoint['optimizer_disc_state_dict'])\n",
    "\n",
    "        print(f\"Resumed training from epoch {start_epoch}\")\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    try:\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            torch.cuda.empty_cache()\n",
    "            for batch_idx, (real_audio, features) in enumerate(train_loader):\n",
    "                real_audio = real_audio.to(device)\n",
    "                features = features.to(device)\n",
    "                batch_size = real_audio.size(0)\n",
    "\n",
    "                # Train Discriminator multiple times\n",
    "                for _ in range(num_critic):\n",
    "                    optimizer_disc.zero_grad()\n",
    "                    noises = [generate_noise(batch_size, z_dim, device) for _ in range(num_generators)]\n",
    "                    fakes = [gen(features, noises[idx]) for idx, gen in enumerate(generators)]\n",
    "\n",
    "                    real_outputs, fake_outputs = discriminator(real_audio, torch.stack(fakes), features)\n",
    "                    loss_disc = sum([torch.mean(fake) - torch.mean(real) for real, fake in zip(real_outputs, fake_outputs)])\n",
    "                    gradient_penalty = compute_gradient_penalty(discriminator, real_audio, fakes[0], device)\n",
    "                    loss_disc += lambda_gp * gradient_penalty\n",
    "                    loss_disc.backward()\n",
    "                    optimizer_disc.step()\n",
    "\n",
    "                # Train Generators\n",
    "                for idx, gen in enumerate(generators):\n",
    "                    optimizer_gens[idx].zero_grad()\n",
    "                    noise = generate_noise(batch_size, z_dim, device)\n",
    "                    fake = gen(features, noise)\n",
    "                    fake_outputs = discriminator(fake, fake, features)[1]\n",
    "                    loss_gen = -torch.mean(torch.stack(fake_outputs))\n",
    "                    loss_gen.backward()\n",
    "                    optimizer_gens[idx].step()\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {loss_disc.item():.4f}, Loss G: {loss_gen.item():.4f}\")\n",
    "\n",
    "            # Save generated samples\n",
    "            visualize_and_save_generated_waveforms(\n",
    "                generators, z_dim, features, num_waveforms=5, device=device, epoch=epoch, sample_rate=16000, output_dir=output_dir\n",
    "            )\n",
    "\n",
    "            # Save checkpoint at the end of each epoch\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'generator_state_dicts': [gen.state_dict() for gen in generators],\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_gens_state_dicts': [opt.state_dict() for opt in optimizer_gens],\n",
    "                'optimizer_disc_state_dict': optimizer_disc.state_dict()\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # Save checkpoint on interrupt\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dicts': [gen.state_dict() for gen in generators],\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_gens_state_dicts': [opt.state_dict() for opt in optimizer_gens],\n",
    "            'optimizer_disc_state_dict': optimizer_disc.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(\"\\nTraining interrupted! Checkpoint saved. You can resume training later.\")\n",
    "        raise\n",
    "\n",
    "    print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def train_with_interrupt_saving(generator, discriminator, optimizer_gen, optimizer_disc,\n",
    "                                train_loader, num_epochs, save_dir, device):\n",
    "    \"\"\"\n",
    "    Train the generator and discriminator. If a KeyboardInterrupt occurs,\n",
    "    save the current model weights before exiting.\n",
    "\n",
    "    Args:\n",
    "        generator (nn.Module): The generator model.\n",
    "        discriminator (nn.Module): The discriminator model.\n",
    "        optimizer_gen (torch.optim.Optimizer): Optimizer for the generator.\n",
    "        optimizer_disc (torch.optim.Optimizer): Optimizer for the discriminator.\n",
    "        train_loader (DataLoader): DataLoader for the training data.\n",
    "        num_epochs (int): Number of epochs to train.\n",
    "        save_dir (str): Directory to save model weights on interruption.\n",
    "        device (torch.device): The device to run training on (cpu or cuda).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            generator.train()\n",
    "            discriminator.train()\n",
    "            for batch_idx, (real_audio, features) in enumerate(train_loader):\n",
    "                # Move data to the device\n",
    "                real_audio = real_audio.to(device)\n",
    "                features = features.to(device)\n",
    "\n",
    "                # Your training steps go here:\n",
    "                # 1. Train discriminator\n",
    "                optimizer_disc.zero_grad()\n",
    "                # ... compute loss for discriminator ...\n",
    "                # loss_disc.backward()\n",
    "                # optimizer_disc.step()\n",
    "\n",
    "                # 2. Train generator\n",
    "                optimizer_gen.zero_grad()\n",
    "                # ... compute loss for generator ...\n",
    "                # loss_gen.backward()\n",
    "                # optimizer_gen.step()\n",
    "\n",
    "                # (The above are placeholders, add your actual loss computations and steps)\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] completed.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # On keyboard interrupt, save the model weights\n",
    "        gen_path = os.path.join(save_dir, \"generator_interrupted.pth\")\n",
    "        disc_path = os.path.join(save_dir, \"discriminator_interrupted.pth\")\n",
    "\n",
    "        torch.save(generator.state_dict(), gen_path)\n",
    "        torch.save(discriminator.state_dict(), disc_path)\n",
    "        print(f\"\\nTraining interrupted. Model weights saved to:\\n{gen_path}\\n{disc_path}\")\n",
    "\n",
    "        # Re-raise the exception so you know training was interrupted\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain a single generator\n",
    "def pretrain_single_generator(num_epochs, z_dim, lr_gen, lr_disc, batch_size, seed, audio_length, output_dir, save_dir, train_dataset):\n",
    "    set_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define the single generator and discriminator\n",
    "    generator = Generator(in_channels=train_dataset[0][1].shape[0], z_channels=z_dim).to(device)\n",
    "    discriminator = Multiple_Random_Window_Discriminators(lc_channels=train_dataset[0][1].shape[0]).to(device)\n",
    "\n",
    "    optimizer_gen = optim.Adam(generator.parameters(), lr=lr_gen, betas=(0.5, 0.9))\n",
    "    optimizer_disc = optim.Adam(discriminator.parameters(), lr=lr_disc, betas=(0.5, 0.9))\n",
    "\n",
    "    # Load and preprocess the audio dataset\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    try:\n",
    "    # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            torch.cuda.empty_cache()\n",
    "            for batch_idx, (real_audio, features) in enumerate(train_loader):\n",
    "                real_audio = real_audio.to(device)\n",
    "                features = features.unsqueeze(2).to(device)\n",
    "                batch_size = real_audio.size(0)\n",
    "\n",
    "                # Train Discriminator\n",
    "                for _ in range(5):\n",
    "                    optimizer_disc.zero_grad()\n",
    "                    noise = generate_noise(batch_size, z_dim, device)\n",
    "                    fake_audio = generator(features, noise).detach()\n",
    "                    real_outputs, fake_outputs = discriminator(real_audio, fake_audio, features)\n",
    "                    loss_disc = sum([torch.mean(fake) - torch.mean(real) for real, fake in zip(real_outputs, fake_outputs)])\n",
    "                    gradient_penalty = compute_gradient_penalty(discriminator, real_audio, fake_audio, device)\n",
    "                    loss_disc += 10 * gradient_penalty\n",
    "                    loss_disc.backward()\n",
    "                    optimizer_disc.step()\n",
    "\n",
    "                # Train Generator\n",
    "                optimizer_gen.zero_grad()\n",
    "                noise = generate_noise(batch_size, z_dim, device)\n",
    "                fake_audio = generator(features, noise)\n",
    "                fake_outputs = discriminator(fake_audio, fake_audio, features)[1]\n",
    "                loss_gen = -torch.mean(torch.stack(fake_outputs))\n",
    "                loss_gen.backward()\n",
    "                optimizer_gen.step()\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {loss_disc.item():.4f}, Loss G: {loss_gen.item():.4f}\")\n",
    "\n",
    "            # Save generated samples and models\n",
    "            visualize_and_save_generated_waveforms(\n",
    "                [generator], z_dim, features, num_waveforms=5, device=device, epoch=epoch, sample_rate=16000, output_dir=output_dir\n",
    "            )\n",
    "            torch.save(generator.state_dict(), os.path.join(output_dir, f\"pretrained_generator_epoch{epoch+1}.pth\"))\n",
    "\n",
    "        print(\"Pretraining complete.\")\n",
    "        return generator\n",
    "    except KeyboardInterrupt:\n",
    "        # On keyboard interrupt, save the model weights\n",
    "        gen_path = os.path.join(save_dir, \"generator_interrupted.pth\")\n",
    "        disc_path = os.path.join(save_dir, \"discriminator_interrupted.pth\")\n",
    "\n",
    "        torch.save(generator.state_dict(), gen_path)\n",
    "        torch.save(discriminator.state_dict(), disc_path)\n",
    "        print(f\"\\nTraining interrupted. Model weights saved to:\\n{gen_path}\\n{disc_path}\")\n",
    "\n",
    "        # Re-raise the exception so you know training was interrupted\n",
    "        raise\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "audio_length = 64000\n",
    "z_dim = 128\n",
    "lr_gen = 0.0002\n",
    "lr_disc = 0.0002\n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "root_dir = \"./data\"\n",
    "sample_rate = 16000\n",
    "num_generators = 5\n",
    "output_dir = 'generated_audio'\n",
    "save_dir=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/tameszaza/My Passport/ml/.venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ[\"PATH\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/tameszaza/My Passport/ml/.venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin:c:\\Program Files (x86)\\eSpeak\\command_line\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PATH\"] += os.pathsep + r\"c:\\Program Files (x86)\\eSpeak\\command_line\"\n",
    "print(os.environ[\"PATH\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "print(shutil.which(\"espeak\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eSpeak is not found. Ensure the path is set correctly.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "try:\n",
    "    subprocess.run([\"espeak\", \"--version\"], check=True)\n",
    "    print(\"eSpeak is accessible from Python.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"eSpeak is not found. Ensure the path is set correctly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ESPEAK_DATA_PATH\"] = r\"c:\\Program Files (x86)\\eSpeak\\espeak-data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available backends: dict_keys(['espeak', 'festival', 'segments', 'espeak-mbrola'])\n"
     ]
    }
   ],
   "source": [
    "from phonemizer.backend import BACKENDS\n",
    "\n",
    "print(\"Available backends:\", BACKENDS.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH environment variable: /media/tameszaza/My Passport/ml/.venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin:c:\\Program Files (x86)\\eSpeak\\command_line\n",
      "eSpeak location: None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Check PATH\n",
    "print(\"PATH environment variable:\", os.environ[\"PATH\"])\n",
    "\n",
    "# Check if espeak is accessible\n",
    "espeak_path = shutil.which(\"espeak\")\n",
    "print(\"eSpeak location:\", espeak_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2703 .flac files in ./data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50072/206904885.py:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  audio_dataset = torch.tensor(audio_dataset, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n"
     ]
    }
   ],
   "source": [
    "train_dataset = preprocess_dataset(root_dir, target_sr=sample_rate, target_length=audio_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D or 4D (batch mode) tensor with optional 0 dim batch size for input, but got:[16, 1, 1, 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pretrained_generator \u001b[38;5;241m=\u001b[39m \u001b[43mpretrain_single_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mz_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_gen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_disc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_disc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43maudio_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwaveform_pre\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlinguist_pretrain1/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 34\u001b[0m, in \u001b[0;36mpretrain_single_generator\u001b[0;34m(num_epochs, z_dim, lr_gen, lr_disc, batch_size, seed, audio_length, output_dir, save_dir, train_dataset)\u001b[0m\n\u001b[1;32m     32\u001b[0m noise \u001b[38;5;241m=\u001b[39m generate_noise(batch_size, z_dim, device)\n\u001b[1;32m     33\u001b[0m fake_audio \u001b[38;5;241m=\u001b[39m generator(features, noise)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m---> 34\u001b[0m real_outputs, fake_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m loss_disc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([torch\u001b[38;5;241m.\u001b[39mmean(fake) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(real) \u001b[38;5;28;01mfor\u001b[39;00m real, fake \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(real_outputs, fake_outputs)])\n\u001b[1;32m     36\u001b[0m gradient_penalty \u001b[38;5;241m=\u001b[39m compute_gradient_penalty(discriminator, real_audio, fake_audio, device)\n",
      "File \u001b[0;32m/media/tameszaza/My Passport/ml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/tameszaza/My Passport/ml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[13], line 38\u001b[0m, in \u001b[0;36mMultiple_Random_Window_Discriminators.forward\u001b[0;34m(self, real_samples, fake_samples, conditions)\u001b[0m\n\u001b[1;32m     36\u001b[0m     real_output \u001b[38;5;241m=\u001b[39m layer(real_samples[:, :, index: index \u001b[38;5;241m+\u001b[39m size])\n\u001b[1;32m     37\u001b[0m     real_outputs\u001b[38;5;241m.\u001b[39mappend(real_output)\n\u001b[0;32m---> 38\u001b[0m     fake_output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     fake_outputs\u001b[38;5;241m.\u001b[39mappend(fake_output)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Conditional discriminator\u001b[39;00m\n",
      "File \u001b[0;32m/media/tameszaza/My Passport/ml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/tameszaza/My Passport/ml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[17], line 20\u001b[0m, in \u001b[0;36mUnConditionalDBlocks.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 20\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/media/tameszaza/My Passport/ml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/tameszaza/My Passport/ml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[14], line 20\u001b[0m, in \u001b[0;36mDBlock.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m---> 20\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual(inputs)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/media/tameszaza/My Passport/ml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/tameszaza/My Passport/ml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/media/tameszaza/My Passport/ml/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/media/tameszaza/My Passport/ml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/tameszaza/My Passport/ml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/media/tameszaza/My Passport/ml/.venv/lib/python3.12/site-packages/torch/nn/modules/pooling.py:651\u001b[0m, in \u001b[0;36mAvgPool1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg_pool1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_include_pad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D or 4D (batch mode) tensor with optional 0 dim batch size for input, but got:[16, 1, 1, 0]"
     ]
    }
   ],
   "source": [
    "pretrained_generator = pretrain_single_generator(\n",
    "        num_epochs=20,\n",
    "        z_dim=z_dim,\n",
    "        lr_gen=lr_gen,\n",
    "        lr_disc=lr_disc,\n",
    "        batch_size=batch_size,\n",
    "        seed=42,\n",
    "        audio_length=audio_length,\n",
    "        output_dir='waveform_pre',\n",
    "        train_dataset=train_dataset,\n",
    "        save_dir='linguist_pretrain1/'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan_with_pretrained_generators(\n",
    "        pretrained_generator,\n",
    "        num_epochs=num_epochs,\n",
    "        z_dim=z_dim,\n",
    "        lr_gen=lr_gen,\n",
    "        lr_disc=lr_disc,\n",
    "        batch_size=batch_size,\n",
    "        train_dataset=train_dataset,\n",
    "        num_generators=num_generators,\n",
    "        seed=42,\n",
    "        audio_length=audio_length,\n",
    "        output_dir=output_dir,\n",
    "        checkpoint_dir='my_checkpoints',\n",
    "        resume=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
